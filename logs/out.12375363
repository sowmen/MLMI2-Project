Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.3)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.10437135219574
  batch 100 loss: 3.1921063089370727
  batch 150 loss: 3.092411551475525
  batch 200 loss: 2.978049077987671
  batch 250 loss: 2.880886979103088
  batch 300 loss: 2.690992875099182
  batch 350 loss: 2.5452268981933592
  batch 400 loss: 2.5882340335845946
  batch 450 loss: 2.3695208692550658
  batch 500 loss: 2.227629861831665
  batch 550 loss: 2.148625090122223
  batch 600 loss: 2.0943929243087767
  batch 650 loss: 1.981892955303192
  batch 700 loss: 1.9913638544082641
  batch 750 loss: 1.9105203199386596
  batch 800 loss: 1.8832227873802185
  batch 850 loss: 1.8329950022697448
  batch 900 loss: 1.8123845148086548
LOSS train 1.81238 valid 1.79104, valid PER 69.96%
EPOCH 2:
  batch 50 loss: 1.754531548023224
  batch 100 loss: 1.6968349051475524
  batch 150 loss: 1.6732747054100037
  batch 200 loss: 1.6699717164039611
  batch 250 loss: 1.6687696766853333
  batch 300 loss: 1.6282153797149659
  batch 350 loss: 1.5350563049316406
  batch 400 loss: 1.5463607454299926
  batch 450 loss: 1.499435739517212
  batch 500 loss: 1.5318782329559326
  batch 550 loss: 1.5209600114822388
  batch 600 loss: 1.4551299071311952
  batch 650 loss: 1.4673456311225892
  batch 700 loss: 1.4656608939170837
  batch 750 loss: 1.4132474541664124
  batch 800 loss: 1.377667624950409
  batch 850 loss: 1.3587771844863892
  batch 900 loss: 1.3947306680679321
LOSS train 1.39473 valid 1.34463, valid PER 41.87%
EPOCH 3:
  batch 50 loss: 1.3267650699615479
  batch 100 loss: 1.30743008852005
  batch 150 loss: 1.3121890592575074
  batch 200 loss: 1.275418326854706
  batch 250 loss: 1.2647386467456818
  batch 300 loss: 1.242982236146927
  batch 350 loss: 1.3032667994499207
  batch 400 loss: 1.2768591845035553
  batch 450 loss: 1.2495890760421753
  batch 500 loss: 1.2264652693271636
  batch 550 loss: 1.2490292227268218
  batch 600 loss: 1.2237703835964202
  batch 650 loss: 1.1893040978908538
  batch 700 loss: 1.248096579313278
  batch 750 loss: 1.2851564514636993
  batch 800 loss: 1.1769151735305785
  batch 850 loss: 1.2209740579128265
  batch 900 loss: 1.1886001443862915
LOSS train 1.18860 valid 1.25728, valid PER 37.21%
EPOCH 4:
  batch 50 loss: 1.1612199461460113
  batch 100 loss: 1.1805851531028748
  batch 150 loss: 1.121767784357071
  batch 200 loss: 1.1719984018802643
  batch 250 loss: 1.1619334924221039
  batch 300 loss: 1.177610194683075
  batch 350 loss: 1.1056816852092743
  batch 400 loss: 1.1475676417350769
  batch 450 loss: 1.1273885655403137
  batch 500 loss: 1.1190900647640227
  batch 550 loss: 1.1303849768638612
  batch 600 loss: 1.1589293944835664
  batch 650 loss: 1.140359719991684
  batch 700 loss: 1.1109042084217071
  batch 750 loss: 1.0935354924201965
  batch 800 loss: 1.0722986829280854
  batch 850 loss: 1.106119076013565
  batch 900 loss: 1.1558368182182313
LOSS train 1.15584 valid 1.20183, valid PER 35.55%
EPOCH 5:
  batch 50 loss: 1.0638888192176819
  batch 100 loss: 1.0671285355091096
  batch 150 loss: 1.173517380952835
  batch 200 loss: 1.0447878313064576
  batch 250 loss: 1.0621738278865813
  batch 300 loss: 1.1048895275592805
  batch 350 loss: 1.0750929594039917
  batch 400 loss: 1.0843370008468627
  batch 450 loss: 1.0794391596317292
  batch 500 loss: 1.0720684921741486
  batch 550 loss: 1.023107453584671
  batch 600 loss: 1.1004817926883697
  batch 650 loss: 1.0433885335922242
  batch 700 loss: 1.0984532690048219
  batch 750 loss: 1.0309113335609437
  batch 800 loss: 1.0502740490436553
  batch 850 loss: 1.0690804898738862
  batch 900 loss: 1.0391266655921936
LOSS train 1.03913 valid 1.07715, valid PER 33.06%
EPOCH 6:
  batch 50 loss: 1.0318870890140532
  batch 100 loss: 0.9775107097625733
  batch 150 loss: 0.9638682174682617
  batch 200 loss: 1.0178308236598967
  batch 250 loss: 1.0282433140277862
  batch 300 loss: 1.0155420446395873
  batch 350 loss: 1.0176550769805908
  batch 400 loss: 1.0200472748279572
  batch 450 loss: 1.0377678024768828
  batch 500 loss: 1.055121318101883
  batch 550 loss: 1.0522332906723022
  batch 600 loss: 0.9908325016498566
  batch 650 loss: 1.0354455316066742
  batch 700 loss: 1.026525193452835
  batch 750 loss: 0.9833744561672211
  batch 800 loss: 0.9806751775741577
  batch 850 loss: 0.9810732054710388
  batch 900 loss: 1.0260732686519622
LOSS train 1.02607 valid 1.10115, valid PER 34.31%
EPOCH 7:
  batch 50 loss: 0.9889783680438995
  batch 100 loss: 1.002300808429718
  batch 150 loss: 0.9937205386161804
  batch 200 loss: 0.9536708056926727
  batch 250 loss: 0.9607757937908172
  batch 300 loss: 0.9521729135513306
  batch 350 loss: 0.9521950578689575
  batch 400 loss: 0.9645727860927582
  batch 450 loss: 0.980455505847931
  batch 500 loss: 0.9603410422801971
  batch 550 loss: 0.9608585858345031
  batch 600 loss: 0.9970314717292785
  batch 650 loss: 0.9703495800495148
  batch 700 loss: 1.011612845659256
  batch 750 loss: 1.0229485285282136
  batch 800 loss: 0.965846197605133
  batch 850 loss: 0.9909606862068177
  batch 900 loss: 1.0270051991939544
LOSS train 1.02701 valid 1.02952, valid PER 32.68%
EPOCH 8:
  batch 50 loss: 0.921958509683609
  batch 100 loss: 0.9332324981689453
  batch 150 loss: 0.9419731521606445
  batch 200 loss: 0.9375994193553925
  batch 250 loss: 0.9557804214954376
  batch 300 loss: 0.8871290051937103
  batch 350 loss: 0.9538369262218476
  batch 400 loss: 0.9202807378768921
  batch 450 loss: 0.9459959971904754
  batch 500 loss: 0.9731817221641541
  batch 550 loss: 0.9343988764286041
  batch 600 loss: 0.9618902158737183
  batch 650 loss: 1.0117942476272583
  batch 700 loss: 0.9405882394313813
  batch 750 loss: 0.9696696734428406
  batch 800 loss: 0.9453540909290313
  batch 850 loss: 0.9400512111186982
  batch 900 loss: 0.9337013244628907
LOSS train 0.93370 valid 1.02781, valid PER 31.80%
EPOCH 9:
  batch 50 loss: 0.8736555933952331
  batch 100 loss: 0.9094074785709381
  batch 150 loss: 0.9193921780586243
  batch 200 loss: 0.8733446967601776
  batch 250 loss: 0.9134734725952148
  batch 300 loss: 0.9086404466629028
  batch 350 loss: 0.9363882756233215
  batch 400 loss: 0.8917147731781006
  batch 450 loss: 0.8974252390861511
  batch 500 loss: 0.8745049059391021
  batch 550 loss: 0.9464471411705017
  batch 600 loss: 0.9538047933578491
  batch 650 loss: 0.9109590721130371
  batch 700 loss: 0.8764892971515655
  batch 750 loss: 0.8987959849834443
  batch 800 loss: 0.9508511888980865
  batch 850 loss: 0.9411179804801941
  batch 900 loss: 0.8941505932807923
LOSS train 0.89415 valid 1.01484, valid PER 31.42%
EPOCH 10:
  batch 50 loss: 0.8597638833522797
  batch 100 loss: 0.897800862789154
  batch 150 loss: 0.9449887144565582
  batch 200 loss: 0.923611900806427
  batch 250 loss: 0.9188850688934326
  batch 300 loss: 0.865875334739685
  batch 350 loss: 0.9086461985111236
  batch 400 loss: 0.8482787954807282
  batch 450 loss: 0.8432763302326203
  batch 500 loss: 0.8893123495578766
  batch 550 loss: 0.8859675800800324
  batch 600 loss: 0.8775222206115723
  batch 650 loss: 0.8826040613651276
  batch 700 loss: 0.8928919243812561
  batch 750 loss: 0.8651566672325134
  batch 800 loss: 0.8799759340286255
  batch 850 loss: 0.8988351786136627
  batch 900 loss: 0.8813369011878968
LOSS train 0.88134 valid 1.03621, valid PER 32.69%
EPOCH 11:
  batch 50 loss: 0.8445699501037598
  batch 100 loss: 0.8339737772941589
  batch 150 loss: 0.8697816371917725
  batch 200 loss: 0.9122550904750824
  batch 250 loss: 0.8788518762588501
  batch 300 loss: 0.8499030089378357
  batch 350 loss: 0.8559434354305268
  batch 400 loss: 0.8934778964519501
  batch 450 loss: 0.8880932748317718
  batch 500 loss: 0.8893205118179321
  batch 550 loss: 0.8894355547428131
  batch 600 loss: 0.8783748197555542
  batch 650 loss: 0.921323333978653
  batch 700 loss: 0.8462183701992035
  batch 750 loss: 0.8718794357776641
  batch 800 loss: 0.884120008945465
  batch 850 loss: 0.910671535730362
  batch 900 loss: 0.8984741199016572
LOSS train 0.89847 valid 0.98645, valid PER 30.42%
EPOCH 12:
  batch 50 loss: 0.8537374305725097
  batch 100 loss: 0.9138275134563446
  batch 150 loss: 0.8327438127994538
  batch 200 loss: 0.8497125786542893
  batch 250 loss: 0.8517998468875885
  batch 300 loss: 0.8247224903106689
  batch 350 loss: 0.8434175372123718
  batch 400 loss: 0.8595695984363556
  batch 450 loss: 0.8602403545379639
  batch 500 loss: 0.8698383235931396
  batch 550 loss: 0.8017238008975983
  batch 600 loss: 0.8265952241420745
  batch 650 loss: 0.8441932713985443
  batch 700 loss: 0.8642523467540741
  batch 750 loss: 0.8273834693431854
  batch 800 loss: 0.8300762057304383
  batch 850 loss: 0.8543874645233154
  batch 900 loss: 0.8814762049913406
LOSS train 0.88148 valid 0.96762, valid PER 29.99%
EPOCH 13:
  batch 50 loss: 0.7693395125865936
  batch 100 loss: 0.811448472738266
  batch 150 loss: 0.7739895534515381
  batch 200 loss: 0.82310542345047
  batch 250 loss: 0.8158404684066772
  batch 300 loss: 0.7973579168319702
  batch 350 loss: 0.816049530506134
  batch 400 loss: 0.8199532878398895
  batch 450 loss: 0.8288224005699157
  batch 500 loss: 0.7935242688655854
  batch 550 loss: 0.822814495563507
  batch 600 loss: 0.8050667965412139
  batch 650 loss: 0.8227013921737671
  batch 700 loss: 0.8168314552307129
  batch 750 loss: 0.782931734919548
  batch 800 loss: 0.8226432263851166
  batch 850 loss: 0.8438556909561157
  batch 900 loss: 0.843908463716507
LOSS train 0.84391 valid 0.99618, valid PER 30.38%
EPOCH 14:
  batch 50 loss: 0.7942280131578445
  batch 100 loss: 0.7813727128505706
  batch 150 loss: 0.7584030795097351
  batch 200 loss: 0.7828889179229737
  batch 250 loss: 0.7909434771537781
  batch 300 loss: 0.8374049592018128
  batch 350 loss: 0.7692293190956115
  batch 400 loss: 0.7896055817604065
  batch 450 loss: 0.7814462554454803
  batch 500 loss: 0.8250156903266906
  batch 550 loss: 0.8242403519153595
  batch 600 loss: 0.7714256000518799
  batch 650 loss: 0.8302717065811157
  batch 700 loss: 0.8265470671653747
  batch 750 loss: 0.774072573184967
  batch 800 loss: 0.7685661327838897
  batch 850 loss: 0.8227011144161225
  batch 900 loss: 0.7941596460342407
LOSS train 0.79416 valid 0.98061, valid PER 30.29%
EPOCH 15:
  batch 50 loss: 0.7609496974945068
  batch 100 loss: 0.7457001340389252
  batch 150 loss: 0.7622450125217438
  batch 200 loss: 0.7916273540258407
  batch 250 loss: 0.7903004431724548
  batch 300 loss: 0.7828452175855637
  batch 350 loss: 0.767295548915863
  batch 400 loss: 0.7680212277173996
  batch 450 loss: 0.781956901550293
  batch 500 loss: 0.7419582068920135
  batch 550 loss: 0.7723672115802764
  batch 600 loss: 0.796742080450058
  batch 650 loss: 0.809067211151123
  batch 700 loss: 0.8024208605289459
  batch 750 loss: 0.7987784194946289
  batch 800 loss: 0.7770887470245361
  batch 850 loss: 0.7791056656837463
  batch 900 loss: 0.8145168256759644
LOSS train 0.81452 valid 1.01315, valid PER 30.71%
EPOCH 16:
  batch 50 loss: 0.8080137264728546
  batch 100 loss: 0.7649956810474395
  batch 150 loss: 0.7589434051513672
  batch 200 loss: 0.8233129286766052
  batch 250 loss: 0.8527787148952484
  batch 300 loss: 0.7819617938995361
  batch 350 loss: 0.8196891069412231
  batch 400 loss: 0.8116311848163604
  batch 450 loss: 0.8449828350543975
  batch 500 loss: 0.7863030707836152
  batch 550 loss: 0.8021930539608002
  batch 600 loss: 0.7833031141757965
  batch 650 loss: 0.7961353421211242
  batch 700 loss: 0.747464075088501
  batch 750 loss: 0.7934136080741883
  batch 800 loss: 0.7948209643363953
  batch 850 loss: 0.7962722897529602
  batch 900 loss: 0.7671758937835693
LOSS train 0.76718 valid 0.94630, valid PER 28.43%
EPOCH 17:
  batch 50 loss: 0.7457503288984298
  batch 100 loss: 0.7418675267696381
  batch 150 loss: 0.7429816943407058
  batch 200 loss: 0.733943487405777
  batch 250 loss: 0.7380010044574737
  batch 300 loss: 0.7532246327400207
  batch 350 loss: 0.7175553548336029
  batch 400 loss: 0.7770742702484131
  batch 450 loss: 0.7865334463119507
  batch 500 loss: 0.7401495337486267
  batch 550 loss: 0.752157729268074
  batch 600 loss: 0.7780954134464264
  batch 650 loss: 0.7622718560695648
  batch 700 loss: 0.7512078183889389
  batch 750 loss: 0.7367616605758667
  batch 800 loss: 0.7289643055200576
  batch 850 loss: 0.7492531418800354
  batch 900 loss: 0.743849692940712
LOSS train 0.74385 valid 0.97548, valid PER 28.88%
EPOCH 18:
  batch 50 loss: 0.7281569707393646
  batch 100 loss: 0.7181424856185913
  batch 150 loss: 0.7466362690925599
  batch 200 loss: 0.7184954535961151
  batch 250 loss: 0.7276389122009277
  batch 300 loss: 0.7016548794507981
  batch 350 loss: 0.7193907344341278
  batch 400 loss: 0.7051794630289078
  batch 450 loss: 0.7723629093170166
  batch 500 loss: 0.7499186742305756
  batch 550 loss: 0.7428391206264496
  batch 600 loss: 0.7185034632682801
  batch 650 loss: 0.7109399193525314
  batch 700 loss: 0.7259755539894104
  batch 750 loss: 0.7391066747903824
  batch 800 loss: 0.7442264384031296
  batch 850 loss: 0.7490907174348831
  batch 900 loss: 0.7890528559684753
LOSS train 0.78905 valid 0.98393, valid PER 29.56%
EPOCH 19:
  batch 50 loss: 0.6888611280918121
  batch 100 loss: 0.6779898744821549
  batch 150 loss: 0.7028155034780502
  batch 200 loss: 0.7137623739242553
  batch 250 loss: 0.7191596138477325
  batch 300 loss: 0.7284419500827789
  batch 350 loss: 0.7271734309196473
  batch 400 loss: 0.7301658022403718
  batch 450 loss: 0.7290167593955994
  batch 500 loss: 0.7396174871921539
  batch 550 loss: 0.7211270046234131
  batch 600 loss: 0.7356175947189331
  batch 650 loss: 0.8213670694828034
  batch 700 loss: 0.7328824746608734
  batch 750 loss: 0.728426228761673
  batch 800 loss: 0.7580101239681244
  batch 850 loss: 0.7502627086639404
  batch 900 loss: 0.7384692418575287
LOSS train 0.73847 valid 0.98575, valid PER 29.76%
EPOCH 20:
  batch 50 loss: 0.6832294154167176
  batch 100 loss: 0.6894796562194824
  batch 150 loss: 0.6940586018562317
  batch 200 loss: 0.6782413351535798
  batch 250 loss: 0.7026980805397034
  batch 300 loss: 0.7163398122787475
  batch 350 loss: 0.6924230188131333
  batch 400 loss: 0.726053649187088
  batch 450 loss: 0.7272102946043014
  batch 500 loss: 0.7202806997299195
  batch 550 loss: 0.7841560316085815
  batch 600 loss: 0.7106039869785309
  batch 650 loss: 0.720508908033371
  batch 700 loss: 0.7226595360040665
  batch 750 loss: 0.7131359159946442
  batch 800 loss: 0.7376255190372467
  batch 850 loss: 0.7055059146881103
  batch 900 loss: 0.7309735918045044
LOSS train 0.73097 valid 0.99248, valid PER 29.56%
Training finished in 8.0 minutes.
Model saved to checkpoints/20230116_113350/model_16
Loading model from checkpoints/20230116_113350/model_16
SUB: 17.10%, DEL: 10.66%, INS: 2.72%, COR: 72.24%, PER: 30.49%
