Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.2)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.129119358062744
  batch 100 loss: 3.1105474710464476
  batch 150 loss: 2.9906872081756593
  batch 200 loss: 2.862061767578125
  batch 250 loss: 2.789096574783325
  batch 300 loss: 2.6065796327590944
  batch 350 loss: 2.4582936573028564
  batch 400 loss: 2.4234023118019103
  batch 450 loss: 2.3496293950080873
  batch 500 loss: 2.3006021618843078
  batch 550 loss: 2.130531697273254
  batch 600 loss: 2.051071662902832
  batch 650 loss: 1.9461239719390868
  batch 700 loss: 1.943814446926117
  batch 750 loss: 1.874981017112732
  batch 800 loss: 1.857931694984436
  batch 850 loss: 1.790543382167816
  batch 900 loss: 1.7905249047279357
LOSS train 1.79052 valid 1.75080, valid PER 68.50%
EPOCH 2:
  batch 50 loss: 1.719575252532959
  batch 100 loss: 1.6699136519432067
  batch 150 loss: 1.6493594002723695
  batch 200 loss: 1.6285804963111878
  batch 250 loss: 1.661977300643921
  batch 300 loss: 1.5958982372283936
  batch 350 loss: 1.4994884681701661
  batch 400 loss: 1.5136310529708863
  batch 450 loss: 1.4509748578071595
  batch 500 loss: 1.4803628158569335
  batch 550 loss: 1.474288237094879
  batch 600 loss: 1.4198830652236938
  batch 650 loss: 1.4452276086807252
  batch 700 loss: 1.4372106766700745
  batch 750 loss: 1.390718331336975
  batch 800 loss: 1.349063835144043
  batch 850 loss: 1.359698648452759
  batch 900 loss: 1.3847734594345094
LOSS train 1.38477 valid 1.34438, valid PER 42.43%
EPOCH 3:
  batch 50 loss: 1.3364393997192383
  batch 100 loss: 1.318900327682495
  batch 150 loss: 1.3119491887092591
  batch 200 loss: 1.2755881094932555
  batch 250 loss: 1.2787675774097442
  batch 300 loss: 1.2686644434928893
  batch 350 loss: 1.321312906742096
  batch 400 loss: 1.2836735093593596
  batch 450 loss: 1.2659431171417237
  batch 500 loss: 1.2350380313396454
  batch 550 loss: 1.2569724547863006
  batch 600 loss: 1.230710473060608
  batch 650 loss: 1.1972450423240661
  batch 700 loss: 1.2243452870845795
  batch 750 loss: 1.2865933644771577
  batch 800 loss: 1.1832511949539184
  batch 850 loss: 1.247175214290619
  batch 900 loss: 1.1643178904056548
LOSS train 1.16432 valid 1.23814, valid PER 37.57%
EPOCH 4:
  batch 50 loss: 1.1579072320461272
  batch 100 loss: 1.2093992114067078
  batch 150 loss: 1.1438442254066468
  batch 200 loss: 1.1774400901794433
  batch 250 loss: 1.1847512984275819
  batch 300 loss: 1.1668994545936584
  batch 350 loss: 1.1261739015579224
  batch 400 loss: 1.162855259180069
  batch 450 loss: 1.1611885631084442
  batch 500 loss: 1.15271098613739
  batch 550 loss: 1.1554737389087677
  batch 600 loss: 1.1762906837463378
  batch 650 loss: 1.1831473124027252
  batch 700 loss: 1.1240317583084107
  batch 750 loss: 1.1153365910053252
  batch 800 loss: 1.0748233711719513
  batch 850 loss: 1.1438945209980012
  batch 900 loss: 1.1593257546424867
LOSS train 1.15933 valid 1.16264, valid PER 35.15%
EPOCH 5:
  batch 50 loss: 1.08564390540123
  batch 100 loss: 1.0713666260242463
  batch 150 loss: 1.1289614939689636
  batch 200 loss: 1.0452326786518098
  batch 250 loss: 1.066703805923462
  batch 300 loss: 1.0749308037757874
  batch 350 loss: 1.0861965584754945
  batch 400 loss: 1.077704737186432
  batch 450 loss: 1.0759958136081695
  batch 500 loss: 1.0955443751811982
  batch 550 loss: 1.0310084676742555
  batch 600 loss: 1.122681393623352
  batch 650 loss: 1.0721197617053986
  batch 700 loss: 1.1250512135028838
  batch 750 loss: 1.0427462077140808
  batch 800 loss: 1.0939507293701172
  batch 850 loss: 1.07776087641716
  batch 900 loss: 1.078305060863495
LOSS train 1.07831 valid 1.13712, valid PER 35.16%
EPOCH 6:
  batch 50 loss: 1.0492801797389983
  batch 100 loss: 1.0006530642509461
  batch 150 loss: 0.9945123827457428
  batch 200 loss: 1.0290970265865327
  batch 250 loss: 1.0658272910118103
  batch 300 loss: 1.0298969125747681
  batch 350 loss: 1.0264576292037964
  batch 400 loss: 1.0132837879657746
  batch 450 loss: 1.0540649533271789
  batch 500 loss: 1.0432421362400055
  batch 550 loss: 1.0457370936870576
  batch 600 loss: 1.010629426240921
  batch 650 loss: 1.0311502194404603
  batch 700 loss: 1.0182707917690277
  batch 750 loss: 1.0347069346904754
  batch 800 loss: 1.0285962915420532
  batch 850 loss: 1.0386262571811675
  batch 900 loss: 1.060849142074585
LOSS train 1.06085 valid 1.22410, valid PER 37.22%
EPOCH 7:
  batch 50 loss: 1.0005250108242034
  batch 100 loss: 1.0124332463741303
  batch 150 loss: 0.98695467710495
  batch 200 loss: 0.9668845200538635
  batch 250 loss: 0.9708672785758972
  batch 300 loss: 0.9629601907730102
  batch 350 loss: 0.9845283019542694
  batch 400 loss: 0.9847117686271667
  batch 450 loss: 0.9694918060302734
  batch 500 loss: 0.9778380513191223
  batch 550 loss: 0.9749457788467407
  batch 600 loss: 0.9675761282444
  batch 650 loss: 0.9677860248088836
  batch 700 loss: 0.985393648147583
  batch 750 loss: 0.9555319273471832
  batch 800 loss: 0.9470308780670166
  batch 850 loss: 0.9649511706829071
  batch 900 loss: 1.0009860813617706
LOSS train 1.00099 valid 1.02890, valid PER 32.82%
EPOCH 8:
  batch 50 loss: 0.935727380514145
  batch 100 loss: 0.9387110435962677
  batch 150 loss: 0.9544350337982178
  batch 200 loss: 0.9025033760070801
  batch 250 loss: 0.9402564871311188
  batch 300 loss: 0.8932261598110199
  batch 350 loss: 0.9542942929267884
  batch 400 loss: 0.9236757504940033
  batch 450 loss: 0.954553575515747
  batch 500 loss: 0.9834794247150421
  batch 550 loss: 0.9239537727832794
  batch 600 loss: 0.9599769508838654
  batch 650 loss: 0.9778262054920197
  batch 700 loss: 0.9342372417449951
  batch 750 loss: 0.9256497025489807
  batch 800 loss: 0.9482579851150512
  batch 850 loss: 0.9348071265220642
  batch 900 loss: 0.9366893076896667
LOSS train 0.93669 valid 1.11849, valid PER 34.22%
EPOCH 9:
  batch 50 loss: 0.8934770464897156
  batch 100 loss: 0.9263047814369202
  batch 150 loss: 0.978989908695221
  batch 200 loss: 0.916181697845459
  batch 250 loss: 0.9577942681312561
  batch 300 loss: 0.9891773736476899
  batch 350 loss: 0.9682389104366302
  batch 400 loss: 0.9443493092060089
  batch 450 loss: 0.9376247870922089
  batch 500 loss: 0.8993185198307038
  batch 550 loss: 0.9499265372753143
  batch 600 loss: 0.9527719986438751
  batch 650 loss: 0.9278500247001648
  batch 700 loss: 0.9201338076591492
  batch 750 loss: 0.9176977670192719
  batch 800 loss: 0.9318673586845398
  batch 850 loss: 0.9383927214145661
  batch 900 loss: 0.8927186036109924
LOSS train 0.89272 valid 1.01539, valid PER 31.00%
EPOCH 10:
  batch 50 loss: 0.8536135911941528
  batch 100 loss: 0.8683279776573181
  batch 150 loss: 0.9008093225955963
  batch 200 loss: 0.9073582708835601
  batch 250 loss: 0.9249432766437531
  batch 300 loss: 0.8802523446083069
  batch 350 loss: 0.9071535611152649
  batch 400 loss: 0.8721399164199829
  batch 450 loss: 0.9662099623680115
  batch 500 loss: 0.9835952126979828
  batch 550 loss: 0.9358437943458557
  batch 600 loss: 0.9001674079895019
  batch 650 loss: 0.8784108781814575
  batch 700 loss: 0.8965212059020996
  batch 750 loss: 0.8882200396060944
  batch 800 loss: 0.8995434021949769
  batch 850 loss: 0.9276445281505584
  batch 900 loss: 0.9151834237575531
LOSS train 0.91518 valid 1.03289, valid PER 33.22%
EPOCH 11:
  batch 50 loss: 0.8438818120956421
  batch 100 loss: 0.8073356986045838
  batch 150 loss: 0.8407746624946594
  batch 200 loss: 0.8660070204734802
  batch 250 loss: 0.8755820649862289
  batch 300 loss: 0.827167694568634
  batch 350 loss: 0.8500628316402435
  batch 400 loss: 0.9167497873306274
  batch 450 loss: 0.8825896716117859
  batch 500 loss: 0.873990740776062
  batch 550 loss: 0.876841002702713
  batch 600 loss: 0.8494297206401825
  batch 650 loss: 0.8986482095718383
  batch 700 loss: 0.8400227797031402
  batch 750 loss: 0.8793082964420319
  batch 800 loss: 0.952043365240097
  batch 850 loss: 0.9463419222831726
  batch 900 loss: 0.9074284756183624
LOSS train 0.90743 valid 0.99605, valid PER 30.38%
EPOCH 12:
  batch 50 loss: 0.8445133757591248
  batch 100 loss: 0.8207973599433899
  batch 150 loss: 0.8293012547492981
  batch 200 loss: 0.839976373910904
  batch 250 loss: 0.8397390067577362
  batch 300 loss: 0.8261622881889343
  batch 350 loss: 0.8347370326519012
  batch 400 loss: 0.8455241173505783
  batch 450 loss: 0.8581275606155395
  batch 500 loss: 0.8808435654640198
  batch 550 loss: 0.7739115297794342
  batch 600 loss: 0.8154892456531525
  batch 650 loss: 0.8618464803695679
  batch 700 loss: 0.8665108609199524
  batch 750 loss: 0.8127906942367553
  batch 800 loss: 0.8416253805160523
  batch 850 loss: 0.8702237677574157
  batch 900 loss: 0.8752213191986083
LOSS train 0.87522 valid 0.96964, valid PER 30.86%
EPOCH 13:
  batch 50 loss: 0.7878674697875977
  batch 100 loss: 0.8152166414260864
  batch 150 loss: 0.7639165592193603
  batch 200 loss: 0.8101133930683136
  batch 250 loss: 0.8067940986156463
  batch 300 loss: 0.8115349125862121
  batch 350 loss: 0.82318204164505
  batch 400 loss: 0.8498783445358277
  batch 450 loss: 0.8297929906845093
  batch 500 loss: 0.7897896373271942
  batch 550 loss: 0.831651097536087
  batch 600 loss: 0.8201316964626312
  batch 650 loss: 0.8463774085044861
  batch 700 loss: 0.8610494720935822
  batch 750 loss: 0.8327330553531647
  batch 800 loss: 0.8147627830505371
  batch 850 loss: 0.8883560812473297
  batch 900 loss: 0.8546896731853485
LOSS train 0.85469 valid 0.98702, valid PER 30.14%
EPOCH 14:
  batch 50 loss: 0.8066989147663116
  batch 100 loss: 0.7927652788162232
  batch 150 loss: 0.8025898480415344
  batch 200 loss: 0.8087148976325989
  batch 250 loss: 0.7973613607883453
  batch 300 loss: 0.8407264983654023
  batch 350 loss: 0.7944511473178864
  batch 400 loss: 0.8025070571899414
  batch 450 loss: 0.800701732635498
  batch 500 loss: 0.8070101356506347
  batch 550 loss: 0.823294861316681
  batch 600 loss: 0.8045656663179398
  batch 650 loss: 0.8250710546970368
  batch 700 loss: 0.8485958290100097
  batch 750 loss: 0.8037320780754089
  batch 800 loss: 0.7873542964458465
  batch 850 loss: 0.847971498966217
  batch 900 loss: 0.8336116874217987
LOSS train 0.83361 valid 0.98405, valid PER 30.00%
EPOCH 15:
  batch 50 loss: 0.7816447478532791
  batch 100 loss: 0.7900797313451767
  batch 150 loss: 0.7859489762783051
  batch 200 loss: 0.85135897397995
  batch 250 loss: 0.8030178904533386
  batch 300 loss: 0.7761447489261627
  batch 350 loss: 0.7885137158632278
  batch 400 loss: 0.7577056300640106
  batch 450 loss: 0.7816220843791961
  batch 500 loss: 0.7492942070960998
  batch 550 loss: 0.7982709181308746
  batch 600 loss: 0.8212820446491241
  batch 650 loss: 0.8229286801815033
  batch 700 loss: 0.8118012714385986
  batch 750 loss: 0.8162045526504517
  batch 800 loss: 0.7703057944774627
  batch 850 loss: 0.7636473453044892
  batch 900 loss: 0.8039053082466125
LOSS train 0.80391 valid 0.98792, valid PER 31.01%
EPOCH 16:
  batch 50 loss: 0.7906848371028901
  batch 100 loss: 0.7234889453649521
  batch 150 loss: 0.7582111757993698
  batch 200 loss: 0.7498830115795135
  batch 250 loss: 0.7649943065643311
  batch 300 loss: 0.7601974046230316
  batch 350 loss: 0.7760444897413253
  batch 400 loss: 0.8185825312137603
  batch 450 loss: 0.8095196175575257
  batch 500 loss: 0.743935923576355
  batch 550 loss: 0.7626564764976501
  batch 600 loss: 0.7688369423151016
  batch 650 loss: 0.8024651026725769
  batch 700 loss: 0.7590493094921112
  batch 750 loss: 0.7807368195056915
  batch 800 loss: 0.7899992716312408
  batch 850 loss: 0.772428674697876
  batch 900 loss: 0.7684336054325104
LOSS train 0.76843 valid 0.97232, valid PER 29.53%
EPOCH 17:
  batch 50 loss: 0.7466219055652619
  batch 100 loss: 0.7664863330125808
  batch 150 loss: 0.71958241045475
  batch 200 loss: 0.7140509021282196
  batch 250 loss: 0.7511040663719177
  batch 300 loss: 0.7556691670417786
  batch 350 loss: 0.7329705202579498
  batch 400 loss: 0.8070482265949249
  batch 450 loss: 0.9524181044101715
  batch 500 loss: 0.8370043635368347
  batch 550 loss: 0.8289166796207428
  batch 600 loss: 0.8659363341331482
  batch 650 loss: 0.810956414937973
  batch 700 loss: 0.8116733992099762
  batch 750 loss: 0.754498016834259
  batch 800 loss: 0.7650189828872681
  batch 850 loss: 0.7793025946617127
  batch 900 loss: 0.7383604300022125
LOSS train 0.73836 valid 0.95665, valid PER 29.22%
EPOCH 18:
  batch 50 loss: 0.7141787481307983
  batch 100 loss: 0.7882909965515137
  batch 150 loss: 0.763791537284851
  batch 200 loss: 0.7382873845100403
  batch 250 loss: 0.7533498120307922
  batch 300 loss: 0.7336571753025055
  batch 350 loss: 0.7848504555225372
  batch 400 loss: 0.7866763687133789
  batch 450 loss: 0.8137510371208191
  batch 500 loss: 0.7630041897296905
  batch 550 loss: 0.7719352650642395
  batch 600 loss: 0.745033569931984
  batch 650 loss: 0.7396843194961548
  batch 700 loss: 0.7809714603424073
  batch 750 loss: 0.7524321925640106
  batch 800 loss: 0.7480711913108826
  batch 850 loss: 0.7599227583408356
  batch 900 loss: 0.8307603871822358
LOSS train 0.83076 valid 0.98921, valid PER 30.82%
EPOCH 19:
  batch 50 loss: 0.7193756830692292
  batch 100 loss: 0.7037180632352829
  batch 150 loss: 0.730779657959938
  batch 200 loss: 0.7288982945680619
  batch 250 loss: 0.7650314205884934
  batch 300 loss: 0.7595638597011566
  batch 350 loss: 0.7477058374881744
  batch 400 loss: 0.7520354497432709
  batch 450 loss: 0.7816157662868499
  batch 500 loss: 0.7420414340496063
  batch 550 loss: 0.7554949617385864
  batch 600 loss: 0.7649842917919158
  batch 650 loss: 0.8286708903312683
  batch 700 loss: 0.7705110627412796
  batch 750 loss: 0.7570356827974319
  batch 800 loss: 0.7666614389419556
  batch 850 loss: 0.7612296271324158
  batch 900 loss: 0.7461523187160491
LOSS train 0.74615 valid 0.98701, valid PER 29.86%
EPOCH 20:
  batch 50 loss: 0.6877964586019516
  batch 100 loss: 0.7057435297966004
  batch 150 loss: 0.7431892943382263
  batch 200 loss: 0.7431301391124725
  batch 250 loss: 0.718517581820488
  batch 300 loss: 0.7442897808551788
  batch 350 loss: 0.7116277605295181
  batch 400 loss: 0.720489135980606
  batch 450 loss: 0.7312905818223954
  batch 500 loss: 0.707978128194809
  batch 550 loss: 0.8119370031356812
  batch 600 loss: 0.7243999993801117
  batch 650 loss: 0.7455935037136078
  batch 700 loss: 0.8158535444736481
  batch 750 loss: 0.770177059173584
  batch 800 loss: 0.7946696758270264
  batch 850 loss: 0.7630895376205444
  batch 900 loss: 0.7487522089481353
LOSS train 0.74875 valid 0.98598, valid PER 28.95%
Training finished in 8.0 minutes.
Model saved to checkpoints/20230116_113350/model_17
Loading model from checkpoints/20230116_113350/model_17
SUB: 17.06%, DEL: 12.10%, INS: 2.32%, COR: 70.85%, PER: 31.48%
