Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.133849577903748
  batch 100 loss: 3.1959411573410033
  batch 150 loss: 3.0379239416122434
  batch 200 loss: 2.9123885345458986
  batch 250 loss: 2.8547452163696287
  batch 300 loss: 2.6707307958602904
  batch 350 loss: 2.5132702445983885
  batch 400 loss: 2.441065912246704
  batch 450 loss: 2.4076789808273316
  batch 500 loss: 2.289995505809784
  batch 550 loss: 2.5253437423706053
  batch 600 loss: 2.3342107915878296
  batch 650 loss: 2.260927004814148
  batch 700 loss: 2.1259989404678343
  batch 750 loss: 2.007565426826477
  batch 800 loss: 1.9448031187057495
  batch 850 loss: 1.8622747898101806
  batch 900 loss: 1.8603847360610961
LOSS train 1.86038 valid 1.80516, valid PER 62.08%
EPOCH 2:
  batch 50 loss: 1.7759011054039002
  batch 100 loss: 1.7022744464874267
  batch 150 loss: 1.7175258374214173
  batch 200 loss: 1.7120010018348695
  batch 250 loss: 1.7131710529327393
  batch 300 loss: 1.6515479588508606
  batch 350 loss: 1.5713018107414245
  batch 400 loss: 1.5802444195747376
  batch 450 loss: 1.5367982053756715
  batch 500 loss: 1.5674703192710877
  batch 550 loss: 1.5706882691383361
  batch 600 loss: 1.4929558324813843
  batch 650 loss: 1.5167603135108947
  batch 700 loss: 1.5077848720550537
  batch 750 loss: 1.4966416716575623
  batch 800 loss: 1.4318696355819702
  batch 850 loss: 1.4337963342666626
  batch 900 loss: 1.463791584968567
LOSS train 1.46379 valid 1.42647, valid PER 46.12%
EPOCH 3:
  batch 50 loss: 1.4198275995254517
  batch 100 loss: 1.3858262991905212
  batch 150 loss: 1.392463641166687
  batch 200 loss: 1.3745692014694213
  batch 250 loss: 1.34675523519516
  batch 300 loss: 1.333823456764221
  batch 350 loss: 1.3860369658470153
  batch 400 loss: 1.3678619861602783
  batch 450 loss: 1.3371821761131286
  batch 500 loss: 1.3098947954177858
  batch 550 loss: 1.324918212890625
  batch 600 loss: 1.3021606373786927
  batch 650 loss: 1.2656073355674744
  batch 700 loss: 1.3039733254909516
  batch 750 loss: 1.3267478489875792
  batch 800 loss: 1.259710807800293
  batch 850 loss: 1.296151728630066
  batch 900 loss: 1.2411771774291993
LOSS train 1.24118 valid 1.29974, valid PER 38.47%
EPOCH 4:
  batch 50 loss: 1.212458972930908
  batch 100 loss: 1.240635347366333
  batch 150 loss: 1.1759660959243774
  batch 200 loss: 1.2285060572624207
  batch 250 loss: 1.236238032579422
  batch 300 loss: 1.250302871465683
  batch 350 loss: 1.1512810504436493
  batch 400 loss: 1.2126597130298615
  batch 450 loss: 1.180037568807602
  batch 500 loss: 1.1584849536418915
  batch 550 loss: 1.1983098232746123
  batch 600 loss: 1.2052204525470733
  batch 650 loss: 1.203736366033554
  batch 700 loss: 1.1617473804950713
  batch 750 loss: 1.1406946849822999
  batch 800 loss: 1.10504603266716
  batch 850 loss: 1.1414240968227387
  batch 900 loss: 1.2103629076480866
LOSS train 1.21036 valid 1.20734, valid PER 35.93%
EPOCH 5:
  batch 50 loss: 1.1047414529323578
  batch 100 loss: 1.0917360234260558
  batch 150 loss: 1.1473919796943663
  batch 200 loss: 1.0776919949054717
  batch 250 loss: 1.085160617828369
  batch 300 loss: 1.0750945210456848
  batch 350 loss: 1.0961971139907838
  batch 400 loss: 1.1209648287296294
  batch 450 loss: 1.0967904353141784
  batch 500 loss: 1.1188787448406219
  batch 550 loss: 1.0424665677547456
  batch 600 loss: 1.1307609915733337
  batch 650 loss: 1.0715129554271698
  batch 700 loss: 1.1145714235305786
  batch 750 loss: 1.0479978096485139
  batch 800 loss: 1.0704357707500458
  batch 850 loss: 1.0724099016189574
  batch 900 loss: 1.080199213027954
LOSS train 1.08020 valid 1.11452, valid PER 34.25%
EPOCH 6:
  batch 50 loss: 1.071514936685562
  batch 100 loss: 0.9967706191539765
  batch 150 loss: 0.9945888304710389
  batch 200 loss: 1.0192586040496827
  batch 250 loss: 1.0753415548801422
  batch 300 loss: 1.0210367298126222
  batch 350 loss: 1.0182772493362426
  batch 400 loss: 1.011240222454071
  batch 450 loss: 1.041786985397339
  batch 500 loss: 1.0233265614509583
  batch 550 loss: 1.0427461874485016
  batch 600 loss: 1.0025475454330444
  batch 650 loss: 1.0373761057853699
  batch 700 loss: 1.0352114462852477
  batch 750 loss: 1.0114543008804322
  batch 800 loss: 1.014839859008789
  batch 850 loss: 0.9920210123062134
  batch 900 loss: 1.03237309217453
LOSS train 1.03237 valid 1.10230, valid PER 33.80%
EPOCH 7:
  batch 50 loss: 0.9851861715316772
  batch 100 loss: 0.983803448677063
  batch 150 loss: 0.9736671733856201
  batch 200 loss: 0.9496204674243927
  batch 250 loss: 0.9475367629528045
  batch 300 loss: 0.9546445417404175
  batch 350 loss: 0.9646917581558228
  batch 400 loss: 0.9807989299297333
  batch 450 loss: 0.960025360584259
  batch 500 loss: 0.9606514549255372
  batch 550 loss: 0.9911800026893616
  batch 600 loss: 0.9954060971736908
  batch 650 loss: 0.9393669807910919
  batch 700 loss: 0.981866021156311
  batch 750 loss: 0.9596276593208313
  batch 800 loss: 0.9384775817394256
  batch 850 loss: 0.9953076958656311
  batch 900 loss: 0.9915102386474609
LOSS train 0.99151 valid 1.05235, valid PER 32.42%
EPOCH 8:
  batch 50 loss: 0.9219875764846802
  batch 100 loss: 0.906867127418518
  batch 150 loss: 0.9165648400783539
  batch 200 loss: 0.8937307131290436
  batch 250 loss: 0.9080534791946411
  batch 300 loss: 0.8770617699623108
  batch 350 loss: 0.9547700786590576
  batch 400 loss: 0.9106940317153931
  batch 450 loss: 0.9325516664981842
  batch 500 loss: 0.9622158026695251
  batch 550 loss: 0.894346581697464
  batch 600 loss: 0.9330757892131806
  batch 650 loss: 0.9734316265583038
  batch 700 loss: 0.9121460390090942
  batch 750 loss: 0.9132488000392914
  batch 800 loss: 0.9489470338821411
  batch 850 loss: 0.9306367075443268
  batch 900 loss: 0.9434751355648041
LOSS train 0.94348 valid 1.04785, valid PER 31.68%
EPOCH 9:
  batch 50 loss: 0.843652503490448
  batch 100 loss: 0.8833332622051239
  batch 150 loss: 0.8961496090888977
  batch 200 loss: 0.8356022346019745
  batch 250 loss: 0.8890258300304413
  batch 300 loss: 0.9074879229068756
  batch 350 loss: 0.896172798871994
  batch 400 loss: 0.8844367516040802
  batch 450 loss: 0.8982476782798767
  batch 500 loss: 0.855690462589264
  batch 550 loss: 0.8846139860153198
  batch 600 loss: 0.9183236932754517
  batch 650 loss: 0.8818401491641998
  batch 700 loss: 0.8618801510334015
  batch 750 loss: 0.9034761643409729
  batch 800 loss: 0.9176735436916351
  batch 850 loss: 0.9141543781757355
  batch 900 loss: 0.8795865190029144
LOSS train 0.87959 valid 1.01531, valid PER 30.62%
EPOCH 10:
  batch 50 loss: 0.8107244312763214
  batch 100 loss: 0.8378412806987763
  batch 150 loss: 0.8664608418941497
  batch 200 loss: 0.8488354194164276
  batch 250 loss: 0.8690666425228118
  batch 300 loss: 0.8174614000320435
  batch 350 loss: 0.8634712016582489
  batch 400 loss: 0.8022877657413483
  batch 450 loss: 0.8193131828308106
  batch 500 loss: 0.8508285498619079
  batch 550 loss: 0.8750052380561829
  batch 600 loss: 0.8535881781578064
  batch 650 loss: 0.8476434099674225
  batch 700 loss: 0.8698748171329498
  batch 750 loss: 0.8448338258266449
  batch 800 loss: 0.8658620631694793
  batch 850 loss: 0.8637909495830536
  batch 900 loss: 0.8696322703361511
LOSS train 0.86963 valid 1.01158, valid PER 31.30%
EPOCH 11:
  batch 50 loss: 0.7818903577327728
  batch 100 loss: 0.7493034660816192
  batch 150 loss: 0.7839258754253388
  batch 200 loss: 0.8394061368703842
  batch 250 loss: 0.8136059719324112
  batch 300 loss: 0.7840858054161072
  batch 350 loss: 0.814486391544342
  batch 400 loss: 0.841855125427246
  batch 450 loss: 0.8456145012378693
  batch 500 loss: 0.815295090675354
  batch 550 loss: 0.8232692766189575
  batch 600 loss: 0.7988034904003143
  batch 650 loss: 0.8744620406627654
  batch 700 loss: 0.8157506036758423
  batch 750 loss: 0.8130865764617919
  batch 800 loss: 0.8582477700710297
  batch 850 loss: 0.874745500087738
  batch 900 loss: 0.8549151182174682
LOSS train 0.85492 valid 0.99612, valid PER 30.15%
EPOCH 12:
  batch 50 loss: 0.7827704215049743
  batch 100 loss: 0.7656481504440308
  batch 150 loss: 0.7444750893115998
  batch 200 loss: 0.7620278054475784
  batch 250 loss: 0.7869327974319458
  batch 300 loss: 0.7937058019638061
  batch 350 loss: 0.7737818276882171
  batch 400 loss: 0.8199452471733093
  batch 450 loss: 0.8267261159420013
  batch 500 loss: 0.8113892757892609
  batch 550 loss: 0.7667407834529877
  batch 600 loss: 0.7740521502494812
  batch 650 loss: 0.8117378711700439
  batch 700 loss: 0.808460858464241
  batch 750 loss: 0.7927755534648895
  batch 800 loss: 0.7889340746402741
  batch 850 loss: 0.8245261657238007
  batch 900 loss: 0.8327262145280838
LOSS train 0.83273 valid 0.97531, valid PER 29.54%
EPOCH 13:
  batch 50 loss: 0.7463253104686737
  batch 100 loss: 0.7645127272605896
  batch 150 loss: 0.743080267906189
  batch 200 loss: 0.7669381427764893
  batch 250 loss: 0.7667937576770782
  batch 300 loss: 0.7472016739845276
  batch 350 loss: 0.7463432395458222
  batch 400 loss: 0.7803701162338257
  batch 450 loss: 0.7747106224298477
  batch 500 loss: 0.7344375330209733
  batch 550 loss: 0.7873968720436096
  batch 600 loss: 0.7726947677135467
  batch 650 loss: 0.7768285846710206
  batch 700 loss: 0.8003630286455155
  batch 750 loss: 0.7509883487224579
  batch 800 loss: 0.769108213186264
  batch 850 loss: 0.7972457182407379
  batch 900 loss: 0.7857419633865357
LOSS train 0.78574 valid 0.99595, valid PER 28.84%
EPOCH 14:
  batch 50 loss: 0.7050120103359222
  batch 100 loss: 0.7223247182369232
  batch 150 loss: 0.7369262486696243
  batch 200 loss: 0.724289516210556
  batch 250 loss: 0.7370598697662354
  batch 300 loss: 0.7611659324169159
  batch 350 loss: 0.7072096252441407
  batch 400 loss: 0.7220721310377121
  batch 450 loss: 0.7393814069032669
  batch 500 loss: 0.7470333015918732
  batch 550 loss: 0.7676768809556961
  batch 600 loss: 0.7232504343986511
  batch 650 loss: 0.7525980699062348
  batch 700 loss: 0.801949679851532
  batch 750 loss: 0.7281877744197846
  batch 800 loss: 0.7071841847896576
  batch 850 loss: 0.7623630833625793
  batch 900 loss: 0.756167688369751
LOSS train 0.75617 valid 0.98161, valid PER 29.11%
EPOCH 15:
  batch 50 loss: 0.6970442551374435
  batch 100 loss: 0.6950383085012436
  batch 150 loss: 0.685771392583847
  batch 200 loss: 0.7310406363010407
  batch 250 loss: 0.7214142107963561
  batch 300 loss: 0.706435633301735
  batch 350 loss: 0.7028578650951386
  batch 400 loss: 0.7082268631458283
  batch 450 loss: 0.7117724597454071
  batch 500 loss: 0.6953480672836304
  batch 550 loss: 0.7189684212207794
  batch 600 loss: 0.7362660396099091
  batch 650 loss: 0.7478087496757507
  batch 700 loss: 0.7534907603263855
  batch 750 loss: 0.7472035312652587
  batch 800 loss: 0.7468583786487579
  batch 850 loss: 0.7147402620315552
  batch 900 loss: 0.7252076989412308
LOSS train 0.72521 valid 0.98776, valid PER 28.96%
EPOCH 16:
  batch 50 loss: 0.6962938171625137
  batch 100 loss: 0.6631927919387818
  batch 150 loss: 0.6572279131412506
  batch 200 loss: 0.6755684357881546
  batch 250 loss: 0.7043954366445542
  batch 300 loss: 0.6993666696548462
  batch 350 loss: 0.7142767143249512
  batch 400 loss: 0.7121210235357285
  batch 450 loss: 0.7177859854698181
  batch 500 loss: 0.6710187584161759
  batch 550 loss: 0.7330567967891694
  batch 600 loss: 0.7096408081054687
  batch 650 loss: 0.7147694391012192
  batch 700 loss: 0.7022557055950165
  batch 750 loss: 0.715698568224907
  batch 800 loss: 0.7252518701553344
  batch 850 loss: 0.7011316484212875
  batch 900 loss: 0.7148775827884674
LOSS train 0.71488 valid 0.97128, valid PER 28.28%
EPOCH 17:
  batch 50 loss: 0.6695464026927948
  batch 100 loss: 0.662460566163063
  batch 150 loss: 0.6446633875370026
  batch 200 loss: 0.6493330198526382
  batch 250 loss: 0.6762206900119782
  batch 300 loss: 0.6805329829454422
  batch 350 loss: 0.6816526967287063
  batch 400 loss: 0.7440278762578965
  batch 450 loss: 0.7183682942390441
  batch 500 loss: 0.7078582978248597
  batch 550 loss: 0.7103105223178864
  batch 600 loss: 0.7333205449581146
  batch 650 loss: 0.693453084230423
  batch 700 loss: 0.7035197657346726
  batch 750 loss: 0.6771000427007675
  batch 800 loss: 0.6866178131103515
  batch 850 loss: 0.7119901376962662
  batch 900 loss: 0.6858956438302993
LOSS train 0.68590 valid 0.98289, valid PER 28.44%
EPOCH 18:
  batch 50 loss: 0.6246940445899963
  batch 100 loss: 0.6521241050958634
  batch 150 loss: 0.6751853060722351
  batch 200 loss: 0.6807394939661026
  batch 250 loss: 0.670419117808342
  batch 300 loss: 0.6430456292629242
  batch 350 loss: 0.6525139904022217
  batch 400 loss: 0.6496487736701966
  batch 450 loss: 0.689933431148529
  batch 500 loss: 0.6589608007669449
  batch 550 loss: 0.6718125289678574
  batch 600 loss: 0.6306315159797669
  batch 650 loss: 0.6650812768936157
  batch 700 loss: 0.6949766373634338
  batch 750 loss: 0.6625953966379166
  batch 800 loss: 0.6583753389120102
  batch 850 loss: 0.6647040611505508
  batch 900 loss: 0.7103030860424042
LOSS train 0.71030 valid 0.98190, valid PER 28.35%
EPOCH 19:
  batch 50 loss: 0.6047514814138413
  batch 100 loss: 0.5889314675331115
  batch 150 loss: 0.6179939812421799
  batch 200 loss: 0.6176007187366486
  batch 250 loss: 0.6229963290691376
  batch 300 loss: 0.6312676268815994
  batch 350 loss: 0.633110174536705
  batch 400 loss: 0.6481812065839767
  batch 450 loss: 0.6780426841974259
  batch 500 loss: 0.6630337989330292
  batch 550 loss: 0.6328222280740738
  batch 600 loss: 0.6421474915742874
  batch 650 loss: 0.6996730577945709
  batch 700 loss: 0.642637545466423
  batch 750 loss: 0.65111414372921
  batch 800 loss: 0.6829119372367859
  batch 850 loss: 0.6700570911169053
  batch 900 loss: 0.6689703631401062
LOSS train 0.66897 valid 0.99903, valid PER 28.07%
EPOCH 20:
  batch 50 loss: 0.5981840693950653
  batch 100 loss: 0.6077054649591446
  batch 150 loss: 0.61591443836689
  batch 200 loss: 0.6385124939680099
  batch 250 loss: 0.6222971886396408
  batch 300 loss: 0.6474275654554367
  batch 350 loss: 0.5965290528535843
  batch 400 loss: 0.6079969823360443
  batch 450 loss: 0.6053679084777832
  batch 500 loss: 0.5987416511774063
  batch 550 loss: 0.6551507222652435
  batch 600 loss: 0.6102088516950608
  batch 650 loss: 0.6413517659902572
  batch 700 loss: 0.6458857649564743
  batch 750 loss: 0.6128163874149323
  batch 800 loss: 0.6485898584127426
  batch 850 loss: 0.6511209225654602
  batch 900 loss: 0.6689262360334396
LOSS train 0.66893 valid 1.00251, valid PER 28.23%
Training finished in 8.0 minutes.
Model saved to checkpoints/20230116_113425/model_16
Loading model from checkpoints/20230116_113425/model_16
SUB: 16.59%, DEL: 10.47%, INS: 2.64%, COR: 72.95%, PER: 29.70%
