Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.5)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 3.9942547845840455
  batch 100 loss: 3.1507644510269164
  batch 150 loss: 3.0433089447021486
  batch 200 loss: 2.8896573543548585
  batch 250 loss: 2.8030317068099975
  batch 300 loss: 2.657514576911926
  batch 350 loss: 2.582391786575317
  batch 400 loss: 2.4944913482666013
  batch 450 loss: 2.4237747097015383
  batch 500 loss: 2.4277510404586793
  batch 550 loss: 2.1543701839447023
  batch 600 loss: 2.0895299696922303
  batch 650 loss: 1.973466763496399
  batch 700 loss: 1.981754961013794
  batch 750 loss: 1.9058431315422057
  batch 800 loss: 1.8663768243789673
  batch 850 loss: 1.808398780822754
  batch 900 loss: 1.7912874484062196
LOSS train 1.79129 valid 1.83404, valid PER 71.52%
EPOCH 2:
  batch 50 loss: 1.741127405166626
  batch 100 loss: 1.6876928472518922
  batch 150 loss: 1.6575358486175538
  batch 200 loss: 1.6678669047355652
  batch 250 loss: 1.6683599710464478
  batch 300 loss: 1.6213670253753663
  batch 350 loss: 1.5202494215965272
  batch 400 loss: 1.5486782264709473
  batch 450 loss: 1.4998266196250916
  batch 500 loss: 1.5250310921669006
  batch 550 loss: 1.5124976634979248
  batch 600 loss: 1.4594100952148437
  batch 650 loss: 1.4568436813354493
  batch 700 loss: 1.453772463798523
  batch 750 loss: 1.405150818824768
  batch 800 loss: 1.3698866653442383
  batch 850 loss: 1.3588933157920837
  batch 900 loss: 1.376167621612549
LOSS train 1.37617 valid 1.34886, valid PER 42.23%
EPOCH 3:
  batch 50 loss: 1.3121697533130645
  batch 100 loss: 1.2996531867980956
  batch 150 loss: 1.3152762269973755
  batch 200 loss: 1.2708843803405763
  batch 250 loss: 1.2756506705284119
  batch 300 loss: 1.241759330034256
  batch 350 loss: 1.3067413115501403
  batch 400 loss: 1.267714993953705
  batch 450 loss: 1.242792967557907
  batch 500 loss: 1.2348667454719544
  batch 550 loss: 1.2599418842792511
  batch 600 loss: 1.252005054950714
  batch 650 loss: 1.1997179436683654
  batch 700 loss: 1.232446823120117
  batch 750 loss: 1.2742297220230103
  batch 800 loss: 1.1928232073783875
  batch 850 loss: 1.2281884801387788
  batch 900 loss: 1.1540494418144227
LOSS train 1.15405 valid 1.27212, valid PER 37.74%
EPOCH 4:
  batch 50 loss: 1.1735610246658326
  batch 100 loss: 1.1889703929424287
  batch 150 loss: 1.134458464384079
  batch 200 loss: 1.1663850164413452
  batch 250 loss: 1.1603104424476625
  batch 300 loss: 1.1567419564723969
  batch 350 loss: 1.1012165534496308
  batch 400 loss: 1.143637284040451
  batch 450 loss: 1.130442750453949
  batch 500 loss: 1.1092215800285339
  batch 550 loss: 1.1472787272930145
  batch 600 loss: 1.1494080030918121
  batch 650 loss: 1.1599680411815643
  batch 700 loss: 1.1343796932697296
  batch 750 loss: 1.1133488965034486
  batch 800 loss: 1.0620124125480652
  batch 850 loss: 1.1129398441314697
  batch 900 loss: 1.1487616729736327
LOSS train 1.14876 valid 1.12114, valid PER 34.53%
EPOCH 5:
  batch 50 loss: 1.0386878156661987
  batch 100 loss: 1.0481249618530273
  batch 150 loss: 1.1059617602825165
  batch 200 loss: 1.034123693704605
  batch 250 loss: 1.0562595307826996
  batch 300 loss: 1.0505346143245697
  batch 350 loss: 1.0454379546642303
  batch 400 loss: 1.0636922740936279
  batch 450 loss: 1.0700651228427887
  batch 500 loss: 1.0668853151798248
  batch 550 loss: 1.022790093421936
  batch 600 loss: 1.0884589040279389
  batch 650 loss: 1.0619617879390717
  batch 700 loss: 1.0812327909469603
  batch 750 loss: 1.0206698644161225
  batch 800 loss: 1.0564318406581878
  batch 850 loss: 1.074081609249115
  batch 900 loss: 1.0550168526172639
LOSS train 1.05502 valid 1.06516, valid PER 33.56%
EPOCH 6:
  batch 50 loss: 1.0223451113700868
  batch 100 loss: 0.9866236591339111
  batch 150 loss: 0.9739985191822051
  batch 200 loss: 1.0066509771347045
  batch 250 loss: 1.030771633386612
  batch 300 loss: 1.0134605443477631
  batch 350 loss: 1.0188300180435181
  batch 400 loss: 1.011624482870102
  batch 450 loss: 1.0565090811252593
  batch 500 loss: 1.0312274944782258
  batch 550 loss: 1.0519390285015107
  batch 600 loss: 1.0156713795661927
  batch 650 loss: 1.0146776473522185
  batch 700 loss: 1.0133880531787873
  batch 750 loss: 1.0045266938209534
  batch 800 loss: 1.0188999128341676
  batch 850 loss: 0.9846729516983033
  batch 900 loss: 1.0239580869674683
LOSS train 1.02396 valid 1.07448, valid PER 33.29%
EPOCH 7:
  batch 50 loss: 0.9745926868915558
  batch 100 loss: 0.9983375227451324
  batch 150 loss: 0.9874149823188781
  batch 200 loss: 0.9706621098518372
  batch 250 loss: 0.961950341463089
  batch 300 loss: 0.9299109661579132
  batch 350 loss: 0.9667303061485291
  batch 400 loss: 0.9711377263069153
  batch 450 loss: 0.9721224069595337
  batch 500 loss: 0.9648891294002533
  batch 550 loss: 0.9604757249355316
  batch 600 loss: 0.9824064898490906
  batch 650 loss: 0.9590691721439362
  batch 700 loss: 0.9631622135639191
  batch 750 loss: 0.9244952630996705
  batch 800 loss: 0.9435834562778473
  batch 850 loss: 0.9644205749034882
  batch 900 loss: 1.001071845293045
LOSS train 1.00107 valid 1.02574, valid PER 32.26%
EPOCH 8:
  batch 50 loss: 0.9163024926185608
  batch 100 loss: 0.909210364818573
  batch 150 loss: 0.9191782152652741
  batch 200 loss: 0.9019514739513397
  batch 250 loss: 0.9260733425617218
  batch 300 loss: 0.8744317567348481
  batch 350 loss: 0.9334871864318848
  batch 400 loss: 0.9038495028018951
  batch 450 loss: 0.9317271375656128
  batch 500 loss: 0.9601852691173554
  batch 550 loss: 0.8999486756324768
  batch 600 loss: 0.9439075291156769
  batch 650 loss: 0.9817761945724487
  batch 700 loss: 0.9198664629459381
  batch 750 loss: 0.9246703898906707
  batch 800 loss: 0.9131579959392547
  batch 850 loss: 0.9175555968284607
  batch 900 loss: 0.9194657754898071
LOSS train 0.91947 valid 1.01149, valid PER 30.94%
EPOCH 9:
  batch 50 loss: 0.8411539804935455
  batch 100 loss: 0.9009511363506317
  batch 150 loss: 0.8961622273921966
  batch 200 loss: 0.8547953784465789
  batch 250 loss: 0.8955276262760162
  batch 300 loss: 0.8986927700042725
  batch 350 loss: 0.9367522287368775
  batch 400 loss: 0.910910451412201
  batch 450 loss: 0.8959036231040954
  batch 500 loss: 0.8664053857326508
  batch 550 loss: 0.9097387170791627
  batch 600 loss: 0.9203115999698639
  batch 650 loss: 0.896392662525177
  batch 700 loss: 0.8799787342548371
  batch 750 loss: 0.8937833714485168
  batch 800 loss: 0.9106070113182068
  batch 850 loss: 0.9054756939411164
  batch 900 loss: 0.8770911669731141
LOSS train 0.87709 valid 1.01808, valid PER 31.37%
EPOCH 10:
  batch 50 loss: 0.8177327561378479
  batch 100 loss: 0.9009280157089233
  batch 150 loss: 0.9226384568214416
  batch 200 loss: 0.906919790506363
  batch 250 loss: 0.9084601378440857
  batch 300 loss: 0.8420990979671479
  batch 350 loss: 0.8996041333675384
  batch 400 loss: 0.8497016298770904
  batch 450 loss: 0.8512619030475617
  batch 500 loss: 0.9116530644893647
  batch 550 loss: 0.895043363571167
  batch 600 loss: 0.8603053069114686
  batch 650 loss: 0.8589117038249969
  batch 700 loss: 0.8761621689796448
  batch 750 loss: 0.863003830909729
  batch 800 loss: 0.8817764270305634
  batch 850 loss: 0.9078225302696228
  batch 900 loss: 0.8858285582065583
LOSS train 0.88583 valid 1.01929, valid PER 32.28%
EPOCH 11:
  batch 50 loss: 0.8208973610401153
  batch 100 loss: 0.808750398159027
  batch 150 loss: 0.8244412684440613
  batch 200 loss: 0.873133819103241
  batch 250 loss: 0.8651790177822113
  batch 300 loss: 0.824059909582138
  batch 350 loss: 0.8335750710964203
  batch 400 loss: 0.8639014863967895
  batch 450 loss: 0.8597986650466919
  batch 500 loss: 0.8418209731578827
  batch 550 loss: 0.8622466516494751
  batch 600 loss: 0.8247839713096619
  batch 650 loss: 0.9125648677349091
  batch 700 loss: 0.8397205376625061
  batch 750 loss: 0.8584774816036225
  batch 800 loss: 0.8756572103500366
  batch 850 loss: 0.8999683785438538
  batch 900 loss: 0.8798021507263184
LOSS train 0.87980 valid 0.96868, valid PER 30.53%
EPOCH 12:
  batch 50 loss: 0.8164263689517974
  batch 100 loss: 0.8346164202690125
  batch 150 loss: 0.8199556124210358
  batch 200 loss: 0.8043369257450104
  batch 250 loss: 0.8322511124610901
  batch 300 loss: 0.8067627096176148
  batch 350 loss: 0.8240896916389465
  batch 400 loss: 0.8353176057338715
  batch 450 loss: 0.8401606559753418
  batch 500 loss: 0.887206689119339
  batch 550 loss: 0.7978662300109863
  batch 600 loss: 0.8130300927162171
  batch 650 loss: 0.8524971175193786
  batch 700 loss: 0.8423859608173371
  batch 750 loss: 0.8163785123825074
  batch 800 loss: 0.8337291872501373
  batch 850 loss: 0.8765242755413055
  batch 900 loss: 0.8393186700344085
LOSS train 0.83932 valid 0.95827, valid PER 30.45%
EPOCH 13:
  batch 50 loss: 0.7928274703025818
  batch 100 loss: 0.7994966459274292
  batch 150 loss: 0.7556503635644912
  batch 200 loss: 0.7961167740821838
  batch 250 loss: 0.7897789603471757
  batch 300 loss: 0.7710611510276795
  batch 350 loss: 0.8167944359779358
  batch 400 loss: 0.7940472537279128
  batch 450 loss: 0.7890126240253449
  batch 500 loss: 0.7844632875919342
  batch 550 loss: 0.8366517496109008
  batch 600 loss: 0.8143191957473754
  batch 650 loss: 0.8194128626585007
  batch 700 loss: 0.8096574413776397
  batch 750 loss: 0.7716454780101776
  batch 800 loss: 0.797594518661499
  batch 850 loss: 0.8202532386779785
  batch 900 loss: 0.8209286725521088
LOSS train 0.82093 valid 0.97906, valid PER 30.50%
EPOCH 14:
  batch 50 loss: 0.7877873730659485
  batch 100 loss: 0.7860674989223481
  batch 150 loss: 0.7811370015144348
  batch 200 loss: 0.7849710714817048
  batch 250 loss: 0.7855321687459945
  batch 300 loss: 0.8105552220344543
  batch 350 loss: 0.7661164343357086
  batch 400 loss: 0.7718000555038452
  batch 450 loss: 0.7759962558746338
  batch 500 loss: 0.7944148457050324
  batch 550 loss: 0.8129096841812133
  batch 600 loss: 0.7788681226968766
  batch 650 loss: 0.8120541799068451
  batch 700 loss: 0.8532258653640747
  batch 750 loss: 0.7846622669696808
  batch 800 loss: 0.7528217124938965
  batch 850 loss: 0.8151424527168274
  batch 900 loss: 0.8260852885246277
LOSS train 0.82609 valid 0.97876, valid PER 30.37%
EPOCH 15:
  batch 50 loss: 0.7824397552013397
  batch 100 loss: 0.7415960377454758
  batch 150 loss: 0.7542706453800201
  batch 200 loss: 0.7999169254302978
  batch 250 loss: 0.7904264008998871
  batch 300 loss: 0.7393937134742736
  batch 350 loss: 0.7485302513837815
  batch 400 loss: 0.7597498524188996
  batch 450 loss: 0.7714552319049836
  batch 500 loss: 0.7269335067272187
  batch 550 loss: 0.7732677018642425
  batch 600 loss: 0.804575937986374
  batch 650 loss: 0.8165568494796753
  batch 700 loss: 0.7913748228549957
  batch 750 loss: 0.7956907153129578
  batch 800 loss: 0.7969377434253693
  batch 850 loss: 0.7702641713619233
  batch 900 loss: 0.7775123602151871
LOSS train 0.77751 valid 0.98470, valid PER 30.49%
EPOCH 16:
  batch 50 loss: 0.7699018454551697
  batch 100 loss: 0.7306352698802948
  batch 150 loss: 0.7301144182682038
  batch 200 loss: 0.7293445086479187
  batch 250 loss: 0.7437207877635956
  batch 300 loss: 0.7466226196289063
  batch 350 loss: 0.7494456899166108
  batch 400 loss: 0.7508310532569885
  batch 450 loss: 0.7605589437484741
  batch 500 loss: 0.7162960338592529
  batch 550 loss: 0.747936818599701
  batch 600 loss: 0.7428534317016602
  batch 650 loss: 0.7578749942779541
  batch 700 loss: 0.743183388710022
  batch 750 loss: 0.7469738912582398
  batch 800 loss: 0.7682557034492493
  batch 850 loss: 0.7617184484004974
  batch 900 loss: 0.7455075132846832
LOSS train 0.74551 valid 0.93611, valid PER 28.78%
EPOCH 17:
  batch 50 loss: 0.7218557626008988
  batch 100 loss: 0.7333089244365693
  batch 150 loss: 0.7296303701400757
  batch 200 loss: 0.7164249587059021
  batch 250 loss: 0.7503568184375763
  batch 300 loss: 0.7380341351032257
  batch 350 loss: 0.7362598991394043
  batch 400 loss: 0.7837998533248901
  batch 450 loss: 0.776087988615036
  batch 500 loss: 0.74805266559124
  batch 550 loss: 0.7611123234033584
  batch 600 loss: 0.7734843611717224
  batch 650 loss: 0.7611157822608948
  batch 700 loss: 0.7288084942102432
  batch 750 loss: 0.7204452455043793
  batch 800 loss: 0.7413859927654266
  batch 850 loss: 0.7419477421045303
  batch 900 loss: 0.8081577169895172
LOSS train 0.80816 valid 1.00620, valid PER 30.42%
EPOCH 18:
  batch 50 loss: 0.7879667961597443
  batch 100 loss: 0.7675162148475647
  batch 150 loss: 0.7673315054178238
  batch 200 loss: 0.7579838067293168
  batch 250 loss: 0.7389836680889129
  batch 300 loss: 0.7374347090721131
  batch 350 loss: 0.74517036318779
  batch 400 loss: 0.7180905383825302
  batch 450 loss: 0.7701056748628616
  batch 500 loss: 0.7605506038665771
  batch 550 loss: 0.8255999505519866
  batch 600 loss: 0.8113385343551636
  batch 650 loss: 0.7472618460655213
  batch 700 loss: 0.7807610857486725
  batch 750 loss: 0.7584039175510406
  batch 800 loss: 0.7518254762887955
  batch 850 loss: 0.754539144039154
  batch 900 loss: 0.7933992564678192
LOSS train 0.79340 valid 0.95198, valid PER 29.28%
EPOCH 19:
  batch 50 loss: 0.6690590608119965
  batch 100 loss: 0.6906126976013184
  batch 150 loss: 0.6920244079828263
  batch 200 loss: 0.7084234535694123
  batch 250 loss: 0.7403569877147674
  batch 300 loss: 0.7449532222747802
  batch 350 loss: 0.7107099378108979
  batch 400 loss: 0.7125753557682037
  batch 450 loss: 0.7304969269037247
  batch 500 loss: 0.7368481528759002
  batch 550 loss: 0.7448631250858306
  batch 600 loss: 0.7255330866575241
  batch 650 loss: 0.7984438574314118
  batch 700 loss: 0.7498529481887818
  batch 750 loss: 0.7229596501588822
  batch 800 loss: 0.7755713391304017
  batch 850 loss: 0.772488494515419
  batch 900 loss: 0.7460021638870239
LOSS train 0.74600 valid 0.96427, valid PER 29.44%
EPOCH 20:
  batch 50 loss: 0.7053330790996551
  batch 100 loss: 0.6993848204612731
  batch 150 loss: 0.6730330842733383
  batch 200 loss: 0.688644849061966
  batch 250 loss: 0.6872974288463592
  batch 300 loss: 0.7389539945125579
  batch 350 loss: 0.6850930690765381
  batch 400 loss: 0.7489037704467774
  batch 450 loss: 0.7201166915893554
  batch 500 loss: 0.7101345372200012
  batch 550 loss: 0.7706494510173798
  batch 600 loss: 0.6953101396560669
  batch 650 loss: 0.7374051582813262
  batch 700 loss: 0.7300434976816177
  batch 750 loss: 0.7043241196870804
  batch 800 loss: 0.7567377114295959
  batch 850 loss: 0.7570985317230224
  batch 900 loss: 0.7348428910970688
LOSS train 0.73484 valid 0.94360, valid PER 28.33%
Training finished in 6.0 minutes.
Model saved to checkpoints/20230116_113832/model_16
Loading model from checkpoints/20230116_113832/model_16
SUB: 17.27%, DEL: 11.14%, INS: 2.36%, COR: 71.60%, PER: 30.77%
