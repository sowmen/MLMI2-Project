Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.2)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.025111970901489
  batch 100 loss: 3.1563077116012574
  batch 150 loss: 3.020949993133545
  batch 200 loss: 2.8870388507843017
  batch 250 loss: 2.835932688713074
  batch 300 loss: 2.8655839109420778
  batch 350 loss: 2.5803283929824827
  batch 400 loss: 2.4470336055755615
  batch 450 loss: 2.3226242589950563
  batch 500 loss: 2.2063287830352785
  batch 550 loss: 2.126694130897522
  batch 600 loss: 2.069789729118347
  batch 650 loss: 1.9897172379493713
  batch 700 loss: 1.9783805084228516
  batch 750 loss: 1.9134768009185792
  batch 800 loss: 1.883978908061981
  batch 850 loss: 1.824102373123169
  batch 900 loss: 1.821579134464264
LOSS train 1.82158 valid 1.77528, valid PER 68.40%
EPOCH 2:
  batch 50 loss: 1.7725981235504151
  batch 100 loss: 1.7378944659233093
  batch 150 loss: 1.622185432910919
  batch 200 loss: 1.651753032207489
  batch 250 loss: 1.6416615700721742
  batch 300 loss: 1.6075594639778137
  batch 350 loss: 1.6061922931671142
  batch 400 loss: 1.556416506767273
  batch 450 loss: 1.5580679726600648
  batch 500 loss: 1.525827510356903
  batch 550 loss: 1.5062297010421752
  batch 600 loss: 1.509538595676422
  batch 650 loss: 1.4375898218154908
  batch 700 loss: 1.4749669742584228
  batch 750 loss: 1.41858567237854
  batch 800 loss: 1.3723601627349853
  batch 850 loss: 1.4040756225585938
  batch 900 loss: 1.3520925426483155
LOSS train 1.35209 valid 1.35024, valid PER 43.73%
EPOCH 3:
  batch 50 loss: 1.298007252216339
  batch 100 loss: 1.3472308504581452
  batch 150 loss: 1.351362133026123
  batch 200 loss: 1.2508873319625855
  batch 250 loss: 1.3216087198257447
  batch 300 loss: 1.3078923845291137
  batch 350 loss: 1.3245238780975341
  batch 400 loss: 1.2699657893180847
  batch 450 loss: 1.2480551517009735
  batch 500 loss: 1.2318590784072876
  batch 550 loss: 1.2720728945732116
  batch 600 loss: 1.1959530329704284
  batch 650 loss: 1.2344024407863616
  batch 700 loss: 1.2348199713230132
  batch 750 loss: 1.2587201714515686
  batch 800 loss: 1.242118901014328
  batch 850 loss: 1.206119600534439
  batch 900 loss: 1.2211634421348572
LOSS train 1.22116 valid 1.18443, valid PER 38.59%
EPOCH 4:
  batch 50 loss: 1.1944842886924745
  batch 100 loss: 1.1294322550296783
  batch 150 loss: 1.166422600746155
  batch 200 loss: 1.1410288834571838
  batch 250 loss: 1.155262600183487
  batch 300 loss: 1.144634417295456
  batch 350 loss: 1.1564957356452943
  batch 400 loss: 1.0910302996635437
  batch 450 loss: 1.1317608094215392
  batch 500 loss: 1.16109565615654
  batch 550 loss: 1.106637773513794
  batch 600 loss: 1.0830022716522216
  batch 650 loss: 1.186180213689804
  batch 700 loss: 1.214174292087555
  batch 750 loss: 1.1283617472648622
  batch 800 loss: 1.1046793234348298
  batch 850 loss: 1.1154624617099762
  batch 900 loss: 1.1241125357151032
LOSS train 1.12411 valid 1.16733, valid PER 36.78%
EPOCH 5:
  batch 50 loss: 1.0891354382038116
  batch 100 loss: 1.059344173669815
  batch 150 loss: 1.0975117778778076
  batch 200 loss: 1.112131381034851
  batch 250 loss: 1.0771351110935212
  batch 300 loss: 1.0878973758220674
  batch 350 loss: 1.0674099540710449
  batch 400 loss: 1.1465707671642305
  batch 450 loss: 1.0671849608421327
  batch 500 loss: 1.0554780745506287
  batch 550 loss: 1.075586267709732
  batch 600 loss: 1.1104407036304473
  batch 650 loss: 1.0809612476825714
  batch 700 loss: 1.0660738968849182
  batch 750 loss: 1.0925154900550842
  batch 800 loss: 1.145740466117859
  batch 850 loss: 1.0952078104019165
  batch 900 loss: 1.0712726974487305
LOSS train 1.07127 valid 1.12560, valid PER 34.86%
EPOCH 6:
  batch 50 loss: 1.0450407099723815
  batch 100 loss: 1.0642294681072235
  batch 150 loss: 1.0422473239898682
  batch 200 loss: 1.007697674036026
  batch 250 loss: 1.0378951478004455
  batch 300 loss: 1.0540675795078278
  batch 350 loss: 1.033981819152832
  batch 400 loss: 1.0138709580898284
  batch 450 loss: 1.032593342065811
  batch 500 loss: 0.9937241435050964
  batch 550 loss: 1.0236055767536163
  batch 600 loss: 0.9987155866622924
  batch 650 loss: 0.9705894720554352
  batch 700 loss: 0.9741042137145997
  batch 750 loss: 1.0070268750190734
  batch 800 loss: 1.0178084778785705
  batch 850 loss: 1.0329728376865388
  batch 900 loss: 1.0279464316368103
LOSS train 1.02795 valid 1.05548, valid PER 33.38%
EPOCH 7:
  batch 50 loss: 0.9631454300880432
  batch 100 loss: 1.0255763733386993
  batch 150 loss: 0.9566587126255035
  batch 200 loss: 0.952513473033905
  batch 250 loss: 0.9892148923873901
  batch 300 loss: 0.9716114330291749
  batch 350 loss: 1.0116959118843079
  batch 400 loss: 0.9646584177017212
  batch 450 loss: 0.9758089077472687
  batch 500 loss: 0.9847673678398132
  batch 550 loss: 0.9559368681907654
  batch 600 loss: 0.9793436789512634
  batch 650 loss: 0.9592021012306213
  batch 700 loss: 0.994311866760254
  batch 750 loss: 0.9435952699184418
  batch 800 loss: 0.9608828020095825
  batch 850 loss: 0.9666729736328125
  batch 900 loss: 0.9642527747154236
LOSS train 0.96425 valid 1.03281, valid PER 33.13%
EPOCH 8:
  batch 50 loss: 0.946224535703659
  batch 100 loss: 0.9118653547763824
  batch 150 loss: 0.9470082116127014
  batch 200 loss: 0.9288082253932953
  batch 250 loss: 0.9214568555355072
  batch 300 loss: 0.894758905172348
  batch 350 loss: 0.9217744994163514
  batch 400 loss: 0.914852705001831
  batch 450 loss: 0.96594229221344
  batch 500 loss: 0.9438456523418427
  batch 550 loss: 0.9130465853214264
  batch 600 loss: 0.9221028530597687
  batch 650 loss: 0.915369701385498
  batch 700 loss: 0.9317145872116089
  batch 750 loss: 0.9372202372550964
  batch 800 loss: 0.9329399752616883
  batch 850 loss: 0.9201007020473481
  batch 900 loss: 0.9249720919132233
LOSS train 0.92497 valid 1.03312, valid PER 31.30%
EPOCH 9:
  batch 50 loss: 0.9040644991397858
  batch 100 loss: 0.8570387637615204
  batch 150 loss: 0.8707491362094879
  batch 200 loss: 0.8431857049465179
  batch 250 loss: 0.86380375623703
  batch 300 loss: 0.8773923873901367
  batch 350 loss: 0.8575459134578705
  batch 400 loss: 0.9088027656078339
  batch 450 loss: 0.9116799914836884
  batch 500 loss: 0.8797883987426758
  batch 550 loss: 0.8930249214172363
  batch 600 loss: 0.9467594146728515
  batch 650 loss: 0.9100774621963501
  batch 700 loss: 0.9003630876541138
  batch 750 loss: 0.921714323759079
  batch 800 loss: 0.9274175524711609
  batch 850 loss: 0.9267268502712249
  batch 900 loss: 0.89279256939888
LOSS train 0.89279 valid 0.99853, valid PER 30.24%
EPOCH 10:
  batch 50 loss: 0.8398493361473084
  batch 100 loss: 0.8548481750488282
  batch 150 loss: 0.911656721830368
  batch 200 loss: 0.8445961380004883
  batch 250 loss: 0.8637328827381134
  batch 300 loss: 0.8578264951705933
  batch 350 loss: 0.8815953969955445
  batch 400 loss: 0.8494477951526642
  batch 450 loss: 0.8492057693004608
  batch 500 loss: 0.8869014954566956
  batch 550 loss: 1.0432518994808198
  batch 600 loss: 0.9643054640293122
  batch 650 loss: 0.934226690530777
  batch 700 loss: 0.9441246378421784
  batch 750 loss: 0.95359654545784
  batch 800 loss: 0.9329469764232635
  batch 850 loss: 0.9249267089366913
  batch 900 loss: 0.9085607504844666
LOSS train 0.90856 valid 1.01042, valid PER 32.04%
EPOCH 11:
  batch 50 loss: 0.8613033294677734
  batch 100 loss: 0.8534984481334686
  batch 150 loss: 0.8599875831604004
  batch 200 loss: 0.8318354284763336
  batch 250 loss: 0.8584772682189942
  batch 300 loss: 0.8609279644489288
  batch 350 loss: 0.8978383111953735
  batch 400 loss: 0.8501942408084869
  batch 450 loss: 0.8384693777561187
  batch 500 loss: 0.838056378364563
  batch 550 loss: 0.8748061811923981
  batch 600 loss: 0.8696138286590576
  batch 650 loss: 0.910756278038025
  batch 700 loss: 0.9533694219589234
  batch 750 loss: 0.8625347471237182
  batch 800 loss: 0.8817492938041687
  batch 850 loss: 0.8806913280487061
  batch 900 loss: 0.8815669906139374
LOSS train 0.88157 valid 0.98210, valid PER 31.00%
EPOCH 12:
  batch 50 loss: 0.8036346077919007
  batch 100 loss: 0.7975372457504273
  batch 150 loss: 0.8182178008556366
  batch 200 loss: 0.8489511966705322
  batch 250 loss: 0.813557094335556
  batch 300 loss: 0.8579718565940857
  batch 350 loss: 0.8248964750766754
  batch 400 loss: 0.8599642407894135
  batch 450 loss: 0.8523957431316376
  batch 500 loss: 0.8702002811431885
  batch 550 loss: 0.8742267376184464
  batch 600 loss: 1.0279424345493318
  batch 650 loss: 1.1818063390254974
  batch 700 loss: 1.0573956739902497
  batch 750 loss: 1.051551603078842
  batch 800 loss: 0.9461239707469941
  batch 850 loss: 0.9717664968967438
  batch 900 loss: 0.9932555711269379
LOSS train 0.99326 valid 1.07783, valid PER 32.75%
EPOCH 13:
  batch 50 loss: 0.9615722250938415
  batch 100 loss: 0.9470998108386993
  batch 150 loss: 0.9514363729953765
  batch 200 loss: 0.8935250127315522
  batch 250 loss: 0.9265312504768372
  batch 300 loss: 0.9804661095142364
  batch 350 loss: 0.8989138305187225
  batch 400 loss: 0.9175683069229126
  batch 450 loss: 0.9326419758796692
  batch 500 loss: 0.9082090842723847
  batch 550 loss: 0.9730907428264618
  batch 600 loss: 0.9150249552726746
  batch 650 loss: 0.9204973125457764
  batch 700 loss: 0.9280502247810364
  batch 750 loss: 0.8791447412967682
  batch 800 loss: 0.9191751313209534
  batch 850 loss: 0.903787260055542
  batch 900 loss: 0.8830201482772827
LOSS train 0.88302 valid 0.98205, valid PER 30.34%
EPOCH 14:
  batch 50 loss: 0.8531864893436432
  batch 100 loss: 0.8316197252273559
  batch 150 loss: 0.8484269380569458
  batch 200 loss: 0.8382987534999847
  batch 250 loss: 0.8303589963912964
  batch 300 loss: 0.8256401515007019
  batch 350 loss: 0.8446598744392395
  batch 400 loss: 0.9186860001087189
  batch 450 loss: 0.856014347076416
  batch 500 loss: 0.8625419008731842
  batch 550 loss: 0.8593679583072662
  batch 600 loss: 0.8762195706367493
  batch 650 loss: 0.8722663581371307
  batch 700 loss: 0.8781693482398987
  batch 750 loss: 0.8616953122615815
  batch 800 loss: 0.8512987840175629
  batch 850 loss: 0.8998892736434937
  batch 900 loss: 0.8756185239553451
LOSS train 0.87562 valid 0.97136, valid PER 30.08%
EPOCH 15:
  batch 50 loss: 0.7769067752361297
  batch 100 loss: 0.8181992149353028
  batch 150 loss: 0.7819368517398835
  batch 200 loss: 0.8339276289939881
  batch 250 loss: 0.8349138212203979
  batch 300 loss: 0.8216586792469025
  batch 350 loss: 0.8152533400058747
  batch 400 loss: 0.8160244703292847
  batch 450 loss: 0.9467980694770813
  batch 500 loss: 0.8342621099948883
  batch 550 loss: 0.886120411157608
  batch 600 loss: 0.9051360642910004
  batch 650 loss: 0.8271286475658417
  batch 700 loss: 0.8966280269622803
  batch 750 loss: 0.9152452957630157
  batch 800 loss: 0.8848158931732177
  batch 850 loss: 0.877050861120224
  batch 900 loss: 0.8279647862911225
LOSS train 0.82796 valid 1.01331, valid PER 32.04%
EPOCH 16:
  batch 50 loss: 0.8453953874111175
  batch 100 loss: 0.8440690803527832
  batch 150 loss: 0.9156567394733429
  batch 200 loss: 0.8944856405258179
  batch 250 loss: 0.8614605414867401
  batch 300 loss: 0.8938695025444031
  batch 350 loss: 0.8894901132583618
  batch 400 loss: 0.8420262539386749
  batch 450 loss: 0.8628901755809784
  batch 500 loss: 0.8500007474422455
  batch 550 loss: 0.8557681965827942
  batch 600 loss: 0.8872911024093628
  batch 650 loss: 0.8624517226219177
  batch 700 loss: 0.8238134157657623
  batch 750 loss: 0.8383440709114075
  batch 800 loss: 0.8454109740257263
  batch 850 loss: 0.826863602399826
  batch 900 loss: 0.8428627574443817
LOSS train 0.84286 valid 1.00247, valid PER 31.38%
EPOCH 17:
  batch 50 loss: 0.836295200586319
  batch 100 loss: 0.7836501359939575
  batch 150 loss: 0.8372743690013885
  batch 200 loss: 0.7995357882976531
  batch 250 loss: 0.8394369900226593
  batch 300 loss: 0.8191547024250031
  batch 350 loss: 0.8249788796901703
  batch 400 loss: 0.8431742393970489
  batch 450 loss: 0.8176281952857971
  batch 500 loss: 0.8275821185112
  batch 550 loss: 0.8323686993122101
  batch 600 loss: 0.836246782541275
  batch 650 loss: 0.7871047365665436
  batch 700 loss: 0.8044052290916442
  batch 750 loss: 0.8026824176311493
  batch 800 loss: 0.9218941974639893
  batch 850 loss: 0.860934602022171
  batch 900 loss: 0.9172462165355683
LOSS train 0.91725 valid 0.99773, valid PER 29.77%
EPOCH 18:
  batch 50 loss: 0.8075175523757935
  batch 100 loss: 0.7999460959434509
  batch 150 loss: 0.8419126784801483
  batch 200 loss: 0.7880494451522827
  batch 250 loss: 0.7846958673000336
  batch 300 loss: 0.7811054134368897
  batch 350 loss: 0.7931192344427109
  batch 400 loss: 0.8168865895271301
  batch 450 loss: 0.8421948266029358
  batch 500 loss: 0.8742642760276794
  batch 550 loss: 0.8859730362892151
  batch 600 loss: 0.8684290134906769
  batch 650 loss: 0.8170014452934266
  batch 700 loss: 0.7876302337646485
  batch 750 loss: 0.8203149223327637
  batch 800 loss: 0.8403716909885407
  batch 850 loss: 0.8174513792991638
  batch 900 loss: 0.8053860664367676
LOSS train 0.80539 valid 0.95598, valid PER 29.64%
EPOCH 19:
  batch 50 loss: 0.7820689427852631
  batch 100 loss: 0.8204882204532623
  batch 150 loss: 0.8051944172382355
  batch 200 loss: 0.7953014922142029
  batch 250 loss: 0.8767954444885254
  batch 300 loss: 0.8348565036058426
  batch 350 loss: 0.8441309010982514
  batch 400 loss: 0.8028978252410889
  batch 450 loss: 0.7504142928123474
  batch 500 loss: 0.7615849220752716
  batch 550 loss: 0.8383518147468567
  batch 600 loss: 0.8085450732707977
  batch 650 loss: 0.8409630155563355
  batch 700 loss: 0.8447666442394257
  batch 750 loss: 0.7717915832996368
  batch 800 loss: 0.766956832408905
  batch 850 loss: 0.7924827265739441
  batch 900 loss: 0.771438113451004
LOSS train 0.77144 valid 0.93721, valid PER 28.88%
EPOCH 20:
  batch 50 loss: 0.7093583744764328
  batch 100 loss: 0.7500754821300507
  batch 150 loss: 0.7632272231578827
  batch 200 loss: 0.7520402002334595
  batch 250 loss: 0.7694334840774536
  batch 300 loss: 0.7596725517511368
  batch 350 loss: 0.7568588650226593
  batch 400 loss: 0.7676100665330887
  batch 450 loss: 0.7528637290000916
  batch 500 loss: 0.7914114820957184
  batch 550 loss: 0.751378767490387
  batch 600 loss: 0.7490913248062134
  batch 650 loss: 0.7913890492916107
  batch 700 loss: 0.7622717547416688
  batch 750 loss: 0.7936260771751403
  batch 800 loss: 0.8059987378120422
  batch 850 loss: 0.7878655129671097
  batch 900 loss: 0.7681469309329987
LOSS train 0.76815 valid 0.94886, valid PER 29.28%
Training finished in 6.0 minutes.
Model saved to checkpoints/20230116_114501/model_19
Loading model from checkpoints/20230116_114501/model_19
SUB: 16.97%, DEL: 10.59%, INS: 2.61%, COR: 72.43%, PER: 30.18%
