Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.3)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.194703054428101
  batch 100 loss: 3.3091007566452024
  batch 150 loss: 3.2921880054473878
  batch 200 loss: 3.116001696586609
  batch 250 loss: 2.8825282621383668
  batch 300 loss: 2.6846849298477173
  batch 350 loss: 2.5269844818115232
  batch 400 loss: 2.45925030708313
  batch 450 loss: 2.4195855474472046
  batch 500 loss: 2.266890301704407
  batch 550 loss: 2.2141040062904356
  batch 600 loss: 2.1363983941078186
  batch 650 loss: 2.074939715862274
  batch 700 loss: 2.0450080966949464
  batch 750 loss: 1.977778959274292
  batch 800 loss: 1.9044526505470276
  batch 850 loss: 1.8759491086006164
  batch 900 loss: 1.857779574394226
LOSS train 1.85778 valid 1.75711, valid PER 67.26%
EPOCH 2:
  batch 50 loss: 1.7995314717292785
  batch 100 loss: 1.754390389919281
  batch 150 loss: 1.638248565196991
  batch 200 loss: 1.6557040476799012
  batch 250 loss: 1.656810884475708
  batch 300 loss: 1.613507902622223
  batch 350 loss: 1.587807605266571
  batch 400 loss: 1.5598094415664674
  batch 450 loss: 1.5620853424072265
  batch 500 loss: 1.5210900354385375
  batch 550 loss: 1.5185766887664796
  batch 600 loss: 1.5070886063575744
  batch 650 loss: 1.4553440880775452
  batch 700 loss: 1.4756430244445802
  batch 750 loss: 1.4501097440719604
  batch 800 loss: 1.4089006352424622
  batch 850 loss: 1.4284824562072753
  batch 900 loss: 1.3828734636306763
LOSS train 1.38287 valid 1.41527, valid PER 45.26%
EPOCH 3:
  batch 50 loss: 1.3106305384635926
  batch 100 loss: 1.3914243292808532
  batch 150 loss: 1.3829027366638185
  batch 200 loss: 1.299151222705841
  batch 250 loss: 1.3150762403011322
  batch 300 loss: 1.2983602833747865
  batch 350 loss: 1.3259063720703126
  batch 400 loss: 1.2710337400436402
  batch 450 loss: 1.259726060628891
  batch 500 loss: 1.2284667658805848
  batch 550 loss: 1.2435972809791564
  batch 600 loss: 1.1858097004890442
  batch 650 loss: 1.2117192196846007
  batch 700 loss: 1.2164574122428895
  batch 750 loss: 1.262016385793686
  batch 800 loss: 1.2568794572353363
  batch 850 loss: 1.2194102120399475
  batch 900 loss: 1.2025019943714141
LOSS train 1.20250 valid 1.22345, valid PER 38.08%
EPOCH 4:
  batch 50 loss: 1.192898417711258
  batch 100 loss: 1.1248086071014405
  batch 150 loss: 1.173772202730179
  batch 200 loss: 1.1257746863365172
  batch 250 loss: 1.1513178312778474
  batch 300 loss: 1.151402405500412
  batch 350 loss: 1.159253807067871
  batch 400 loss: 1.1012808656692505
  batch 450 loss: 1.1264008283615112
  batch 500 loss: 1.1877732467651367
  batch 550 loss: 1.1250279200077058
  batch 600 loss: 1.0899182724952698
  batch 650 loss: 1.1596346652507783
  batch 700 loss: 1.1900750064849854
  batch 750 loss: 1.1201432538032532
  batch 800 loss: 1.090398461818695
  batch 850 loss: 1.101497266292572
  batch 900 loss: 1.1027204120159149
LOSS train 1.10272 valid 1.15407, valid PER 34.56%
EPOCH 5:
  batch 50 loss: 1.066129344701767
  batch 100 loss: 1.0608783531188966
  batch 150 loss: 1.0781928646564483
  batch 200 loss: 1.1015627431869506
  batch 250 loss: 1.042234274148941
  batch 300 loss: 1.076367974281311
  batch 350 loss: 1.0443440890312194
  batch 400 loss: 1.028085731267929
  batch 450 loss: 1.0285017156600953
  batch 500 loss: 1.0054394125938415
  batch 550 loss: 1.0693343353271485
  batch 600 loss: 1.0614400684833527
  batch 650 loss: 1.0700957465171814
  batch 700 loss: 1.0757263672351838
  batch 750 loss: 1.0431398570537567
  batch 800 loss: 1.080440285205841
  batch 850 loss: 1.0645870888233184
  batch 900 loss: 1.0393219697475433
LOSS train 1.03932 valid 1.08091, valid PER 33.86%
EPOCH 6:
  batch 50 loss: 1.0124051904678344
  batch 100 loss: 1.0149800717830657
  batch 150 loss: 1.0173589777946472
  batch 200 loss: 0.9941348874568939
  batch 250 loss: 1.024439960718155
  batch 300 loss: 1.0341407465934753
  batch 350 loss: 1.0394290041923524
  batch 400 loss: 0.9991832733154297
  batch 450 loss: 1.043191909790039
  batch 500 loss: 1.0051679253578185
  batch 550 loss: 1.0098251545429229
  batch 600 loss: 0.9962104547023773
  batch 650 loss: 0.9682645332813263
  batch 700 loss: 0.9687359762191773
  batch 750 loss: 1.0021019434928895
  batch 800 loss: 1.0025161790847779
  batch 850 loss: 1.020890817642212
  batch 900 loss: 1.0402342748641968
LOSS train 1.04023 valid 1.03590, valid PER 32.94%
EPOCH 7:
  batch 50 loss: 0.9320358312129975
  batch 100 loss: 1.0035325992107391
  batch 150 loss: 0.9360035002231598
  batch 200 loss: 0.9746667528152466
  batch 250 loss: 0.9767861127853393
  batch 300 loss: 0.9434106147289276
  batch 350 loss: 1.0011413645744325
  batch 400 loss: 0.948053286075592
  batch 450 loss: 0.969189544916153
  batch 500 loss: 0.9775721871852875
  batch 550 loss: 0.9583568882942199
  batch 600 loss: 0.9760744774341583
  batch 650 loss: 0.9672865629196167
  batch 700 loss: 0.986291047334671
  batch 750 loss: 0.9521482729911804
  batch 800 loss: 0.9571469712257386
  batch 850 loss: 0.9440145289897919
  batch 900 loss: 0.950021196603775
LOSS train 0.95002 valid 1.04605, valid PER 32.62%
EPOCH 8:
  batch 50 loss: 0.9460316121578216
  batch 100 loss: 0.892895188331604
  batch 150 loss: 0.9382978045940399
  batch 200 loss: 0.917154575586319
  batch 250 loss: 0.9052062869071961
  batch 300 loss: 0.8998093211650848
  batch 350 loss: 0.9235401308536529
  batch 400 loss: 0.9158324158191681
  batch 450 loss: 0.9760322082042694
  batch 500 loss: 0.9441951632499694
  batch 550 loss: 0.9177290099859238
  batch 600 loss: 0.883427484035492
  batch 650 loss: 0.9071100676059722
  batch 700 loss: 0.9213841056823731
  batch 750 loss: 0.9250745272636414
  batch 800 loss: 0.9278676295280457
  batch 850 loss: 0.9363471388816833
  batch 900 loss: 0.9090373170375824
LOSS train 0.90904 valid 1.01084, valid PER 31.56%
EPOCH 9:
  batch 50 loss: 0.8854480993747711
  batch 100 loss: 0.8384072983264923
  batch 150 loss: 0.8638873243331909
  batch 200 loss: 0.8695462954044342
  batch 250 loss: 0.8433993458747864
  batch 300 loss: 0.8852823436260223
  batch 350 loss: 0.8815661501884461
  batch 400 loss: 0.9094168531894684
  batch 450 loss: 0.9283847451210022
  batch 500 loss: 0.8994154858589173
  batch 550 loss: 0.9062390148639679
  batch 600 loss: 0.939929552078247
  batch 650 loss: 0.9102384877204895
  batch 700 loss: 0.8795053040981293
  batch 750 loss: 0.8645644950866699
  batch 800 loss: 0.9136354291439056
  batch 850 loss: 0.9104214823246002
  batch 900 loss: 0.8835551428794861
LOSS train 0.88356 valid 0.98942, valid PER 30.18%
EPOCH 10:
  batch 50 loss: 0.8328895187377929
  batch 100 loss: 0.8491173958778382
  batch 150 loss: 0.8794737434387208
  batch 200 loss: 0.8242420625686645
  batch 250 loss: 0.8392468082904816
  batch 300 loss: 0.8471739828586579
  batch 350 loss: 0.8409475386142731
  batch 400 loss: 0.8092689156532288
  batch 450 loss: 0.828727513551712
  batch 500 loss: 0.8498073434829712
  batch 550 loss: 0.880782995223999
  batch 600 loss: 0.8435195696353912
  batch 650 loss: 0.8655172169208527
  batch 700 loss: 0.9251237154006958
  batch 750 loss: 0.9172989797592163
  batch 800 loss: 0.9364474177360534
  batch 850 loss: 0.8968178296089172
  batch 900 loss: 0.863702448606491
LOSS train 0.86370 valid 1.01013, valid PER 31.71%
EPOCH 11:
  batch 50 loss: 0.8518767571449279
  batch 100 loss: 0.8051591408252716
  batch 150 loss: 0.8112677073478699
  batch 200 loss: 0.8186049914360046
  batch 250 loss: 0.832069708108902
  batch 300 loss: 0.8898121416568756
  batch 350 loss: 0.8941288816928864
  batch 400 loss: 0.8720824205875397
  batch 450 loss: 0.8834307229518891
  batch 500 loss: 0.8545497941970825
  batch 550 loss: 0.8737221598625183
  batch 600 loss: 0.8969261419773101
  batch 650 loss: 0.9108746612071991
  batch 700 loss: 0.9669413948059082
  batch 750 loss: 0.8982315289974213
  batch 800 loss: 0.8882350111007691
  batch 850 loss: 0.8732363963127137
  batch 900 loss: 0.9334568548202514
LOSS train 0.93346 valid 0.99230, valid PER 30.46%
EPOCH 12:
  batch 50 loss: 0.7985925483703613
  batch 100 loss: 0.7991187119483948
  batch 150 loss: 0.8847483170032501
  batch 200 loss: 0.8683528697490692
  batch 250 loss: 0.8309131062030792
  batch 300 loss: 0.8475513613224029
  batch 350 loss: 0.8532001137733459
  batch 400 loss: 0.8589821839332581
  batch 450 loss: 0.9600197219848633
  batch 500 loss: 0.9036381995677948
  batch 550 loss: 0.8769603145122528
  batch 600 loss: 0.8962137591838837
  batch 650 loss: 0.8650188529491425
  batch 700 loss: 0.8510057282447815
  batch 750 loss: 0.8870123136043548
  batch 800 loss: 0.8191022741794586
  batch 850 loss: 0.8498660540580749
  batch 900 loss: 0.8832078051567077
LOSS train 0.88321 valid 0.99709, valid PER 29.89%
EPOCH 13:
  batch 50 loss: 0.7844225680828094
  batch 100 loss: 0.8041092419624328
  batch 150 loss: 0.804855922460556
  batch 200 loss: 0.7703138947486877
  batch 250 loss: 0.7784001064300538
  batch 300 loss: 0.8441637861728668
  batch 350 loss: 0.7960205245018005
  batch 400 loss: 0.8170814561843872
  batch 450 loss: 0.8261913132667541
  batch 500 loss: 0.8281281125545502
  batch 550 loss: 0.885931134223938
  batch 600 loss: 0.8257367551326752
  batch 650 loss: 0.8217338943481445
  batch 700 loss: 0.8794586479663848
  batch 750 loss: 0.8311976850032806
  batch 800 loss: 0.8575650703907013
  batch 850 loss: 0.8175729966163635
  batch 900 loss: 0.8313556092977524
LOSS train 0.83136 valid 0.98143, valid PER 29.51%
EPOCH 14:
  batch 50 loss: 0.7726149147748947
  batch 100 loss: 0.7654166102409363
  batch 150 loss: 0.7761503148078919
  batch 200 loss: 0.7798934948444366
  batch 250 loss: 0.7877055561542511
  batch 300 loss: 0.8165266728401184
  batch 350 loss: 0.8353583562374115
  batch 400 loss: 0.8717859947681427
  batch 450 loss: 0.8173841834068298
  batch 500 loss: 0.7960008668899536
  batch 550 loss: 0.8093746948242188
  batch 600 loss: 0.7958813846111298
  batch 650 loss: 0.8057207906246185
  batch 700 loss: 0.8195399940013885
  batch 750 loss: 0.8038014948368073
  batch 800 loss: 0.7916191220283508
  batch 850 loss: 0.842680413722992
  batch 900 loss: 0.8345551699399948
LOSS train 0.83456 valid 0.97510, valid PER 30.14%
EPOCH 15:
  batch 50 loss: 0.7127269858121872
  batch 100 loss: 0.7542523729801178
  batch 150 loss: 0.7338429772853852
  batch 200 loss: 0.8016312181949615
  batch 250 loss: 0.7802291250228882
  batch 300 loss: 0.7850352644920349
  batch 350 loss: 0.7464750880002975
  batch 400 loss: 0.7975768971443177
  batch 450 loss: 0.8033885359764099
  batch 500 loss: 0.791466680765152
  batch 550 loss: 0.8885000050067902
  batch 600 loss: 0.8594489228725434
  batch 650 loss: 0.7753329503536225
  batch 700 loss: 0.7779625380039215
  batch 750 loss: 0.8171511673927307
  batch 800 loss: 0.8305614542961121
  batch 850 loss: 0.8081076180934906
  batch 900 loss: 0.7693623995780945
LOSS train 0.76936 valid 1.00750, valid PER 31.02%
EPOCH 16:
  batch 50 loss: 0.7684180551767349
  batch 100 loss: 0.7516509014368057
  batch 150 loss: 0.7615391433238983
  batch 200 loss: 0.7878333878517151
  batch 250 loss: 0.760555454492569
  batch 300 loss: 0.7780597805976868
  batch 350 loss: 0.7817275714874268
  batch 400 loss: 0.7578031075000763
  batch 450 loss: 0.807487518787384
  batch 500 loss: 0.7711343193054199
  batch 550 loss: 0.7536962080001831
  batch 600 loss: 0.7874638253450393
  batch 650 loss: 0.7874026417732238
  batch 700 loss: 0.7547471189498901
  batch 750 loss: 0.7861392450332642
  batch 800 loss: 0.7726277267932892
  batch 850 loss: 0.7566914081573486
  batch 900 loss: 0.7909305620193482
LOSS train 0.79093 valid 0.98322, valid PER 30.30%
EPOCH 17:
  batch 50 loss: 0.747485202550888
  batch 100 loss: 0.6855051302909851
  batch 150 loss: 0.7556338113546371
  batch 200 loss: 0.6918172264099121
  batch 250 loss: 0.7471481060981751
  batch 300 loss: 0.706854835152626
  batch 350 loss: 0.7588070440292358
  batch 400 loss: 0.7542617309093476
  batch 450 loss: 0.7323328280448913
  batch 500 loss: 0.7326398521661759
  batch 550 loss: 0.7372324585914611
  batch 600 loss: 0.7759502077102661
  batch 650 loss: 0.7287953579425812
  batch 700 loss: 0.7505514538288116
  batch 750 loss: 0.718304134607315
  batch 800 loss: 0.766729120016098
  batch 850 loss: 0.745150443315506
  batch 900 loss: 0.7619666665792465
LOSS train 0.76197 valid 0.96650, valid PER 29.18%
EPOCH 18:
  batch 50 loss: 0.6948483943939209
  batch 100 loss: 0.7155842500925064
  batch 150 loss: 0.7494090878963471
  batch 200 loss: 0.7181152522563934
  batch 250 loss: 0.6944129699468613
  batch 300 loss: 0.7137247908115387
  batch 350 loss: 0.7062462562322617
  batch 400 loss: 0.7163157939910889
  batch 450 loss: 0.7356191664934159
  batch 500 loss: 0.7508751320838928
  batch 550 loss: 0.8044458591938018
  batch 600 loss: 0.76627869784832
  batch 650 loss: 0.7298138785362244
  batch 700 loss: 0.7362658631801605
  batch 750 loss: 0.7665614211559295
  batch 800 loss: 0.7937648868560792
  batch 850 loss: 0.7988145720958709
  batch 900 loss: 0.7857826495170593
LOSS train 0.78578 valid 0.99078, valid PER 29.95%
EPOCH 19:
  batch 50 loss: 0.7268644112348557
  batch 100 loss: 0.7490725064277649
  batch 150 loss: 0.7142859315872192
  batch 200 loss: 0.7013269126415252
  batch 250 loss: 0.7364896070957184
  batch 300 loss: 0.7543774247169495
  batch 350 loss: 0.7568663668632507
  batch 400 loss: 0.7439719605445861
  batch 450 loss: 0.6866371870040894
  batch 500 loss: 0.7041637885570526
  batch 550 loss: 0.7527101665735245
  batch 600 loss: 0.7565566778182984
  batch 650 loss: 0.7617943572998047
  batch 700 loss: 0.8041241657733917
  batch 750 loss: 0.7331294202804566
  batch 800 loss: 0.7060848659276963
  batch 850 loss: 0.7260754573345184
  batch 900 loss: 0.7365938305854798
LOSS train 0.73659 valid 0.99636, valid PER 29.12%
EPOCH 20:
  batch 50 loss: 0.6548239016532897
  batch 100 loss: 0.6894863164424896
  batch 150 loss: 0.6825226706266403
  batch 200 loss: 0.6967279726266861
  batch 250 loss: 0.7229693049192428
  batch 300 loss: 0.7241295540332794
  batch 350 loss: 0.6956345421075821
  batch 400 loss: 0.7200533056259155
  batch 450 loss: 0.710868182182312
  batch 500 loss: 0.7686012744903564
  batch 550 loss: 0.7331050205230712
  batch 600 loss: 0.7071112817525864
  batch 650 loss: 0.7395683979988098
  batch 700 loss: 0.7198161840438843
  batch 750 loss: 0.7224479252099991
  batch 800 loss: 0.7756676918268204
  batch 850 loss: 0.7824845802783966
  batch 900 loss: 0.7374718964099884
LOSS train 0.73747 valid 1.04573, valid PER 30.16%
Training finished in 6.0 minutes.
Model saved to checkpoints/20230116_114824/model_17
Loading model from checkpoints/20230116_114824/model_17
SUB: 16.61%, DEL: 11.29%, INS: 2.52%, COR: 72.10%, PER: 30.42%
