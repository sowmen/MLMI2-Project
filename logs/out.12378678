Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 3.9092780494689943
  batch 100 loss: 3.12426326751709
  batch 150 loss: 3.012786831855774
  batch 200 loss: 2.880787000656128
  batch 250 loss: 2.813025803565979
  batch 300 loss: 2.7321130228042603
  batch 350 loss: 2.5387405729293824
  batch 400 loss: 2.4503454113006593
  batch 450 loss: 2.385413465499878
  batch 500 loss: 2.312602882385254
  batch 550 loss: 2.267549979686737
  batch 600 loss: 2.176903414726257
  batch 650 loss: 2.0573068189620973
  batch 700 loss: 2.015881555080414
  batch 750 loss: 2.039268383979797
  batch 800 loss: 1.9245341157913207
  batch 850 loss: 1.8454167413711549
  batch 900 loss: 1.8185293507575988
LOSS train 1.81853 valid 1.80007, valid PER 68.17%
EPOCH 2:
  batch 50 loss: 1.7836451649665832
  batch 100 loss: 1.7519169354438782
  batch 150 loss: 1.6220119071006776
  batch 200 loss: 1.6562751388549806
  batch 250 loss: 1.6335385394096376
  batch 300 loss: 1.6041072702407837
  batch 350 loss: 1.599676752090454
  batch 400 loss: 1.5527473568916321
  batch 450 loss: 1.547805025577545
  batch 500 loss: 1.5063616442680359
  batch 550 loss: 1.4813220024108886
  batch 600 loss: 1.4721874952316285
  batch 650 loss: 1.4125367021560669
  batch 700 loss: 1.4689699769020081
  batch 750 loss: 1.4098187255859376
  batch 800 loss: 1.3815301990509032
  batch 850 loss: 1.4173269271850586
  batch 900 loss: 1.3588729596138
LOSS train 1.35887 valid 1.35781, valid PER 43.74%
EPOCH 3:
  batch 50 loss: 1.289507610797882
  batch 100 loss: 1.356894235610962
  batch 150 loss: 1.3673933744430542
  batch 200 loss: 1.268448988199234
  batch 250 loss: 1.3467160511016845
  batch 300 loss: 1.322067723274231
  batch 350 loss: 1.329120945930481
  batch 400 loss: 1.2867988133430481
  batch 450 loss: 1.2716604351997376
  batch 500 loss: 1.250486469268799
  batch 550 loss: 1.2611634278297423
  batch 600 loss: 1.218037291765213
  batch 650 loss: 1.2372495198249818
  batch 700 loss: 1.2354697895050049
  batch 750 loss: 1.266582305431366
  batch 800 loss: 1.2578596270084381
  batch 850 loss: 1.212774577140808
  batch 900 loss: 1.2558441925048829
LOSS train 1.25584 valid 1.23206, valid PER 39.56%
EPOCH 4:
  batch 50 loss: 1.2251691162586211
  batch 100 loss: 1.1610619473457335
  batch 150 loss: 1.1942646181583405
  batch 200 loss: 1.163151513338089
  batch 250 loss: 1.184309378862381
  batch 300 loss: 1.1814569985866548
  batch 350 loss: 1.183784794807434
  batch 400 loss: 1.1189968597888946
  batch 450 loss: 1.167220116853714
  batch 500 loss: 1.214680461883545
  batch 550 loss: 1.1583021330833434
  batch 600 loss: 1.1051314771175385
  batch 650 loss: 1.2013870179653168
  batch 700 loss: 1.2185858523845672
  batch 750 loss: 1.1581086122989654
  batch 800 loss: 1.1266569113731384
  batch 850 loss: 1.1011149048805238
  batch 900 loss: 1.1273957395553589
LOSS train 1.12740 valid 1.17180, valid PER 36.28%
EPOCH 5:
  batch 50 loss: 1.086669360399246
  batch 100 loss: 1.0835310351848602
  batch 150 loss: 1.1411159598827363
  batch 200 loss: 1.14737242937088
  batch 250 loss: 1.1008069586753846
  batch 300 loss: 1.1457290613651276
  batch 350 loss: 1.0845014703273774
  batch 400 loss: 1.0729940485954286
  batch 450 loss: 1.056911174058914
  batch 500 loss: 1.0457254564762115
  batch 550 loss: 1.105215801000595
  batch 600 loss: 1.1185412001609802
  batch 650 loss: 1.0955639362335206
  batch 700 loss: 1.0753127145767212
  batch 750 loss: 1.0643179249763488
  batch 800 loss: 1.1075685560703277
  batch 850 loss: 1.101523405313492
  batch 900 loss: 1.0570296597480775
LOSS train 1.05703 valid 1.11154, valid PER 34.56%
EPOCH 6:
  batch 50 loss: 1.0382532918453216
  batch 100 loss: 1.0452933430671691
  batch 150 loss: 1.0403062522411346
  batch 200 loss: 1.0014302718639374
  batch 250 loss: 1.0515682482719422
  batch 300 loss: 1.0600865960121155
  batch 350 loss: 1.0728846645355226
  batch 400 loss: 1.0446780502796174
  batch 450 loss: 1.0712682402133942
  batch 500 loss: 0.9860855448246002
  batch 550 loss: 1.031816716194153
  batch 600 loss: 1.0254598820209504
  batch 650 loss: 1.035102504491806
  batch 700 loss: 1.022336826324463
  batch 750 loss: 1.046599704027176
  batch 800 loss: 1.027459623813629
  batch 850 loss: 1.1508069586753846
  batch 900 loss: 1.070281457901001
LOSS train 1.07028 valid 1.08554, valid PER 34.35%
EPOCH 7:
  batch 50 loss: 0.9684339952468872
  batch 100 loss: 1.0496573317050935
  batch 150 loss: 0.9636795890331268
  batch 200 loss: 0.9902537727355957
  batch 250 loss: 1.0189331662654877
  batch 300 loss: 0.9880373299121856
  batch 350 loss: 1.0806273424625397
  batch 400 loss: 1.0773992145061493
  batch 450 loss: 1.0311942207813263
  batch 500 loss: 1.0201173150539398
  batch 550 loss: 1.0291725409030914
  batch 600 loss: 1.0076441884040832
  batch 650 loss: 0.9743332517147064
  batch 700 loss: 1.016735017299652
  batch 750 loss: 0.9994427561759949
  batch 800 loss: 1.0052377414703368
  batch 850 loss: 1.0038075602054597
  batch 900 loss: 0.9727544248104095
LOSS train 0.97275 valid 1.07253, valid PER 33.41%
EPOCH 8:
  batch 50 loss: 0.9755538833141327
  batch 100 loss: 0.9463845276832581
  batch 150 loss: 1.0024637353420258
  batch 200 loss: 0.971453663110733
  batch 250 loss: 0.9652759289741516
  batch 300 loss: 0.9502160024642944
  batch 350 loss: 0.954558926820755
  batch 400 loss: 0.9601970565319061
  batch 450 loss: 1.010748234987259
  batch 500 loss: 0.9746397984027863
  batch 550 loss: 0.9663961410522461
  batch 600 loss: 0.9248525989055634
  batch 650 loss: 0.9445319378376007
  batch 700 loss: 0.968496881723404
  batch 750 loss: 0.9659719169139862
  batch 800 loss: 0.9714043986797333
  batch 850 loss: 0.9297899174690246
  batch 900 loss: 0.9385268127918244
LOSS train 0.93853 valid 1.05325, valid PER 32.46%
EPOCH 9:
  batch 50 loss: 0.9078149318695068
  batch 100 loss: 0.8923272526264191
  batch 150 loss: 0.9098796391487122
  batch 200 loss: 0.8935965311527252
  batch 250 loss: 0.8955555331707
  batch 300 loss: 0.9229541933536529
  batch 350 loss: 0.897448627948761
  batch 400 loss: 0.946645542383194
  batch 450 loss: 0.9520046770572662
  batch 500 loss: 0.923946681022644
  batch 550 loss: 0.9286495733261109
  batch 600 loss: 0.9504121577739716
  batch 650 loss: 0.9381794142723083
  batch 700 loss: 0.9082104468345642
  batch 750 loss: 0.9181279742717743
  batch 800 loss: 0.9481181132793427
  batch 850 loss: 0.9428529942035675
  batch 900 loss: 0.9006136679649352
LOSS train 0.90061 valid 1.01795, valid PER 30.92%
EPOCH 10:
  batch 50 loss: 0.855059050321579
  batch 100 loss: 0.8770350861549377
  batch 150 loss: 0.90879816532135
  batch 200 loss: 0.8691534948348999
  batch 250 loss: 0.8727686703205109
  batch 300 loss: 0.8813057565689086
  batch 350 loss: 0.883808685541153
  batch 400 loss: 0.921715087890625
  batch 450 loss: 0.8825617158412933
  batch 500 loss: 0.9005894243717194
  batch 550 loss: 0.8822421109676362
  batch 600 loss: 0.8786928415298462
  batch 650 loss: 0.9121811330318451
  batch 700 loss: 0.9251690781116486
  batch 750 loss: 0.9363716554641723
  batch 800 loss: 0.8944822573661804
  batch 850 loss: 0.8887246072292327
  batch 900 loss: 0.8904629361629486
LOSS train 0.89046 valid 1.01420, valid PER 31.99%
EPOCH 11:
  batch 50 loss: 0.8365295875072479
  batch 100 loss: 0.8329991257190704
  batch 150 loss: 0.8432714068889617
  batch 200 loss: 0.8205674719810486
  batch 250 loss: 0.8339370155334472
  batch 300 loss: 0.8470876002311707
  batch 350 loss: 0.9135018157958984
  batch 400 loss: 0.8752984929084778
  batch 450 loss: 1.0069466197490693
  batch 500 loss: 0.9622941875457763
  batch 550 loss: 0.9667598378658294
  batch 600 loss: 0.9095498979091644
  batch 650 loss: 0.9230356299877167
  batch 700 loss: 1.0061517584323882
  batch 750 loss: 0.9267016136646271
  batch 800 loss: 0.9775315356254578
  batch 850 loss: 0.9287826192378997
  batch 900 loss: 0.9546508944034576
LOSS train 0.95465 valid 1.03763, valid PER 32.02%
EPOCH 12:
  batch 50 loss: 0.8608354878425598
  batch 100 loss: 0.8519516122341156
  batch 150 loss: 0.8781939995288849
  batch 200 loss: 0.8931180620193482
  batch 250 loss: 0.8649307310581207
  batch 300 loss: 0.8879548275470733
  batch 350 loss: 0.8561420488357544
  batch 400 loss: 0.8971442592144012
  batch 450 loss: 0.8496639263629914
  batch 500 loss: 0.8669467306137085
  batch 550 loss: 0.8925129020214081
  batch 600 loss: 0.8899973714351654
  batch 650 loss: 0.8696017169952392
  batch 700 loss: 0.894360978603363
  batch 750 loss: 0.8978683066368103
  batch 800 loss: 0.8413112664222717
  batch 850 loss: 0.8820984601974488
  batch 900 loss: 0.8889382982254028
LOSS train 0.88894 valid 1.00027, valid PER 30.77%
EPOCH 13:
  batch 50 loss: 0.8171749222278595
  batch 100 loss: 0.8244479584693909
  batch 150 loss: 0.8392872440814972
  batch 200 loss: 0.806474004983902
  batch 250 loss: 0.8247394168376923
  batch 300 loss: 0.8573430979251861
  batch 350 loss: 0.8239366543293
  batch 400 loss: 0.8244928812980652
  batch 450 loss: 0.872875919342041
  batch 500 loss: 0.8232144284248352
  batch 550 loss: 0.9002902066707611
  batch 600 loss: 0.8429921400547028
  batch 650 loss: 0.8468447327613831
  batch 700 loss: 0.8635521554946899
  batch 750 loss: 0.8328888750076294
  batch 800 loss: 0.8591153132915497
  batch 850 loss: 0.842822072505951
  batch 900 loss: 0.8819372296333313
LOSS train 0.88194 valid 0.97937, valid PER 29.72%
EPOCH 14:
  batch 50 loss: 0.8165852379798889
  batch 100 loss: 0.8115086483955384
  batch 150 loss: 0.8381328582763672
  batch 200 loss: 0.8231068277359008
  batch 250 loss: 0.8306613659858704
  batch 300 loss: 0.8115223824977875
  batch 350 loss: 0.810465579032898
  batch 400 loss: 0.859463552236557
  batch 450 loss: 0.819310884475708
  batch 500 loss: 0.8182592046260834
  batch 550 loss: 0.851915066242218
  batch 600 loss: 0.8184866881370545
  batch 650 loss: 0.8299910342693329
  batch 700 loss: 0.9579562425613404
  batch 750 loss: 0.9358828413486481
  batch 800 loss: 0.8764794838428497
  batch 850 loss: 0.900102778673172
  batch 900 loss: 0.8612476217746735
LOSS train 0.86125 valid 1.01036, valid PER 31.00%
EPOCH 15:
  batch 50 loss: 0.7937834477424621
  batch 100 loss: 0.8059415876865387
  batch 150 loss: 0.7818568515777587
  batch 200 loss: 0.8174678122997284
  batch 250 loss: 0.8291023874282837
  batch 300 loss: 0.8200484573841095
  batch 350 loss: 0.8239648652076721
  batch 400 loss: 0.8489523828029633
  batch 450 loss: 0.8369823789596558
  batch 500 loss: 0.8335214227437973
  batch 550 loss: 0.9131970167160034
  batch 600 loss: 0.9252969598770142
  batch 650 loss: 0.8383461356163024
  batch 700 loss: 0.836192044019699
  batch 750 loss: 0.8715365719795227
  batch 800 loss: 0.8147720277309418
  batch 850 loss: 0.8220033025741578
  batch 900 loss: 0.8165184634923935
LOSS train 0.81652 valid 0.99522, valid PER 31.14%
EPOCH 16:
  batch 50 loss: 0.8175114238262177
  batch 100 loss: 0.801416552066803
  batch 150 loss: 0.8117382597923278
  batch 200 loss: 0.8598224759101868
  batch 250 loss: 0.8008190631866455
  batch 300 loss: 0.8654184615612031
  batch 350 loss: 0.8221889650821685
  batch 400 loss: 0.8075059759616852
  batch 450 loss: 0.8513477098941803
  batch 500 loss: 0.8310994720458984
  batch 550 loss: 0.8383715605735779
  batch 600 loss: 0.84396479845047
  batch 650 loss: 0.8636595404148102
  batch 700 loss: 0.8125102531909942
  batch 750 loss: 0.8224694585800171
  batch 800 loss: 0.8354886984825134
  batch 850 loss: 0.8543231165409089
  batch 900 loss: 0.869706552028656
LOSS train 0.86971 valid 1.03561, valid PER 31.29%
EPOCH 17:
  batch 50 loss: 0.8191473352909088
  batch 100 loss: 0.7636990118026733
  batch 150 loss: 0.8328471767902375
  batch 200 loss: 0.8461416149139405
  batch 250 loss: 0.8597321784496308
  batch 300 loss: 0.85411323428154
  batch 350 loss: 0.8676301002502441
  batch 400 loss: 0.9615829145908356
  batch 450 loss: 1.0062262737751007
  batch 500 loss: 0.9431057417392731
  batch 550 loss: 0.9428853189945221
  batch 600 loss: 0.9401302123069764
  batch 650 loss: 0.9347824633121491
  batch 700 loss: 0.9353755366802216
  batch 750 loss: 0.8765172326564789
  batch 800 loss: 0.9256414902210236
  batch 850 loss: 0.9089570736885071
  batch 900 loss: 0.918572508096695
LOSS train 0.91857 valid 1.06935, valid PER 32.06%
EPOCH 18:
  batch 50 loss: 0.8537287127971649
  batch 100 loss: 0.8308847177028656
  batch 150 loss: 0.8727678000926972
  batch 200 loss: 0.8437643992900848
  batch 250 loss: 0.8479457819461822
  batch 300 loss: 0.8609862327575684
  batch 350 loss: 0.8552933633327484
  batch 400 loss: 0.8628244888782501
  batch 450 loss: 0.9024277651309967
  batch 500 loss: 0.8996862852573395
  batch 550 loss: 0.8941807889938355
  batch 600 loss: 0.864466724395752
  batch 650 loss: 0.8437799394130707
  batch 700 loss: 0.8471448028087616
  batch 750 loss: 0.8762301176786422
  batch 800 loss: 0.8645762598514557
  batch 850 loss: 0.888541544675827
  batch 900 loss: 0.8455720949172973
LOSS train 0.84557 valid 1.02622, valid PER 31.18%
EPOCH 19:
  batch 50 loss: 0.7858806359767914
  batch 100 loss: 0.8124256634712219
  batch 150 loss: 0.7942275935411454
  batch 200 loss: 0.7826272809505462
  batch 250 loss: 0.8255188584327697
  batch 300 loss: 0.8639658284187317
  batch 350 loss: 0.8658584415912628
  batch 400 loss: 0.8249005806446076
  batch 450 loss: 0.8013760375976563
  batch 500 loss: 0.7856971383094787
  batch 550 loss: 0.8251638925075531
  batch 600 loss: 0.8328893393278122
  batch 650 loss: 0.8416804981231689
  batch 700 loss: 0.858880820274353
  batch 750 loss: 0.8008779633045197
  batch 800 loss: 0.7927445042133331
  batch 850 loss: 0.8256429636478424
  batch 900 loss: 0.8117011821269989
LOSS train 0.81170 valid 1.02860, valid PER 30.90%
EPOCH 20:
  batch 50 loss: 0.7589145618677139
  batch 100 loss: 0.7711893510818482
  batch 150 loss: 0.782638179063797
  batch 200 loss: 0.8161211514472961
  batch 250 loss: 0.8048193418979644
  batch 300 loss: 0.8073010098934174
  batch 350 loss: 0.7900957000255585
  batch 400 loss: 0.7984999084472656
  batch 450 loss: 0.8160170686244964
  batch 500 loss: 0.835961354970932
  batch 550 loss: 0.7911704051494598
  batch 600 loss: 0.9152526974678039
  batch 650 loss: 0.9063542878627777
  batch 700 loss: 0.8608863973617553
  batch 750 loss: 0.8081209695339203
  batch 800 loss: 0.8409527945518493
  batch 850 loss: 0.8649837517738342
  batch 900 loss: 0.8135628223419189
LOSS train 0.81356 valid 1.03478, valid PER 31.82%
Training finished in 5.0 minutes.
Model saved to checkpoints/20230116_114837/model_13
Loading model from checkpoints/20230116_114837/model_13
SUB: 17.03%, DEL: 11.53%, INS: 2.72%, COR: 71.44%, PER: 31.28%
