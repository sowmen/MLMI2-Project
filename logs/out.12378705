Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.5)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.334682359695434
  batch 100 loss: 3.193569278717041
  batch 150 loss: 3.0335601568222046
  batch 200 loss: 2.9061584949493406
  batch 250 loss: 2.8324824810028075
  batch 300 loss: 2.6369752645492555
  batch 350 loss: 2.5027304697036743
  batch 400 loss: 2.6847200202941894
  batch 450 loss: 2.4844995069503786
  batch 500 loss: 2.2700844907760622
  batch 550 loss: 2.2081963205337525
  batch 600 loss: 2.122740235328674
  batch 650 loss: 2.030284571647644
  batch 700 loss: 2.028718421459198
  batch 750 loss: 1.9452341747283937
  batch 800 loss: 1.9227689695358277
  batch 850 loss: 1.8528594207763671
  batch 900 loss: 1.842405414581299
LOSS train 1.84241 valid 1.78324, valid PER 69.46%
EPOCH 2:
  batch 50 loss: 1.794202711582184
  batch 100 loss: 1.7611721348762512
  batch 150 loss: 1.650080647468567
  batch 200 loss: 1.6771335768699647
  batch 250 loss: 1.657284607887268
  batch 300 loss: 1.621011197566986
  batch 350 loss: 1.6144983291625976
  batch 400 loss: 1.562927315235138
  batch 450 loss: 1.5627348494529725
  batch 500 loss: 1.507343397140503
  batch 550 loss: 1.5059161520004272
  batch 600 loss: 1.4836975884437562
  batch 650 loss: 1.4300602412223815
  batch 700 loss: 1.4753210496902467
  batch 750 loss: 1.4119536471366883
  batch 800 loss: 1.3783346509933472
  batch 850 loss: 1.3947589349746705
  batch 900 loss: 1.3412710666656493
LOSS train 1.34127 valid 1.34857, valid PER 43.67%
EPOCH 3:
  batch 50 loss: 1.2807213115692138
  batch 100 loss: 1.3366615843772889
  batch 150 loss: 1.3603443455696107
  batch 200 loss: 1.2698954558372497
  batch 250 loss: 1.3385799622535706
  batch 300 loss: 1.3210493397712708
  batch 350 loss: 1.3369548404216767
  batch 400 loss: 1.3241634690761566
  batch 450 loss: 1.292609257698059
  batch 500 loss: 1.2345445609092713
  batch 550 loss: 1.2680729746818542
  batch 600 loss: 1.2063162350654602
  batch 650 loss: 1.2102875101566315
  batch 700 loss: 1.2370582127571106
  batch 750 loss: 1.2721677100658417
  batch 800 loss: 1.2560425901412964
  batch 850 loss: 1.2332046020030976
  batch 900 loss: 1.2235048019886017
LOSS train 1.22350 valid 1.23271, valid PER 38.41%
EPOCH 4:
  batch 50 loss: 1.2177359020709992
  batch 100 loss: 1.1480176544189453
  batch 150 loss: 1.1739670217037201
  batch 200 loss: 1.149027581214905
  batch 250 loss: 1.165836921930313
  batch 300 loss: 1.170795637369156
  batch 350 loss: 1.191273547410965
  batch 400 loss: 1.1225100767612457
  batch 450 loss: 1.1521418035030364
  batch 500 loss: 1.1804511320590974
  batch 550 loss: 1.1217623484134673
  batch 600 loss: 1.0850653672218322
  batch 650 loss: 1.168551983833313
  batch 700 loss: 1.2304217791557313
  batch 750 loss: 1.1247366344928742
  batch 800 loss: 1.1104917860031127
  batch 850 loss: 1.1335018622875213
  batch 900 loss: 1.135965291261673
LOSS train 1.13597 valid 1.14956, valid PER 35.21%
EPOCH 5:
  batch 50 loss: 1.1011088335514068
  batch 100 loss: 1.0886190140247345
  batch 150 loss: 1.1048580193519593
  batch 200 loss: 1.1175973176956178
  batch 250 loss: 1.0668132197856903
  batch 300 loss: 1.0907119631767273
  batch 350 loss: 1.054780534505844
  batch 400 loss: 1.0541016709804536
  batch 450 loss: 1.062084618806839
  batch 500 loss: 1.0454431581497192
  batch 550 loss: 1.0874535095691682
  batch 600 loss: 1.085613317489624
  batch 650 loss: 1.0964434921741486
  batch 700 loss: 1.097550619840622
  batch 750 loss: 1.0543290555477143
  batch 800 loss: 1.099897451400757
  batch 850 loss: 1.084372512102127
  batch 900 loss: 1.0623468005657195
LOSS train 1.06235 valid 1.08802, valid PER 33.63%
EPOCH 6:
  batch 50 loss: 1.0221579205989837
  batch 100 loss: 1.037745748758316
  batch 150 loss: 1.0244771552085876
  batch 200 loss: 0.9929149317741394
  batch 250 loss: 1.0720561909675599
  batch 300 loss: 1.071754070520401
  batch 350 loss: 1.0539523649215699
  batch 400 loss: 1.0052751183509827
  batch 450 loss: 1.054710282087326
  batch 500 loss: 0.9875541138648987
  batch 550 loss: 1.0153590774536132
  batch 600 loss: 0.9959725654125213
  batch 650 loss: 0.9777151703834533
  batch 700 loss: 0.9790896224975586
  batch 750 loss: 1.0132952451705932
  batch 800 loss: 0.9955711364746094
  batch 850 loss: 1.0360317969322204
  batch 900 loss: 1.051468288898468
LOSS train 1.05147 valid 1.06940, valid PER 33.82%
EPOCH 7:
  batch 50 loss: 0.9671019291877747
  batch 100 loss: 1.0303141939640046
  batch 150 loss: 0.9468021678924561
  batch 200 loss: 0.9701941382884979
  batch 250 loss: 0.9850172114372253
  batch 300 loss: 0.9781753993034363
  batch 350 loss: 1.0342009937763215
  batch 400 loss: 0.9968513989448547
  batch 450 loss: 0.9913961291313171
  batch 500 loss: 0.9991357994079589
  batch 550 loss: 0.9810836517810821
  batch 600 loss: 0.9630579388141632
  batch 650 loss: 0.9574426114559174
  batch 700 loss: 0.9977399778366088
  batch 750 loss: 0.9765532648563385
  batch 800 loss: 0.9593588125705719
  batch 850 loss: 0.9585974419116974
  batch 900 loss: 0.9414930248260498
LOSS train 0.94149 valid 1.05476, valid PER 33.38%
EPOCH 8:
  batch 50 loss: 0.9550136733055115
  batch 100 loss: 0.9107602250576019
  batch 150 loss: 0.9681147646903991
  batch 200 loss: 0.9668815386295319
  batch 250 loss: 0.9420846998691559
  batch 300 loss: 0.9007674586772919
  batch 350 loss: 0.9553201401233673
  batch 400 loss: 0.9272867333889008
  batch 450 loss: 0.9926979875564576
  batch 500 loss: 0.9594610977172852
  batch 550 loss: 0.9665970492362976
  batch 600 loss: 0.9184175193309784
  batch 650 loss: 0.9225216579437255
  batch 700 loss: 0.9537706387042999
  batch 750 loss: 0.9645512223243713
  batch 800 loss: 0.9515663409233093
  batch 850 loss: 0.9303553557395935
  batch 900 loss: 0.9221836316585541
LOSS train 0.92218 valid 1.05346, valid PER 32.15%
EPOCH 9:
  batch 50 loss: 0.9067369639873505
  batch 100 loss: 0.8702297723293304
  batch 150 loss: 0.8993001222610474
  batch 200 loss: 0.8759934711456299
  batch 250 loss: 0.8794313025474548
  batch 300 loss: 0.893922084569931
  batch 350 loss: 0.8768266928195954
  batch 400 loss: 0.9386642837524414
  batch 450 loss: 0.9467610371112823
  batch 500 loss: 0.9080918192863464
  batch 550 loss: 0.9197076439857483
  batch 600 loss: 0.9367423629760743
  batch 650 loss: 0.9093258535861969
  batch 700 loss: 0.8954450941085815
  batch 750 loss: 0.9106341814994812
  batch 800 loss: 0.962755196094513
  batch 850 loss: 0.9694919908046722
  batch 900 loss: 1.0048959851264954
LOSS train 1.00490 valid 1.06393, valid PER 32.26%
EPOCH 10:
  batch 50 loss: 0.8980688548088074
  batch 100 loss: 0.9124948120117188
  batch 150 loss: 0.9560358917713165
  batch 200 loss: 0.8952714610099792
  batch 250 loss: 0.8923289775848389
  batch 300 loss: 0.8950978994369507
  batch 350 loss: 0.8967845606803894
  batch 400 loss: 0.8689987218379974
  batch 450 loss: 0.868290559053421
  batch 500 loss: 0.9062304413318634
  batch 550 loss: 0.8938566088676453
  batch 600 loss: 0.90476487159729
  batch 650 loss: 0.9131614851951599
  batch 700 loss: 0.9090508782863617
  batch 750 loss: 0.9203850352764129
  batch 800 loss: 0.9011340951919555
  batch 850 loss: 0.8782452046871185
  batch 900 loss: 0.8790071678161621
LOSS train 0.87901 valid 1.01985, valid PER 31.89%
EPOCH 11:
  batch 50 loss: 0.857134929895401
  batch 100 loss: 0.839677494764328
  batch 150 loss: 0.8420039618015289
  batch 200 loss: 0.8086665904521942
  batch 250 loss: 0.8274492597579957
  batch 300 loss: 0.8254476094245911
  batch 350 loss: 0.8701971173286438
  batch 400 loss: 0.8535679697990417
  batch 450 loss: 0.8504275822639465
  batch 500 loss: 0.8460424089431763
  batch 550 loss: 0.8765788733959198
  batch 600 loss: 0.8601033627986908
  batch 650 loss: 0.8557535028457641
  batch 700 loss: 0.9240057289600372
  batch 750 loss: 0.8592487967014313
  batch 800 loss: 0.8834379494190217
  batch 850 loss: 0.8673543560504914
  batch 900 loss: 0.889600772857666
LOSS train 0.88960 valid 0.98603, valid PER 30.59%
EPOCH 12:
  batch 50 loss: 0.805809565782547
  batch 100 loss: 0.7965850818157196
  batch 150 loss: 0.8382996833324432
  batch 200 loss: 0.8539521312713623
  batch 250 loss: 0.8382882988452911
  batch 300 loss: 0.8901771509647369
  batch 350 loss: 0.8827281522750855
  batch 400 loss: 0.9061124944686889
  batch 450 loss: 0.8355116772651673
  batch 500 loss: 0.8635048961639404
  batch 550 loss: 0.8664057087898255
  batch 600 loss: 0.852880026102066
  batch 650 loss: 0.8855240607261657
  batch 700 loss: 0.8639164292812347
  batch 750 loss: 0.8686212646961212
  batch 800 loss: 0.8281761145591736
  batch 850 loss: 0.8707158625125885
  batch 900 loss: 0.8845477747917175
LOSS train 0.88455 valid 0.99881, valid PER 30.83%
EPOCH 13:
  batch 50 loss: 0.8017108452320099
  batch 100 loss: 0.8275395143032074
  batch 150 loss: 0.8278730618953705
  batch 200 loss: 0.7962830877304077
  batch 250 loss: 0.8284075009822846
  batch 300 loss: 0.8790043330192566
  batch 350 loss: 0.8337433016300202
  batch 400 loss: 0.8309766817092895
  batch 450 loss: 0.8362932324409484
  batch 500 loss: 0.8174140405654907
  batch 550 loss: 0.8689190566539764
  batch 600 loss: 0.8379497718811035
  batch 650 loss: 0.8278745031356811
  batch 700 loss: 0.8481208992004394
  batch 750 loss: 0.8213971984386444
  batch 800 loss: 0.8354194819927215
  batch 850 loss: 0.8303701901435852
  batch 900 loss: 0.8451086330413818
LOSS train 0.84511 valid 0.97588, valid PER 29.48%
EPOCH 14:
  batch 50 loss: 0.7815778303146362
  batch 100 loss: 0.7704793989658356
  batch 150 loss: 0.7948691022396087
  batch 200 loss: 0.775817049741745
  batch 250 loss: 0.8352333557605743
  batch 300 loss: 0.7846706318855285
  batch 350 loss: 0.7954482161998748
  batch 400 loss: 0.8402701312303543
  batch 450 loss: 0.8090792709589004
  batch 500 loss: 0.8042719078063965
  batch 550 loss: 0.8312348353862763
  batch 600 loss: 0.8153282487392426
  batch 650 loss: 0.8145999705791473
  batch 700 loss: 0.8273643958568573
  batch 750 loss: 0.826459835767746
  batch 800 loss: 0.8016499835252762
  batch 850 loss: 0.8402841448783874
  batch 900 loss: 0.8253603386878967
LOSS train 0.82536 valid 0.96188, valid PER 29.66%
EPOCH 15:
  batch 50 loss: 0.7485920512676238
  batch 100 loss: 0.7698957073688507
  batch 150 loss: 0.7646100831031799
  batch 200 loss: 0.8066868817806244
  batch 250 loss: 0.8018575984239579
  batch 300 loss: 0.7928701758384704
  batch 350 loss: 0.7920908761024475
  batch 400 loss: 0.8323766553401947
  batch 450 loss: 0.8333597815036774
  batch 500 loss: 0.7928422927856446
  batch 550 loss: 0.837304310798645
  batch 600 loss: 0.8443706488609314
  batch 650 loss: 0.7915233886241912
  batch 700 loss: 0.7845740616321564
  batch 750 loss: 0.8201365280151367
  batch 800 loss: 0.7939061015844345
  batch 850 loss: 0.7799199247360229
  batch 900 loss: 0.7513690310716629
LOSS train 0.75137 valid 0.94361, valid PER 28.73%
EPOCH 16:
  batch 50 loss: 0.7553284537792205
  batch 100 loss: 0.7330924499034882
  batch 150 loss: 0.7589275336265564
  batch 200 loss: 0.795297338962555
  batch 250 loss: 0.7670191478729248
  batch 300 loss: 0.7794728994369506
  batch 350 loss: 0.7686952352523804
  batch 400 loss: 0.7545760518312454
  batch 450 loss: 0.7797307229042053
  batch 500 loss: 0.775537701845169
  batch 550 loss: 0.7577626597881317
  batch 600 loss: 0.8119996511936187
  batch 650 loss: 0.7797436475753784
  batch 700 loss: 0.7538226026296616
  batch 750 loss: 0.772374427318573
  batch 800 loss: 0.7850647497177125
  batch 850 loss: 0.7729111433029174
  batch 900 loss: 0.7605608165264129
LOSS train 0.76056 valid 0.93897, valid PER 29.48%
EPOCH 17:
  batch 50 loss: 0.7536122268438339
  batch 100 loss: 0.6719624602794647
  batch 150 loss: 0.7543362843990326
  batch 200 loss: 0.6949811804294587
  batch 250 loss: 0.7482913148403167
  batch 300 loss: 0.7439359033107757
  batch 350 loss: 0.7606289023160935
  batch 400 loss: 0.776238864660263
  batch 450 loss: 0.7824053430557251
  batch 500 loss: 0.7606579566001892
  batch 550 loss: 0.7807938086986542
  batch 600 loss: 0.805721423625946
  batch 650 loss: 0.7551182574033737
  batch 700 loss: 0.7689837193489075
  batch 750 loss: 0.7644831484556198
  batch 800 loss: 0.8021974611282349
  batch 850 loss: 0.7824006187915802
  batch 900 loss: 0.7830087220668793
LOSS train 0.78301 valid 0.95963, valid PER 28.64%
EPOCH 18:
  batch 50 loss: 0.7228969436883926
  batch 100 loss: 0.7270223808288574
  batch 150 loss: 0.760090964436531
  batch 200 loss: 0.7234886753559112
  batch 250 loss: 0.7068150138854981
  batch 300 loss: 0.723962483406067
  batch 350 loss: 0.7156372570991516
  batch 400 loss: 0.713927505016327
  batch 450 loss: 0.7529186129570007
  batch 500 loss: 0.7216760635375976
  batch 550 loss: 0.7574396848678588
  batch 600 loss: 0.7456621158123017
  batch 650 loss: 0.7301801669597626
  batch 700 loss: 0.7531666159629822
  batch 750 loss: 0.7931487309932709
  batch 800 loss: 0.8022403967380524
  batch 850 loss: 0.7644282579421997
  batch 900 loss: 0.79314668238163
LOSS train 0.79315 valid 0.98081, valid PER 29.71%
EPOCH 19:
  batch 50 loss: 0.7418321669101715
  batch 100 loss: 0.7571799647808075
  batch 150 loss: 0.7150399142503738
  batch 200 loss: 0.7135370302200318
  batch 250 loss: 0.7654752331972122
  batch 300 loss: 0.7719509869813919
  batch 350 loss: 0.7789074230194092
  batch 400 loss: 0.765031470656395
  batch 450 loss: 0.7238752174377442
  batch 500 loss: 0.7273358845710755
  batch 550 loss: 0.7689009404182434
  batch 600 loss: 0.7658665430545807
  batch 650 loss: 0.7917084664106369
  batch 700 loss: 0.8135259461402893
  batch 750 loss: 0.738518078327179
  batch 800 loss: 0.7285649758577347
  batch 850 loss: 0.7801974904537201
  batch 900 loss: 0.7450067353248596
LOSS train 0.74501 valid 0.97261, valid PER 29.03%
EPOCH 20:
  batch 50 loss: 0.703197818994522
  batch 100 loss: 0.7046199107170105
  batch 150 loss: 0.768578610420227
  batch 200 loss: 0.7714188301563263
  batch 250 loss: 0.7766823422908783
  batch 300 loss: 0.7731901156902313
  batch 350 loss: 0.7290971678495407
  batch 400 loss: 0.7527185666561127
  batch 450 loss: 0.7224769306182861
  batch 500 loss: 0.7701987731456756
  batch 550 loss: 0.7476259166002274
  batch 600 loss: 0.7265946340560913
  batch 650 loss: 0.8116187345981598
  batch 700 loss: 0.7451101791858673
  batch 750 loss: 0.7138007694482803
  batch 800 loss: 0.7678955888748169
  batch 850 loss: 0.7861759161949158
  batch 900 loss: 0.7533191788196564
LOSS train 0.75332 valid 0.97235, valid PER 29.98%
Training finished in 5.0 minutes.
Model saved to checkpoints/20230116_115121/model_16
Loading model from checkpoints/20230116_115121/model_16
SUB: 17.00%, DEL: 11.40%, INS: 2.38%, COR: 71.60%, PER: 30.78%
