Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=2.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.577373466491699
  batch 100 loss: 3.1835043048858642
  batch 150 loss: 2.910315113067627
  batch 200 loss: 2.6708773136138917
  batch 250 loss: 2.512810220718384
  batch 300 loss: 2.3639462900161745
  batch 350 loss: 2.2340766525268556
  batch 400 loss: 2.2456241750717165
  batch 450 loss: 2.1524616122245788
  batch 500 loss: 2.0349005126953124
  batch 550 loss: 2.008256740570068
  batch 600 loss: 1.9458606052398681
  batch 650 loss: 1.8710981225967407
  batch 700 loss: 1.8609660148620606
  batch 750 loss: 1.79287451505661
  batch 800 loss: 1.7686216235160828
  batch 850 loss: 1.728634967803955
  batch 900 loss: 1.7246609568595885
LOSS train 1.72466 valid 1.67922, valid PER 59.27%
EPOCH 2:
  batch 50 loss: 1.6672644710540772
  batch 100 loss: 1.5961199307441711
  batch 150 loss: 1.5843860912322998
  batch 200 loss: 1.5980813527107238
  batch 250 loss: 1.5831656289100646
  batch 300 loss: 1.5579612970352172
  batch 350 loss: 1.474301872253418
  batch 400 loss: 1.476063072681427
  batch 450 loss: 1.4285465693473816
  batch 500 loss: 1.482233693599701
  batch 550 loss: 1.4787204766273498
  batch 600 loss: 1.4212126016616822
  batch 650 loss: 1.457642548084259
  batch 700 loss: 1.3959481287002564
  batch 750 loss: 1.3984711539745331
  batch 800 loss: 1.3478448390960693
  batch 850 loss: 1.350284721851349
  batch 900 loss: 1.366886351108551
LOSS train 1.36689 valid 1.34365, valid PER 42.52%
EPOCH 3:
  batch 50 loss: 1.3094930601119996
  batch 100 loss: 1.2897423577308655
  batch 150 loss: 1.2777031826972962
  batch 200 loss: 1.2565294623374939
  batch 250 loss: 1.2611224496364593
  batch 300 loss: 1.2593059468269348
  batch 350 loss: 1.2902782583236694
  batch 400 loss: 1.267987985610962
  batch 450 loss: 1.2418082308769227
  batch 500 loss: 1.2246175837516784
  batch 550 loss: 1.2223416996002197
  batch 600 loss: 1.201650437116623
  batch 650 loss: 1.1843396604061127
  batch 700 loss: 1.1997186160087585
  batch 750 loss: 1.2610987043380737
  batch 800 loss: 1.1871660935878754
  batch 850 loss: 1.2179784297943115
  batch 900 loss: 1.142261610031128
LOSS train 1.14226 valid 1.23053, valid PER 37.22%
EPOCH 4:
  batch 50 loss: 1.1340610146522523
  batch 100 loss: 1.1472885131835937
  batch 150 loss: 1.117170604467392
  batch 200 loss: 1.134725079536438
  batch 250 loss: 1.1511171495914458
  batch 300 loss: 1.1484508156776427
  batch 350 loss: 1.0879107785224915
  batch 400 loss: 1.1133280551433564
  batch 450 loss: 1.115044091939926
  batch 500 loss: 1.0944209802150726
  batch 550 loss: 1.120171562433243
  batch 600 loss: 1.1381971991062165
  batch 650 loss: 1.1119024789333343
  batch 700 loss: 1.0639472031593322
  batch 750 loss: 1.0692961776256562
  batch 800 loss: 1.0416567099094391
  batch 850 loss: 1.0983913373947143
  batch 900 loss: 1.112769056558609
LOSS train 1.11277 valid 1.09815, valid PER 34.30%
EPOCH 5:
  batch 50 loss: 1.0167976093292237
  batch 100 loss: 1.0292232322692871
  batch 150 loss: 1.087905184030533
  batch 200 loss: 1.0010503709316254
  batch 250 loss: 1.0180384254455566
  batch 300 loss: 1.021056226491928
  batch 350 loss: 1.0268665432929993
  batch 400 loss: 1.0255039870738982
  batch 450 loss: 1.018116353750229
  batch 500 loss: 1.0385777258872986
  batch 550 loss: 0.9875817787647247
  batch 600 loss: 1.0699250519275665
  batch 650 loss: 1.0058313357830047
  batch 700 loss: 1.0387210869789123
  batch 750 loss: 0.9871082854270935
  batch 800 loss: 1.0248886954784393
  batch 850 loss: 1.0226412165164946
  batch 900 loss: 1.0194493114948273
LOSS train 1.01945 valid 1.07061, valid PER 32.78%
EPOCH 6:
  batch 50 loss: 0.9896051216125489
  batch 100 loss: 0.9530483341217041
  batch 150 loss: 0.9489999115467072
  batch 200 loss: 0.9524757432937622
  batch 250 loss: 0.9889180564880371
  batch 300 loss: 0.9687872099876403
  batch 350 loss: 0.9531138122081757
  batch 400 loss: 0.9507009649276733
  batch 450 loss: 0.9913055264949798
  batch 500 loss: 0.9636892938613891
  batch 550 loss: 0.9842347550392151
  batch 600 loss: 0.9325912022590637
  batch 650 loss: 0.9789491009712219
  batch 700 loss: 0.9592301893234253
  batch 750 loss: 0.9507044434547425
  batch 800 loss: 0.933884152173996
  batch 850 loss: 0.9340925908088684
  batch 900 loss: 0.956677964925766
LOSS train 0.95668 valid 1.02148, valid PER 32.56%
EPOCH 7:
  batch 50 loss: 0.9244726145267487
  batch 100 loss: 0.9274972116947174
  batch 150 loss: 0.9157328963279724
  batch 200 loss: 0.8932998621463776
  batch 250 loss: 0.9022974824905395
  batch 300 loss: 0.8923223245143891
  batch 350 loss: 0.9115721666812897
  batch 400 loss: 0.9002468991279602
  batch 450 loss: 0.9133007407188416
  batch 500 loss: 0.9085503256320954
  batch 550 loss: 0.8967866563796997
  batch 600 loss: 0.9006314945220947
  batch 650 loss: 0.8915750217437745
  batch 700 loss: 0.917952436208725
  batch 750 loss: 0.8936371695995331
  batch 800 loss: 0.8916319215297699
  batch 850 loss: 0.9209697878360749
  batch 900 loss: 0.9431675684452057
LOSS train 0.94317 valid 1.00939, valid PER 31.60%
EPOCH 8:
  batch 50 loss: 0.8468884956836701
  batch 100 loss: 0.8438816928863525
  batch 150 loss: 0.8585125553607941
  batch 200 loss: 0.8459306299686432
  batch 250 loss: 0.861104986667633
  batch 300 loss: 0.8128181326389313
  batch 350 loss: 0.8920559966564179
  batch 400 loss: 0.8626827907562256
  batch 450 loss: 0.8747827661037445
  batch 500 loss: 0.9037279284000397
  batch 550 loss: 0.8256423401832581
  batch 600 loss: 0.888109780550003
  batch 650 loss: 0.9038876724243164
  batch 700 loss: 0.852881691455841
  batch 750 loss: 0.8820413577556611
  batch 800 loss: 0.8748202812671662
  batch 850 loss: 0.8625769209861756
  batch 900 loss: 0.8774722933769226
LOSS train 0.87747 valid 0.99992, valid PER 29.90%
EPOCH 9:
  batch 50 loss: 0.7712122809886932
  batch 100 loss: 0.8238883948326111
  batch 150 loss: 0.8269725477695465
  batch 200 loss: 0.80091632604599
  batch 250 loss: 0.8427085208892823
  batch 300 loss: 0.8420917201042175
  batch 350 loss: 0.8554446804523468
  batch 400 loss: 0.8546986758708954
  batch 450 loss: 0.829763981103897
  batch 500 loss: 0.8106419277191163
  batch 550 loss: 0.8372886753082276
  batch 600 loss: 0.868449285030365
  batch 650 loss: 0.8328010427951813
  batch 700 loss: 0.7980972731113434
  batch 750 loss: 0.8110328471660614
  batch 800 loss: 0.851234142780304
  batch 850 loss: 0.8467979419231415
  batch 900 loss: 0.8080979359149932
LOSS train 0.80810 valid 0.97471, valid PER 29.95%
EPOCH 10:
  batch 50 loss: 0.763315360546112
  batch 100 loss: 0.7829117619991303
  batch 150 loss: 0.7868564045429229
  batch 200 loss: 0.8075225639343262
  batch 250 loss: 0.8112908554077148
  batch 300 loss: 0.7767463743686676
  batch 350 loss: 0.7934188580513001
  batch 400 loss: 0.7522592723369599
  batch 450 loss: 0.7590450406074524
  batch 500 loss: 0.8058832597732544
  batch 550 loss: 0.8422385406494141
  batch 600 loss: 0.8076668548583984
  batch 650 loss: 0.7877763569355011
  batch 700 loss: 0.815536595582962
  batch 750 loss: 0.7804773807525635
  batch 800 loss: 0.8065402060747147
  batch 850 loss: 0.8116964864730835
  batch 900 loss: 0.806818882226944
LOSS train 0.80682 valid 0.98798, valid PER 31.03%
EPOCH 11:
  batch 50 loss: 0.7318386518955231
  batch 100 loss: 0.7108274459838867
  batch 150 loss: 0.7221924436092376
  batch 200 loss: 0.790214341878891
  batch 250 loss: 0.775860196352005
  batch 300 loss: 0.7400720775127411
  batch 350 loss: 0.7551707923412323
  batch 400 loss: 0.7790061187744141
  batch 450 loss: 0.7756456023454666
  batch 500 loss: 0.768202805519104
  batch 550 loss: 0.7721160012483597
  batch 600 loss: 0.7679067492485047
  batch 650 loss: 0.8213661342859269
  batch 700 loss: 0.7588733816146851
  batch 750 loss: 0.7722528266906739
  batch 800 loss: 0.8007674837112426
  batch 850 loss: 0.8075872832536697
  batch 900 loss: 0.7856801664829254
LOSS train 0.78568 valid 0.94046, valid PER 28.94%
EPOCH 12:
  batch 50 loss: 0.7231649744510651
  batch 100 loss: 0.7012749683856964
  batch 150 loss: 0.6892636096477509
  batch 200 loss: 0.7251477980613709
  batch 250 loss: 0.7424695473909378
  batch 300 loss: 0.7135161381959915
  batch 350 loss: 0.7275108599662781
  batch 400 loss: 0.7474377930164338
  batch 450 loss: 0.761746209859848
  batch 500 loss: 0.7633225220441818
  batch 550 loss: 0.7093153834342957
  batch 600 loss: 0.731754332780838
  batch 650 loss: 0.7565341633558273
  batch 700 loss: 0.763553409576416
  batch 750 loss: 0.7535857772827148
  batch 800 loss: 0.7418962401151658
  batch 850 loss: 0.7731105649471283
  batch 900 loss: 0.7841494369506836
LOSS train 0.78415 valid 0.94002, valid PER 28.46%
EPOCH 13:
  batch 50 loss: 0.6809804153442383
  batch 100 loss: 0.7014842712879181
  batch 150 loss: 0.6752925026416778
  batch 200 loss: 0.7066819834709167
  batch 250 loss: 0.7046068704128265
  batch 300 loss: 0.684645791053772
  batch 350 loss: 0.6967349261045456
  batch 400 loss: 0.7160114735364914
  batch 450 loss: 0.7102935814857483
  batch 500 loss: 0.7152718806266785
  batch 550 loss: 0.7378956955671311
  batch 600 loss: 0.7158296364545822
  batch 650 loss: 0.7358786571025848
  batch 700 loss: 0.7506094157695771
  batch 750 loss: 0.7006197929382324
  batch 800 loss: 0.7069419676065445
  batch 850 loss: 0.7483937692642212
  batch 900 loss: 0.7489156627655029
LOSS train 0.74892 valid 0.96595, valid PER 29.50%
EPOCH 14:
  batch 50 loss: 0.676429746747017
  batch 100 loss: 0.6655225628614425
  batch 150 loss: 0.683838524222374
  batch 200 loss: 0.6616221207380295
  batch 250 loss: 0.6712476366758346
  batch 300 loss: 0.7172172063589096
  batch 350 loss: 0.6660626256465911
  batch 400 loss: 0.6753549885749817
  batch 450 loss: 0.6733453804254532
  batch 500 loss: 0.7084136474132537
  batch 550 loss: 0.725221688747406
  batch 600 loss: 0.6815898030996322
  batch 650 loss: 0.7056732076406479
  batch 700 loss: 0.7245467269420623
  batch 750 loss: 0.692443071603775
  batch 800 loss: 0.682419582605362
  batch 850 loss: 0.7313216692209243
  batch 900 loss: 0.7158921647071839
LOSS train 0.71589 valid 0.94943, valid PER 28.94%
EPOCH 15:
  batch 50 loss: 0.6407742702960968
  batch 100 loss: 0.6451163256168365
  batch 150 loss: 0.6605327349901199
  batch 200 loss: 0.6855495464801789
  batch 250 loss: 0.6830342978239059
  batch 300 loss: 0.6433867424726486
  batch 350 loss: 0.648664557337761
  batch 400 loss: 0.6616793107986451
  batch 450 loss: 0.6702278369665146
  batch 500 loss: 0.6393746739625931
  batch 550 loss: 0.6635680717229843
  batch 600 loss: 0.691543989777565
  batch 650 loss: 0.6988494622707367
  batch 700 loss: 0.6980747163295746
  batch 750 loss: 0.6911446881294251
  batch 800 loss: 0.6710111725330353
  batch 850 loss: 0.6600339382886886
  batch 900 loss: 0.6988168382644653
LOSS train 0.69882 valid 0.96963, valid PER 28.96%
EPOCH 16:
  batch 50 loss: 0.6305611318349839
  batch 100 loss: 0.6079501312971115
  batch 150 loss: 0.6293655163049698
  batch 200 loss: 0.6188434028625488
  batch 250 loss: 0.6569241881370544
  batch 300 loss: 0.6334459441900253
  batch 350 loss: 0.6704682672023773
  batch 400 loss: 0.6601435899734497
  batch 450 loss: 0.6728250050544738
  batch 500 loss: 0.6366811484098435
  batch 550 loss: 0.6419434052705765
  batch 600 loss: 0.651745176911354
  batch 650 loss: 0.6637970370054245
  batch 700 loss: 0.6431345802545547
  batch 750 loss: 0.6605450654029846
  batch 800 loss: 0.6638641506433487
  batch 850 loss: 0.6616551184654236
  batch 900 loss: 0.650679525732994
LOSS train 0.65068 valid 0.94076, valid PER 28.28%
EPOCH 17:
  batch 50 loss: 0.6122885209321975
  batch 100 loss: 0.6069151383638381
  batch 150 loss: 0.6228953266143799
  batch 200 loss: 0.6106410551071167
  batch 250 loss: 0.6415871208906174
  batch 300 loss: 0.632467451095581
  batch 350 loss: 0.6080926364660263
  batch 400 loss: 0.6478596264123917
  batch 450 loss: 0.6465512466430664
  batch 500 loss: 0.6024211490154266
  batch 550 loss: 0.6330634224414825
  batch 600 loss: 0.6560807567834854
  batch 650 loss: 0.6439646875858307
  batch 700 loss: 0.6202280139923095
  batch 750 loss: 0.6158739417791367
  batch 800 loss: 0.6304887050390243
  batch 850 loss: 0.649157697558403
  batch 900 loss: 0.6329086208343506
LOSS train 0.63291 valid 0.95168, valid PER 27.52%
EPOCH 18:
  batch 50 loss: 0.566158440709114
  batch 100 loss: 0.6060695594549179
  batch 150 loss: 0.6291205394268036
  batch 200 loss: 0.6031214839220047
  batch 250 loss: 0.6158319282531738
  batch 300 loss: 0.5802572405338288
  batch 350 loss: 0.6180051577091217
  batch 400 loss: 0.5988213652372361
  batch 450 loss: 0.6354798752069474
  batch 500 loss: 0.6313122940063477
  batch 550 loss: 0.6273022204637527
  batch 600 loss: 0.5988171905279159
  batch 650 loss: 0.5870434957742691
  batch 700 loss: 0.6395496726036072
  batch 750 loss: 0.6120711940526963
  batch 800 loss: 0.625946769118309
  batch 850 loss: 0.6410823744535447
  batch 900 loss: 0.6473805010318756
LOSS train 0.64738 valid 0.96654, valid PER 28.72%
EPOCH 19:
  batch 50 loss: 0.5610166937112808
  batch 100 loss: 0.5573383367061615
  batch 150 loss: 0.5747001945972443
  batch 200 loss: 0.5778627359867096
  batch 250 loss: 0.5920420253276825
  batch 300 loss: 0.5957467210292816
  batch 350 loss: 0.5907747656106949
  batch 400 loss: 0.6025653690099716
  batch 450 loss: 0.6092927843332291
  batch 500 loss: 0.6068951070308686
  batch 550 loss: 0.5633020603656769
  batch 600 loss: 0.5997768527269364
  batch 650 loss: 0.6549626535177231
  batch 700 loss: 0.5862501162290573
  batch 750 loss: 0.5971031385660172
  batch 800 loss: 0.6049106472730637
  batch 850 loss: 0.6217856985330582
  batch 900 loss: 0.6198798030614853
LOSS train 0.61988 valid 0.98005, valid PER 28.50%
EPOCH 20:
  batch 50 loss: 0.5485198754072189
  batch 100 loss: 0.5349198162555695
  batch 150 loss: 0.5399922609329224
  batch 200 loss: 0.5536025005578995
  batch 250 loss: 0.541526615023613
  batch 300 loss: 0.5828682953119277
  batch 350 loss: 0.5599692380428314
  batch 400 loss: 0.5800527238845825
  batch 450 loss: 0.5896667808294296
  batch 500 loss: 0.569046443104744
  batch 550 loss: 0.6127147841453552
  batch 600 loss: 0.577080694437027
  batch 650 loss: 0.5970791149139404
  batch 700 loss: 0.6043137979507446
  batch 750 loss: 0.5944655394554138
  batch 800 loss: 0.638040555715561
  batch 850 loss: 0.6211642932891845
  batch 900 loss: 0.6016665369272232
LOSS train 0.60167 valid 0.98170, valid PER 28.08%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230117_184857/model_12
Loading model from checkpoints/20230117_184857/model_12
SUB: 16.42%, DEL: 11.50%, INS: 2.38%, COR: 72.08%, PER: 30.29%
