Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.923446063995361
  batch 100 loss: 3.099365029335022
  batch 150 loss: 2.9614747285842897
  batch 200 loss: 2.82457736492157
  batch 250 loss: 2.7569062185287474
  batch 300 loss: 2.5632196140289305
  batch 350 loss: 2.4381020736694334
  batch 400 loss: 2.3891000890731813
  batch 450 loss: 2.3428941988945007
  batch 500 loss: 2.203713846206665
  batch 550 loss: 2.1162937569618223
  batch 600 loss: 2.0685718941688536
  batch 650 loss: 1.987478907108307
  batch 700 loss: 1.9710752153396607
  batch 750 loss: 1.9041589856147767
  batch 800 loss: 1.8666598010063171
  batch 850 loss: 1.836129479408264
  batch 900 loss: 1.811923644542694
LOSS train 1.81192 valid 1.77066, valid PER 68.33%
EPOCH 2:
  batch 50 loss: 1.765454351902008
  batch 100 loss: 1.7018305134773255
  batch 150 loss: 1.6628625798225403
  batch 200 loss: 1.6776988220214843
  batch 250 loss: 1.6689592361450196
  batch 300 loss: 1.6446379828453064
  batch 350 loss: 1.564423599243164
  batch 400 loss: 1.5794260954856874
  batch 450 loss: 1.5373713040351868
  batch 500 loss: 1.5610207414627075
  batch 550 loss: 1.5644075822830201
  batch 600 loss: 1.5080033802986146
  batch 650 loss: 1.5392404103279114
  batch 700 loss: 1.4908906531333923
  batch 750 loss: 1.478811674118042
  batch 800 loss: 1.4289492225646974
  batch 850 loss: 1.4339326429367065
  batch 900 loss: 1.4598025226593017
LOSS train 1.45980 valid 1.42203, valid PER 46.03%
EPOCH 3:
  batch 50 loss: 1.4084397768974304
  batch 100 loss: 1.3669574713706971
  batch 150 loss: 1.3593396353721618
  batch 200 loss: 1.3353608274459838
  batch 250 loss: 1.32356205701828
  batch 300 loss: 1.325834746360779
  batch 350 loss: 1.3761873984336852
  batch 400 loss: 1.3501836943626404
  batch 450 loss: 1.2969596433639525
  batch 500 loss: 1.2816979742050172
  batch 550 loss: 1.2890430796146393
  batch 600 loss: 1.2598787128925324
  batch 650 loss: 1.2332878601551056
  batch 700 loss: 1.25753098487854
  batch 750 loss: 1.3134243071079255
  batch 800 loss: 1.2299092626571655
  batch 850 loss: 1.2577338302135468
  batch 900 loss: 1.1847044456005096
LOSS train 1.18470 valid 1.23842, valid PER 37.35%
EPOCH 4:
  batch 50 loss: 1.175767147541046
  batch 100 loss: 1.1995079481601716
  batch 150 loss: 1.170527740716934
  batch 200 loss: 1.1934260308742524
  batch 250 loss: 1.1826539897918702
  batch 300 loss: 1.2033440220355986
  batch 350 loss: 1.1285266780853271
  batch 400 loss: 1.163538304567337
  batch 450 loss: 1.1525201570987702
  batch 500 loss: 1.1323366618156434
  batch 550 loss: 1.1672559869289398
  batch 600 loss: 1.1782698273658752
  batch 650 loss: 1.171171910762787
  batch 700 loss: 1.1252754247188568
  batch 750 loss: 1.1149279487133026
  batch 800 loss: 1.0736201691627503
  batch 850 loss: 1.1330116045475007
  batch 900 loss: 1.1531022822856902
LOSS train 1.15310 valid 1.14578, valid PER 35.49%
EPOCH 5:
  batch 50 loss: 1.0725993871688844
  batch 100 loss: 1.0686649763584137
  batch 150 loss: 1.1245849907398224
  batch 200 loss: 1.0518183660507203
  batch 250 loss: 1.0580739974975586
  batch 300 loss: 1.0770462727546692
  batch 350 loss: 1.0603117728233338
  batch 400 loss: 1.0841456317901612
  batch 450 loss: 1.051153188943863
  batch 500 loss: 1.0795434510707855
  batch 550 loss: 1.0191492342948913
  batch 600 loss: 1.116025323867798
  batch 650 loss: 1.0540744376182556
  batch 700 loss: 1.1017591321468354
  batch 750 loss: 1.0300898456573486
  batch 800 loss: 1.063237191438675
  batch 850 loss: 1.0504806637763977
  batch 900 loss: 1.0796254229545594
LOSS train 1.07963 valid 1.08298, valid PER 32.96%
EPOCH 6:
  batch 50 loss: 1.0482510590553284
  batch 100 loss: 0.9962842333316803
  batch 150 loss: 0.9921637558937073
  batch 200 loss: 1.002719830274582
  batch 250 loss: 1.0347056698799133
  batch 300 loss: 1.0220500886440278
  batch 350 loss: 1.0162054336071014
  batch 400 loss: 0.9874496293067933
  batch 450 loss: 1.0212633144855499
  batch 500 loss: 1.0012041807174683
  batch 550 loss: 1.0223623311519623
  batch 600 loss: 1.0108323287963867
  batch 650 loss: 1.000155223608017
  batch 700 loss: 1.0104462325572967
  batch 750 loss: 0.9824638926982879
  batch 800 loss: 0.9806032574176788
  batch 850 loss: 0.9773569309711456
  batch 900 loss: 0.9784632241725921
LOSS train 0.97846 valid 1.07689, valid PER 33.36%
EPOCH 7:
  batch 50 loss: 0.9749626731872558
  batch 100 loss: 0.9930242085456848
  batch 150 loss: 0.9487040674686432
  batch 200 loss: 0.9345952469110489
  batch 250 loss: 0.9330929362773895
  batch 300 loss: 0.9274621379375457
  batch 350 loss: 0.9604940557479859
  batch 400 loss: 0.9479837155342102
  batch 450 loss: 0.9595910251140595
  batch 500 loss: 0.9425406444072724
  batch 550 loss: 0.9392990410327912
  batch 600 loss: 0.960602056980133
  batch 650 loss: 0.9317150008678436
  batch 700 loss: 0.9786990988254547
  batch 750 loss: 0.93978910446167
  batch 800 loss: 0.9485823035240173
  batch 850 loss: 0.9736921632289887
  batch 900 loss: 0.9953665363788605
LOSS train 0.99537 valid 1.03533, valid PER 32.63%
EPOCH 8:
  batch 50 loss: 0.9187054693698883
  batch 100 loss: 0.909192408323288
  batch 150 loss: 0.9167801463603973
  batch 200 loss: 0.8860817968845367
  batch 250 loss: 0.9240623688697815
  batch 300 loss: 0.8686213850975036
  batch 350 loss: 0.9380479001998902
  batch 400 loss: 0.8868906843662262
  batch 450 loss: 0.9090902256965637
  batch 500 loss: 0.9516930723190308
  batch 550 loss: 0.9067591953277588
  batch 600 loss: 0.9229691553115845
  batch 650 loss: 0.9391998434066773
  batch 700 loss: 0.8938816297054291
  batch 750 loss: 0.9066672515869141
  batch 800 loss: 0.9393342041969299
  batch 850 loss: 0.9253871250152588
  batch 900 loss: 0.939894118309021
LOSS train 0.93989 valid 1.00592, valid PER 30.15%
EPOCH 9:
  batch 50 loss: 0.8533124816417694
  batch 100 loss: 0.8768428683280944
  batch 150 loss: 0.8721944463253021
  batch 200 loss: 0.8510148704051972
  batch 250 loss: 0.8844334077835083
  batch 300 loss: 0.893797653913498
  batch 350 loss: 0.9233224403858185
  batch 400 loss: 0.8778948378562927
  batch 450 loss: 0.8823034596443177
  batch 500 loss: 0.8580307972431183
  batch 550 loss: 0.8943316948413849
  batch 600 loss: 0.8850892174243927
  batch 650 loss: 0.8711121809482575
  batch 700 loss: 0.8697285580635071
  batch 750 loss: 0.8713331425189972
  batch 800 loss: 0.8945420837402344
  batch 850 loss: 0.9057783019542694
  batch 900 loss: 0.8524092817306519
LOSS train 0.85241 valid 0.98465, valid PER 29.89%
EPOCH 10:
  batch 50 loss: 0.809445241689682
  batch 100 loss: 0.8175178098678589
  batch 150 loss: 0.8623033535480499
  batch 200 loss: 0.8590642738342286
  batch 250 loss: 0.8637694835662841
  batch 300 loss: 0.8396895408630372
  batch 350 loss: 0.8492577600479126
  batch 400 loss: 0.8192574310302735
  batch 450 loss: 0.826281909942627
  batch 500 loss: 0.8685844695568085
  batch 550 loss: 0.8763345432281494
  batch 600 loss: 0.8418112778663636
  batch 650 loss: 0.8258707183599472
  batch 700 loss: 0.8580147182941437
  batch 750 loss: 0.8347731256484985
  batch 800 loss: 0.8418561291694641
  batch 850 loss: 0.8567815017700195
  batch 900 loss: 0.8733232951164246
LOSS train 0.87332 valid 0.98002, valid PER 31.23%
EPOCH 11:
  batch 50 loss: 0.7961822760105133
  batch 100 loss: 0.7789425563812256
  batch 150 loss: 0.7964581596851349
  batch 200 loss: 0.8601046538352967
  batch 250 loss: 0.8323027908802032
  batch 300 loss: 0.7976491332054139
  batch 350 loss: 0.8391402053833008
  batch 400 loss: 0.8412018811702728
  batch 450 loss: 0.8287548959255219
  batch 500 loss: 0.8047258615493774
  batch 550 loss: 0.8311048042774201
  batch 600 loss: 0.8079025495052338
  batch 650 loss: 0.8694194555282593
  batch 700 loss: 0.79037557721138
  batch 750 loss: 0.8130792653560639
  batch 800 loss: 0.84479900598526
  batch 850 loss: 0.8595646035671234
  batch 900 loss: 0.8633538258075714
LOSS train 0.86335 valid 0.95641, valid PER 28.87%
EPOCH 12:
  batch 50 loss: 0.7863236343860627
  batch 100 loss: 0.7919759213924408
  batch 150 loss: 0.7624706363677979
  batch 200 loss: 0.7885989344120026
  batch 250 loss: 0.8159079945087433
  batch 300 loss: 0.794139930009842
  batch 350 loss: 0.7751961696147919
  batch 400 loss: 0.8148937749862671
  batch 450 loss: 0.815094358921051
  batch 500 loss: 0.8231992435455322
  batch 550 loss: 0.7545351707935333
  batch 600 loss: 0.7797411775588989
  batch 650 loss: 0.8433546555042267
  batch 700 loss: 0.8108588516712188
  batch 750 loss: 0.7898675930500031
  batch 800 loss: 0.7954008889198303
  batch 850 loss: 0.8449585258960723
  batch 900 loss: 0.8349695754051208
LOSS train 0.83497 valid 0.95573, valid PER 29.08%
EPOCH 13:
  batch 50 loss: 0.7527496767044067
  batch 100 loss: 0.7750047945976257
  batch 150 loss: 0.753598524928093
  batch 200 loss: 0.7698673951625824
  batch 250 loss: 0.7806146633625031
  batch 300 loss: 0.7454561233520508
  batch 350 loss: 0.7618563938140869
  batch 400 loss: 0.790842922925949
  batch 450 loss: 0.7928971433639527
  batch 500 loss: 0.7589438754320145
  batch 550 loss: 0.8077862882614135
  batch 600 loss: 0.7694003105163574
  batch 650 loss: 0.7980968260765076
  batch 700 loss: 0.7985468345880509
  batch 750 loss: 0.7451137363910675
  batch 800 loss: 0.7924853789806366
  batch 850 loss: 0.8012762904167176
  batch 900 loss: 0.7949492597579956
LOSS train 0.79495 valid 0.95119, valid PER 28.46%
EPOCH 14:
  batch 50 loss: 0.7243127083778381
  batch 100 loss: 0.7522138500213623
  batch 150 loss: 0.738267965912819
  batch 200 loss: 0.7441424548625946
  batch 250 loss: 0.7342362129688262
  batch 300 loss: 0.7915048587322235
  batch 350 loss: 0.7572951912879944
  batch 400 loss: 0.7448770421743393
  batch 450 loss: 0.7254944455623626
  batch 500 loss: 0.7590028446912765
  batch 550 loss: 0.7723959803581237
  batch 600 loss: 0.7408672952651978
  batch 650 loss: 0.7794608199596404
  batch 700 loss: 0.8032400822639465
  batch 750 loss: 0.7574008333683014
  batch 800 loss: 0.7126203858852387
  batch 850 loss: 0.7891121113300323
  batch 900 loss: 0.7595764923095704
LOSS train 0.75958 valid 0.94712, valid PER 28.70%
EPOCH 15:
  batch 50 loss: 0.7202666461467743
  batch 100 loss: 0.7069644677639008
  batch 150 loss: 0.7219661039113998
  batch 200 loss: 0.7391637319326401
  batch 250 loss: 0.7484252500534058
  batch 300 loss: 0.7370269334316254
  batch 350 loss: 0.7302199256420135
  batch 400 loss: 0.7257376992702484
  batch 450 loss: 0.7347931802272797
  batch 500 loss: 0.7290196776390075
  batch 550 loss: 0.7511840450763703
  batch 600 loss: 0.7673172962665558
  batch 650 loss: 0.7789169538021088
  batch 700 loss: 0.7576319789886474
  batch 750 loss: 0.7562620162963867
  batch 800 loss: 0.7535003662109375
  batch 850 loss: 0.7104474204778671
  batch 900 loss: 0.7553398162126541
LOSS train 0.75534 valid 0.94918, valid PER 28.91%
EPOCH 16:
  batch 50 loss: 0.732907943725586
  batch 100 loss: 0.6959375715255738
  batch 150 loss: 0.7004863286018371
  batch 200 loss: 0.7151596069335937
  batch 250 loss: 0.728991105556488
  batch 300 loss: 0.7301907962560654
  batch 350 loss: 0.7301406985521317
  batch 400 loss: 0.7164732223749161
  batch 450 loss: 0.7294698441028595
  batch 500 loss: 0.7059458649158478
  batch 550 loss: 0.7292277729511261
  batch 600 loss: 0.7175499403476715
  batch 650 loss: 0.7442912888526917
  batch 700 loss: 0.7145639199018479
  batch 750 loss: 0.745429282784462
  batch 800 loss: 0.7172625541687012
  batch 850 loss: 0.7145817053318023
  batch 900 loss: 0.713941160440445
LOSS train 0.71394 valid 0.93102, valid PER 27.35%
EPOCH 17:
  batch 50 loss: 0.6785894268751145
  batch 100 loss: 0.6923125970363617
  batch 150 loss: 0.6815835767984391
  batch 200 loss: 0.6778367418050766
  batch 250 loss: 0.7115315449237823
  batch 300 loss: 0.7057294481992722
  batch 350 loss: 0.6815655225515366
  batch 400 loss: 0.7262056279182434
  batch 450 loss: 0.7112156242132187
  batch 500 loss: 0.6978343349695205
  batch 550 loss: 0.6952550458908081
  batch 600 loss: 0.7298980295658112
  batch 650 loss: 0.6855050349235534
  batch 700 loss: 0.696883704662323
  batch 750 loss: 0.6829238045215607
  batch 800 loss: 0.6962824726104736
  batch 850 loss: 0.7225565999746323
  batch 900 loss: 0.7136729073524475
LOSS train 0.71367 valid 0.94399, valid PER 28.03%
EPOCH 18:
  batch 50 loss: 0.6964321613311768
  batch 100 loss: 0.6809686231613159
  batch 150 loss: 0.6977060091495514
  batch 200 loss: 0.6852626019716263
  batch 250 loss: 0.6924082899093628
  batch 300 loss: 0.6818077635765075
  batch 350 loss: 0.6911464542150497
  batch 400 loss: 0.664497761130333
  batch 450 loss: 0.7076782232522965
  batch 500 loss: 0.6919914788007736
  batch 550 loss: 0.7109554105997086
  batch 600 loss: 0.6591725140810013
  batch 650 loss: 0.6565101861953735
  batch 700 loss: 0.7084714132547378
  batch 750 loss: 0.6986081105470657
  batch 800 loss: 0.6740259569883347
  batch 850 loss: 0.6965233433246613
  batch 900 loss: 0.717087928056717
LOSS train 0.71709 valid 0.95073, valid PER 28.63%
EPOCH 19:
  batch 50 loss: 0.6472733515501022
  batch 100 loss: 0.6360821092128753
  batch 150 loss: 0.6534762686491012
  batch 200 loss: 0.6542494058609009
  batch 250 loss: 0.658726440668106
  batch 300 loss: 0.659804875254631
  batch 350 loss: 0.6451180338859558
  batch 400 loss: 0.6680916672945023
  batch 450 loss: 0.6646930485963821
  batch 500 loss: 0.6813498824834824
  batch 550 loss: 0.65517628967762
  batch 600 loss: 0.6593221628665924
  batch 650 loss: 0.7278643053770065
  batch 700 loss: 0.6580046331882476
  batch 750 loss: 0.6477458471059799
  batch 800 loss: 0.7007192194461822
  batch 850 loss: 0.7045708465576171
  batch 900 loss: 0.6973691356182098
LOSS train 0.69737 valid 0.96707, valid PER 28.27%
EPOCH 20:
  batch 50 loss: 0.6403357595205307
  batch 100 loss: 0.6230597358942032
  batch 150 loss: 0.6193368709087372
  batch 200 loss: 0.6519905668497086
  batch 250 loss: 0.6326899898052215
  batch 300 loss: 0.6603697448968887
  batch 350 loss: 0.6255221199989319
  batch 400 loss: 0.6456224149465561
  batch 450 loss: 0.6485665732622147
  batch 500 loss: 0.6387149238586426
  batch 550 loss: 0.6865020191669464
  batch 600 loss: 0.6398628574609756
  batch 650 loss: 0.6774626624584198
  batch 700 loss: 0.6593015885353088
  batch 750 loss: 0.6399183130264282
  batch 800 loss: 0.6738309234380722
  batch 850 loss: 0.6731745755672455
  batch 900 loss: 0.6867417722940445
LOSS train 0.68674 valid 0.94178, valid PER 27.26%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230117_190452/model_16
Loading model from checkpoints/20230117_190452/model_16
SUB: 16.89%, DEL: 9.20%, INS: 3.28%, COR: 73.91%, PER: 29.38%
