Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=1.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.036503577232361
  batch 100 loss: 3.314820384979248
  batch 150 loss: 3.155870838165283
  batch 200 loss: 2.885014910697937
  batch 250 loss: 2.707704782485962
  batch 300 loss: 2.5372381448745727
  batch 350 loss: 2.426200394630432
  batch 400 loss: 2.3649616289138793
  batch 450 loss: 2.2806033182144163
  batch 500 loss: 2.174742729663849
  batch 550 loss: 2.1107350969314576
  batch 600 loss: 2.0396098732948302
  batch 650 loss: 1.961008551120758
  batch 700 loss: 1.9532167744636535
  batch 750 loss: 1.8846416854858399
  batch 800 loss: 1.8592492127418518
  batch 850 loss: 1.8069216847419738
  batch 900 loss: 1.7846800994873047
LOSS train 1.78468 valid 1.71948, valid PER 63.18%
EPOCH 2:
  batch 50 loss: 1.712606053352356
  batch 100 loss: 1.6564813566207885
  batch 150 loss: 1.6369680285453796
  batch 200 loss: 1.6406964230537415
  batch 250 loss: 1.648039195537567
  batch 300 loss: 1.615316300392151
  batch 350 loss: 1.5277452206611633
  batch 400 loss: 1.541070454120636
  batch 450 loss: 1.4909495949745177
  batch 500 loss: 1.5197594547271729
  batch 550 loss: 1.5253570032119752
  batch 600 loss: 1.4822612476348878
  batch 650 loss: 1.5026563000679016
  batch 700 loss: 1.4582327604293823
  batch 750 loss: 1.4438368082046509
  batch 800 loss: 1.3964328908920287
  batch 850 loss: 1.3861415457725526
  batch 900 loss: 1.4241153836250304
LOSS train 1.42412 valid 1.37345, valid PER 44.68%
EPOCH 3:
  batch 50 loss: 1.370402250289917
  batch 100 loss: 1.3328727531433104
  batch 150 loss: 1.3374714136123658
  batch 200 loss: 1.305373272895813
  batch 250 loss: 1.301058430671692
  batch 300 loss: 1.2973523926734924
  batch 350 loss: 1.3374683260917664
  batch 400 loss: 1.2970182347297667
  batch 450 loss: 1.2875037145614625
  batch 500 loss: 1.2558727741241456
  batch 550 loss: 1.2637799870967865
  batch 600 loss: 1.2380668115615845
  batch 650 loss: 1.2088419604301452
  batch 700 loss: 1.23539177775383
  batch 750 loss: 1.2792451357841492
  batch 800 loss: 1.214697071313858
  batch 850 loss: 1.2327074241638183
  batch 900 loss: 1.1948058569431306
LOSS train 1.19481 valid 1.22993, valid PER 36.90%
EPOCH 4:
  batch 50 loss: 1.1765260267257691
  batch 100 loss: 1.183961066007614
  batch 150 loss: 1.136143945455551
  batch 200 loss: 1.1753363370895387
  batch 250 loss: 1.1841610956192017
  batch 300 loss: 1.1905672311782838
  batch 350 loss: 1.1086850225925446
  batch 400 loss: 1.1721323907375336
  batch 450 loss: 1.1291960287094116
  batch 500 loss: 1.1187979245185853
  batch 550 loss: 1.1561617314815522
  batch 600 loss: 1.1614119279384614
  batch 650 loss: 1.14044615983963
  batch 700 loss: 1.1147107887268066
  batch 750 loss: 1.1001841688156129
  batch 800 loss: 1.0762265264987945
  batch 850 loss: 1.1077267479896546
  batch 900 loss: 1.1485509192943573
LOSS train 1.14855 valid 1.14921, valid PER 35.34%
EPOCH 5:
  batch 50 loss: 1.059573768377304
  batch 100 loss: 1.0614356005191803
  batch 150 loss: 1.1083060944080352
  batch 200 loss: 1.0555731511116029
  batch 250 loss: 1.0420440459251403
  batch 300 loss: 1.0593755757808685
  batch 350 loss: 1.0631403756141662
  batch 400 loss: 1.05876011967659
  batch 450 loss: 1.0510557949543
  batch 500 loss: 1.061648336648941
  batch 550 loss: 1.0173225021362304
  batch 600 loss: 1.09635085105896
  batch 650 loss: 1.0401903128623962
  batch 700 loss: 1.073357025384903
  batch 750 loss: 1.0111066114902496
  batch 800 loss: 1.051414020061493
  batch 850 loss: 1.0543590319156646
  batch 900 loss: 1.055882625579834
LOSS train 1.05588 valid 1.08150, valid PER 34.12%
EPOCH 6:
  batch 50 loss: 1.0412936627864837
  batch 100 loss: 0.9997198915481568
  batch 150 loss: 0.9909988260269165
  batch 200 loss: 1.0319692265987397
  batch 250 loss: 1.0315225327014923
  batch 300 loss: 1.0014759635925292
  batch 350 loss: 1.0077984762191772
  batch 400 loss: 0.9985646569728851
  batch 450 loss: 1.0468740594387054
  batch 500 loss: 1.013072315454483
  batch 550 loss: 1.0342673408985137
  batch 600 loss: 0.9799973773956299
  batch 650 loss: 0.9939532649517059
  batch 700 loss: 0.9955579686164856
  batch 750 loss: 0.9799359357357025
  batch 800 loss: 0.9740160846710205
  batch 850 loss: 0.9716265320777893
  batch 900 loss: 0.9886392652988434
LOSS train 0.98864 valid 1.05466, valid PER 32.79%
EPOCH 7:
  batch 50 loss: 0.9644557344913482
  batch 100 loss: 0.9811050927639008
  batch 150 loss: 0.9736694014072418
  batch 200 loss: 0.9499101364612579
  batch 250 loss: 0.9471888196468353
  batch 300 loss: 0.945966078042984
  batch 350 loss: 0.9568344306945801
  batch 400 loss: 0.9501895463466644
  batch 450 loss: 0.9476807940006257
  batch 500 loss: 0.9528324627876281
  batch 550 loss: 0.9727444064617157
  batch 600 loss: 0.9516412937641143
  batch 650 loss: 0.944781060218811
  batch 700 loss: 0.9906679630279541
  batch 750 loss: 0.9483491635322571
  batch 800 loss: 0.9320061945915222
  batch 850 loss: 0.9661272990703583
  batch 900 loss: 0.9928824245929718
LOSS train 0.99288 valid 1.04658, valid PER 32.38%
EPOCH 8:
  batch 50 loss: 0.9004532516002655
  batch 100 loss: 0.9322006535530091
  batch 150 loss: 0.9212719786167145
  batch 200 loss: 0.9030472767353058
  batch 250 loss: 0.946009088754654
  batch 300 loss: 0.8871291208267212
  batch 350 loss: 0.9401996314525605
  batch 400 loss: 0.9113436937332153
  batch 450 loss: 0.9234180235862732
  batch 500 loss: 0.9667264294624328
  batch 550 loss: 0.9043763828277588
  batch 600 loss: 0.9343505525588989
  batch 650 loss: 0.9513742661476136
  batch 700 loss: 0.9019648635387421
  batch 750 loss: 0.9133035242557526
  batch 800 loss: 0.9156631016731263
  batch 850 loss: 0.9153133428096771
  batch 900 loss: 0.9034270668029785
LOSS train 0.90343 valid 1.02556, valid PER 31.12%
EPOCH 9:
  batch 50 loss: 0.8543314945697784
  batch 100 loss: 0.9005596303939819
  batch 150 loss: 0.9016952228546142
  batch 200 loss: 0.8564530056715012
  batch 250 loss: 0.8878793239593505
  batch 300 loss: 0.8946489441394806
  batch 350 loss: 0.9215562784671784
  batch 400 loss: 0.9136011719703674
  batch 450 loss: 0.8880681562423706
  batch 500 loss: 0.861662563085556
  batch 550 loss: 0.8859840893745422
  batch 600 loss: 0.8965568733215332
  batch 650 loss: 0.8652333915233612
  batch 700 loss: 0.872814017534256
  batch 750 loss: 0.8863401818275451
  batch 800 loss: 0.9363354516029357
  batch 850 loss: 0.9212834858894348
  batch 900 loss: 0.8510473835468292
LOSS train 0.85105 valid 0.98755, valid PER 30.89%
EPOCH 10:
  batch 50 loss: 0.8046404123306274
  batch 100 loss: 0.8407766020298004
  batch 150 loss: 0.8622778594493866
  batch 200 loss: 0.8601579117774963
  batch 250 loss: 0.8795382571220398
  batch 300 loss: 0.8215342783927917
  batch 350 loss: 0.8788101089000702
  batch 400 loss: 0.815696907043457
  batch 450 loss: 0.8456706225872039
  batch 500 loss: 0.8941632056236267
  batch 550 loss: 0.8724104595184327
  batch 600 loss: 0.8617305362224579
  batch 650 loss: 0.8551901769638062
  batch 700 loss: 0.8901753914356232
  batch 750 loss: 0.846755005121231
  batch 800 loss: 0.8745687532424927
  batch 850 loss: 0.8771039569377899
  batch 900 loss: 0.8684139931201935
LOSS train 0.86841 valid 0.99178, valid PER 31.14%
EPOCH 11:
  batch 50 loss: 0.8043725973367691
  batch 100 loss: 0.7965459287166595
  batch 150 loss: 0.8025353085994721
  batch 200 loss: 0.8620554614067077
  batch 250 loss: 0.8458243441581726
  batch 300 loss: 0.8002499258518219
  batch 350 loss: 0.8148749184608459
  batch 400 loss: 0.8358915185928345
  batch 450 loss: 0.8482432925701141
  batch 500 loss: 0.819464908838272
  batch 550 loss: 0.8365659248828888
  batch 600 loss: 0.806947125196457
  batch 650 loss: 0.872896534204483
  batch 700 loss: 0.8266819751262665
  batch 750 loss: 0.819657222032547
  batch 800 loss: 0.8426775264739991
  batch 850 loss: 0.8828791677951813
  batch 900 loss: 0.8594634068012238
LOSS train 0.85946 valid 0.95981, valid PER 29.88%
EPOCH 12:
  batch 50 loss: 0.8150671410560608
  batch 100 loss: 0.8008004915714264
  batch 150 loss: 0.7742274183034897
  batch 200 loss: 0.7932211577892303
  batch 250 loss: 0.8155495989322662
  batch 300 loss: 0.8049628055095672
  batch 350 loss: 0.7972192919254303
  batch 400 loss: 0.8156873321533203
  batch 450 loss: 0.8139925682544709
  batch 500 loss: 0.8334408462047577
  batch 550 loss: 0.7705740284919739
  batch 600 loss: 0.790579115152359
  batch 650 loss: 0.8382565975189209
  batch 700 loss: 0.8258450865745545
  batch 750 loss: 0.7897907733917237
  batch 800 loss: 0.7999859321117401
  batch 850 loss: 0.8438991904258728
  batch 900 loss: 0.8377195227146149
LOSS train 0.83772 valid 0.96408, valid PER 30.04%
EPOCH 13:
  batch 50 loss: 0.768172116279602
  batch 100 loss: 0.776692858338356
  batch 150 loss: 0.7564134657382965
  batch 200 loss: 0.7882283711433411
  batch 250 loss: 0.7715224540233612
  batch 300 loss: 0.7788541483879089
  batch 350 loss: 0.7646367424726486
  batch 400 loss: 0.7983807146549224
  batch 450 loss: 0.8007481372356415
  batch 500 loss: 0.7565707433223724
  batch 550 loss: 0.7990723288059235
  batch 600 loss: 0.771123434305191
  batch 650 loss: 0.7850032103061676
  batch 700 loss: 0.796731978058815
  batch 750 loss: 0.7527003574371338
  batch 800 loss: 0.7836647975444794
  batch 850 loss: 0.8216879749298096
  batch 900 loss: 0.8075209474563598
LOSS train 0.80752 valid 0.95437, valid PER 29.29%
EPOCH 14:
  batch 50 loss: 0.7484645599126816
  batch 100 loss: 0.761983402967453
  batch 150 loss: 0.7656886756420136
  batch 200 loss: 0.7379431587457657
  batch 250 loss: 0.7585684657096863
  batch 300 loss: 0.7852813839912415
  batch 350 loss: 0.7497106254100799
  batch 400 loss: 0.7595176351070404
  batch 450 loss: 0.7569621616601944
  batch 500 loss: 0.7673925411701202
  batch 550 loss: 0.7796347033977509
  batch 600 loss: 0.7433847224712372
  batch 650 loss: 0.7789014059305192
  batch 700 loss: 0.7988544058799744
  batch 750 loss: 0.7605309855937957
  batch 800 loss: 0.7359670734405518
  batch 850 loss: 0.8082274460792541
  batch 900 loss: 0.7675175690650939
LOSS train 0.76752 valid 0.95350, valid PER 29.06%
EPOCH 15:
  batch 50 loss: 0.7434855258464813
  batch 100 loss: 0.7262998449802399
  batch 150 loss: 0.7409905111789703
  batch 200 loss: 0.7678797328472138
  batch 250 loss: 0.7481112217903138
  batch 300 loss: 0.7615032970905304
  batch 350 loss: 0.7357480674982071
  batch 400 loss: 0.7402446156740189
  batch 450 loss: 0.7471358519792557
  batch 500 loss: 0.7125130569934846
  batch 550 loss: 0.7396641027927399
  batch 600 loss: 0.7828334641456604
  batch 650 loss: 0.7576039099693298
  batch 700 loss: 0.7609411156177521
  batch 750 loss: 0.7491346979141236
  batch 800 loss: 0.7428906643390656
  batch 850 loss: 0.7249113750457764
  batch 900 loss: 0.7619875609874726
LOSS train 0.76199 valid 0.94841, valid PER 29.02%
EPOCH 16:
  batch 50 loss: 0.7262249684333801
  batch 100 loss: 0.6981192171573639
  batch 150 loss: 0.7033692955970764
  batch 200 loss: 0.721963222026825
  batch 250 loss: 0.7394811713695526
  batch 300 loss: 0.7109902900457382
  batch 350 loss: 0.7399898171424866
  batch 400 loss: 0.7415228056907653
  batch 450 loss: 0.7456395626068115
  batch 500 loss: 0.7411109340190888
  batch 550 loss: 0.7514163756370544
  batch 600 loss: 0.7440983772277832
  batch 650 loss: 0.7474006056785584
  batch 700 loss: 0.712164255976677
  batch 750 loss: 0.7316041493415832
  batch 800 loss: 0.7295726025104523
  batch 850 loss: 0.7489663076400757
  batch 900 loss: 0.7496695673465729
LOSS train 0.74967 valid 0.95992, valid PER 28.59%
EPOCH 17:
  batch 50 loss: 0.7140249764919281
  batch 100 loss: 0.7017491686344147
  batch 150 loss: 0.6734100502729415
  batch 200 loss: 0.6955988585948945
  batch 250 loss: 0.7179750293493271
  batch 300 loss: 0.7030733227729797
  batch 350 loss: 0.684266722202301
  batch 400 loss: 0.729677711725235
  batch 450 loss: 0.7196719408035278
  batch 500 loss: 0.6928660434484482
  batch 550 loss: 0.7169773751497268
  batch 600 loss: 0.7664006006717682
  batch 650 loss: 0.7066247570514679
  batch 700 loss: 0.7058056962490081
  batch 750 loss: 0.7085510677099228
  batch 800 loss: 0.6940559029579163
  batch 850 loss: 0.7138371986150741
  batch 900 loss: 0.6892428672313691
LOSS train 0.68924 valid 0.95862, valid PER 28.35%
EPOCH 18:
  batch 50 loss: 0.6768086659908295
  batch 100 loss: 0.6934287369251251
  batch 150 loss: 0.7062060981988907
  batch 200 loss: 0.6959504210948944
  batch 250 loss: 0.7063845086097718
  batch 300 loss: 0.671938693523407
  batch 350 loss: 0.6996742033958435
  batch 400 loss: 0.6825467813014984
  batch 450 loss: 0.7126537185907363
  batch 500 loss: 0.695732308626175
  batch 550 loss: 0.7027209788560868
  batch 600 loss: 0.6776127249002457
  batch 650 loss: 0.6874975478649139
  batch 700 loss: 0.7247344732284546
  batch 750 loss: 0.6826013338565826
  batch 800 loss: 0.6889549332857132
  batch 850 loss: 0.7067844301462174
  batch 900 loss: 0.7195003414154053
LOSS train 0.71950 valid 0.94127, valid PER 28.50%
EPOCH 19:
  batch 50 loss: 0.6372625946998596
  batch 100 loss: 0.6434544217586518
  batch 150 loss: 0.6755920660495758
  batch 200 loss: 0.6770787793397903
  batch 250 loss: 0.6827605336904525
  batch 300 loss: 0.6851041609048844
  batch 350 loss: 0.656435894370079
  batch 400 loss: 0.6782383322715759
  batch 450 loss: 0.675759899020195
  batch 500 loss: 0.6983644354343415
  batch 550 loss: 0.6728264319896698
  batch 600 loss: 0.6907626283168793
  batch 650 loss: 0.7482122695446014
  batch 700 loss: 0.6786746698617935
  batch 750 loss: 0.6755085611343383
  batch 800 loss: 0.717942413687706
  batch 850 loss: 0.6953703254461289
  batch 900 loss: 0.6786034625768661
LOSS train 0.67860 valid 0.94708, valid PER 28.69%
EPOCH 20:
  batch 50 loss: 0.6382737380266189
  batch 100 loss: 0.6279915732145309
  batch 150 loss: 0.6438528603315353
  batch 200 loss: 0.6599980890750885
  batch 250 loss: 0.6428843683004379
  batch 300 loss: 0.6756342792510986
  batch 350 loss: 0.6417138367891312
  batch 400 loss: 0.6582675093412399
  batch 450 loss: 0.6724931430816651
  batch 500 loss: 0.6389498794078827
  batch 550 loss: 0.6962184625864029
  batch 600 loss: 0.6331978631019592
  batch 650 loss: 0.6819484257698059
  batch 700 loss: 0.6933699226379395
  batch 750 loss: 0.6537535148859024
  batch 800 loss: 0.6865044438838959
  batch 850 loss: 0.6857738345861435
  batch 900 loss: 0.6804091995954513
LOSS train 0.68041 valid 0.94590, valid PER 28.52%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_191002/model_18
Loading model from checkpoints/20230117_191002/model_18
SUB: 16.46%, DEL: 11.21%, INS: 2.17%, COR: 72.33%, PER: 29.83%
