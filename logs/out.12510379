Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=2.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.579171857833862
  batch 100 loss: 3.193232226371765
  batch 150 loss: 3.0363841438293457
  batch 200 loss: 2.734588222503662
  batch 250 loss: 2.558543577194214
  batch 300 loss: 2.3948286247253416
  batch 350 loss: 2.2578907799720764
  batch 400 loss: 2.2580644249916078
  batch 450 loss: 2.1763476657867433
  batch 500 loss: 2.0560628175735474
  batch 550 loss: 2.026130862236023
  batch 600 loss: 1.962766420841217
  batch 650 loss: 1.9062754344940185
  batch 700 loss: 1.8812804913520813
  batch 750 loss: 1.8168371748924255
  batch 800 loss: 1.802203071117401
  batch 850 loss: 1.7682004952430725
  batch 900 loss: 1.7380948877334594
LOSS train 1.73809 valid 1.70650, valid PER 65.45%
EPOCH 2:
  batch 50 loss: 1.722907772064209
  batch 100 loss: 1.6286458110809325
  batch 150 loss: 1.6463290786743163
  batch 200 loss: 1.6373427820205688
  batch 250 loss: 1.6281350708007813
  batch 300 loss: 1.5964626908302306
  batch 350 loss: 1.5098517775535583
  batch 400 loss: 1.5298552441596984
  batch 450 loss: 1.485472514629364
  batch 500 loss: 1.5236953067779542
  batch 550 loss: 1.5174287700653075
  batch 600 loss: 1.4665167832374573
  batch 650 loss: 1.4847188353538514
  batch 700 loss: 1.466301248073578
  batch 750 loss: 1.471340651512146
  batch 800 loss: 1.3871220541000366
  batch 850 loss: 1.417625744342804
  batch 900 loss: 1.4155195021629334
LOSS train 1.41552 valid 1.37829, valid PER 46.42%
EPOCH 3:
  batch 50 loss: 1.3749786019325256
  batch 100 loss: 1.3376707983016969
  batch 150 loss: 1.327624680995941
  batch 200 loss: 1.3183085250854492
  batch 250 loss: 1.297470030784607
  batch 300 loss: 1.2970267140865326
  batch 350 loss: 1.341192524433136
  batch 400 loss: 1.3209762406349181
  batch 450 loss: 1.2596811521053315
  batch 500 loss: 1.2570849251747132
  batch 550 loss: 1.2755446553230285
  batch 600 loss: 1.2598504543304443
  batch 650 loss: 1.1966347730159759
  batch 700 loss: 1.2364728951454163
  batch 750 loss: 1.3043995475769044
  batch 800 loss: 1.2004446816444396
  batch 850 loss: 1.2494281876087188
  batch 900 loss: 1.1723275351524354
LOSS train 1.17233 valid 1.28541, valid PER 38.95%
EPOCH 4:
  batch 50 loss: 1.1770786952972412
  batch 100 loss: 1.1943253350257874
  batch 150 loss: 1.1316977846622467
  batch 200 loss: 1.1732385194301604
  batch 250 loss: 1.1812209320068359
  batch 300 loss: 1.1877517545223235
  batch 350 loss: 1.1263172221183777
  batch 400 loss: 1.1533447694778443
  batch 450 loss: 1.1452999210357666
  batch 500 loss: 1.1105293929576874
  batch 550 loss: 1.1442029416561126
  batch 600 loss: 1.1682696914672852
  batch 650 loss: 1.1433985459804534
  batch 700 loss: 1.1129721796512604
  batch 750 loss: 1.0985475265979767
  batch 800 loss: 1.0569186151027679
  batch 850 loss: 1.1140874493122102
  batch 900 loss: 1.1559273648262023
LOSS train 1.15593 valid 1.12222, valid PER 35.34%
EPOCH 5:
  batch 50 loss: 1.0720590245723725
  batch 100 loss: 1.0634568560123443
  batch 150 loss: 1.1074451434612274
  batch 200 loss: 1.0402692329883576
  batch 250 loss: 1.0538697588443755
  batch 300 loss: 1.0529695236682892
  batch 350 loss: 1.0557477176189423
  batch 400 loss: 1.0683546447753907
  batch 450 loss: 1.0373411929607392
  batch 500 loss: 1.0809195160865783
  batch 550 loss: 1.023054187297821
  batch 600 loss: 1.1084616684913635
  batch 650 loss: 1.0694741654396056
  batch 700 loss: 1.09644167304039
  batch 750 loss: 1.01198197722435
  batch 800 loss: 1.0602864837646484
  batch 850 loss: 1.0511603188514709
  batch 900 loss: 1.0643915116786957
LOSS train 1.06439 valid 1.07212, valid PER 32.98%
EPOCH 6:
  batch 50 loss: 1.0469113624095916
  batch 100 loss: 0.975095989704132
  batch 150 loss: 0.9805740714073181
  batch 200 loss: 1.0035484981536866
  batch 250 loss: 1.0332189166545869
  batch 300 loss: 1.0222300064563752
  batch 350 loss: 1.0087109410762787
  batch 400 loss: 0.9874260234832763
  batch 450 loss: 1.0195159375667573
  batch 500 loss: 1.0082973229885102
  batch 550 loss: 1.02904900431633
  batch 600 loss: 0.9853039991855621
  batch 650 loss: 1.0125545728206635
  batch 700 loss: 1.0079294455051422
  batch 750 loss: 0.9929058670997619
  batch 800 loss: 0.997493714094162
  batch 850 loss: 0.9724334371089935
  batch 900 loss: 1.0112977588176728
LOSS train 1.01130 valid 1.06472, valid PER 32.72%
EPOCH 7:
  batch 50 loss: 0.9897590351104736
  batch 100 loss: 0.9938426721096039
  batch 150 loss: 0.9530097615718841
  batch 200 loss: 0.9382857513427735
  batch 250 loss: 0.9489260601997376
  batch 300 loss: 0.959010694026947
  batch 350 loss: 0.962731271982193
  batch 400 loss: 0.9490414667129516
  batch 450 loss: 0.9613257753849029
  batch 500 loss: 0.9611760282516479
  batch 550 loss: 0.9543646967411041
  batch 600 loss: 0.9557081663608551
  batch 650 loss: 0.9271575820446014
  batch 700 loss: 0.9711300098896026
  batch 750 loss: 0.9281389963626862
  batch 800 loss: 0.9379318308830261
  batch 850 loss: 0.9773805356025695
  batch 900 loss: 0.9933672833442688
LOSS train 0.99337 valid 1.02119, valid PER 31.66%
EPOCH 8:
  batch 50 loss: 0.9198115718364716
  batch 100 loss: 0.9052175450325012
  batch 150 loss: 0.9198434460163116
  batch 200 loss: 0.8837826418876648
  batch 250 loss: 0.9179406094551087
  batch 300 loss: 0.8524288392066955
  batch 350 loss: 0.9331461548805237
  batch 400 loss: 0.8977479314804078
  batch 450 loss: 0.9156005465984345
  batch 500 loss: 0.9410848546028138
  batch 550 loss: 0.8881285083293915
  batch 600 loss: 0.9262105357646943
  batch 650 loss: 0.9316663706302643
  batch 700 loss: 0.8898758327960968
  batch 750 loss: 0.8841719818115235
  batch 800 loss: 0.9214532315731049
  batch 850 loss: 0.9179690742492675
  batch 900 loss: 0.9298498260974885
LOSS train 0.92985 valid 1.01642, valid PER 29.68%
EPOCH 9:
  batch 50 loss: 0.8395585775375366
  batch 100 loss: 0.8709845590591431
  batch 150 loss: 0.8651562654972076
  batch 200 loss: 0.8604561567306519
  batch 250 loss: 0.8891468811035156
  batch 300 loss: 0.9111568546295166
  batch 350 loss: 0.9289525377750397
  batch 400 loss: 0.9054238164424896
  batch 450 loss: 0.890727573633194
  batch 500 loss: 0.8545732474327088
  batch 550 loss: 0.9041882288455964
  batch 600 loss: 0.9028525531291962
  batch 650 loss: 0.8780401682853699
  batch 700 loss: 0.8501684832572937
  batch 750 loss: 0.8834995472431183
  batch 800 loss: 0.8961518311500549
  batch 850 loss: 0.8900594115257263
  batch 900 loss: 0.8480734670162201
LOSS train 0.84807 valid 0.98800, valid PER 30.42%
EPOCH 10:
  batch 50 loss: 0.7991360127925873
  batch 100 loss: 0.8190041553974151
  batch 150 loss: 0.8460556888580322
  batch 200 loss: 0.845423983335495
  batch 250 loss: 0.8624547624588013
  batch 300 loss: 0.84174919962883
  batch 350 loss: 0.8412699925899506
  batch 400 loss: 0.8270152246952057
  batch 450 loss: 0.8273704326152802
  batch 500 loss: 0.8610102987289429
  batch 550 loss: 0.8804670238494873
  batch 600 loss: 0.8435760521888733
  batch 650 loss: 0.8310550355911255
  batch 700 loss: 0.8668364059925079
  batch 750 loss: 0.8262934660911561
  batch 800 loss: 0.8590608251094818
  batch 850 loss: 0.8576677000522613
  batch 900 loss: 0.8719944489002228
LOSS train 0.87199 valid 0.99276, valid PER 31.41%
EPOCH 11:
  batch 50 loss: 0.7938980436325074
  batch 100 loss: 0.7727995419502258
  batch 150 loss: 0.7795738458633423
  batch 200 loss: 0.840175383090973
  batch 250 loss: 0.8231714969873428
  batch 300 loss: 0.8102039480209351
  batch 350 loss: 0.8119595396518707
  batch 400 loss: 0.8108364498615265
  batch 450 loss: 0.8256450939178467
  batch 500 loss: 0.8154416477680206
  batch 550 loss: 0.8138842296600342
  batch 600 loss: 0.8108727622032166
  batch 650 loss: 0.8643509554862976
  batch 700 loss: 0.8042340707778931
  batch 750 loss: 0.8135159862041473
  batch 800 loss: 0.8486295056343078
  batch 850 loss: 0.8593247544765472
  batch 900 loss: 0.843261057138443
LOSS train 0.84326 valid 0.96330, valid PER 29.55%
EPOCH 12:
  batch 50 loss: 0.7923171091079712
  batch 100 loss: 0.7820522558689117
  batch 150 loss: 0.7697513890266419
  batch 200 loss: 0.7805955958366394
  batch 250 loss: 0.8037489652633667
  batch 300 loss: 0.7924626326560974
  batch 350 loss: 0.7763581514358521
  batch 400 loss: 0.8334569638967514
  batch 450 loss: 0.8263534617424011
  batch 500 loss: 0.8250660145282745
  batch 550 loss: 0.7525109219551086
  batch 600 loss: 0.7841319119930268
  batch 650 loss: 0.8341309368610382
  batch 700 loss: 0.8158882641792298
  batch 750 loss: 0.8074098813533783
  batch 800 loss: 0.7941227716207504
  batch 850 loss: 0.8493261659145355
  batch 900 loss: 0.8345676326751709
LOSS train 0.83457 valid 0.94364, valid PER 29.16%
EPOCH 13:
  batch 50 loss: 0.7570602273941041
  batch 100 loss: 0.7781969392299652
  batch 150 loss: 0.7467518484592438
  batch 200 loss: 0.7807406949996948
  batch 250 loss: 0.760104831457138
  batch 300 loss: 0.7531424510478973
  batch 350 loss: 0.7601926857233048
  batch 400 loss: 0.7903494989871979
  batch 450 loss: 0.7821725594997406
  batch 500 loss: 0.7538733792304992
  batch 550 loss: 0.7928955161571503
  batch 600 loss: 0.7795397293567657
  batch 650 loss: 0.802214447259903
  batch 700 loss: 0.803890945315361
  batch 750 loss: 0.7644276750087738
  batch 800 loss: 0.7777248668670654
  batch 850 loss: 0.7985334748029709
  batch 900 loss: 0.8001159179210663
LOSS train 0.80012 valid 0.96649, valid PER 29.31%
EPOCH 14:
  batch 50 loss: 0.7336546647548675
  batch 100 loss: 0.7475759661197663
  batch 150 loss: 0.7471044450998306
  batch 200 loss: 0.7425626450777054
  batch 250 loss: 0.7540247827768326
  batch 300 loss: 0.7930223298072815
  batch 350 loss: 0.7449238216876983
  batch 400 loss: 0.7375750398635864
  batch 450 loss: 0.750109828710556
  batch 500 loss: 0.7616985547542572
  batch 550 loss: 0.8027628117799759
  batch 600 loss: 0.7512101608514786
  batch 650 loss: 0.7611475217342377
  batch 700 loss: 0.797301926612854
  batch 750 loss: 0.7579825794696808
  batch 800 loss: 0.7319470167160034
  batch 850 loss: 0.7849202454090118
  batch 900 loss: 0.7771665036678315
LOSS train 0.77717 valid 0.97353, valid PER 29.83%
EPOCH 15:
  batch 50 loss: 0.7309202092885971
  batch 100 loss: 0.7368846517801285
  batch 150 loss: 0.7027727514505386
  batch 200 loss: 0.7515999817848206
  batch 250 loss: 0.7542520141601563
  batch 300 loss: 0.740881080031395
  batch 350 loss: 0.7431048095226288
  batch 400 loss: 0.7350044000148773
  batch 450 loss: 0.7355887627601624
  batch 500 loss: 0.7187437409162521
  batch 550 loss: 0.7493204593658447
  batch 600 loss: 0.7692507672309875
  batch 650 loss: 0.7751376044750213
  batch 700 loss: 0.7852661240100861
  batch 750 loss: 0.772902757525444
  batch 800 loss: 0.7433867406845093
  batch 850 loss: 0.7226073956489563
  batch 900 loss: 0.7337039911746979
LOSS train 0.73370 valid 0.96166, valid PER 29.26%
EPOCH 16:
  batch 50 loss: 0.7384831702709198
  batch 100 loss: 0.6991961032152176
  batch 150 loss: 0.7151519650220871
  batch 200 loss: 0.7012177968025207
  batch 250 loss: 0.7312233507633209
  batch 300 loss: 0.7148858243227005
  batch 350 loss: 0.7349095302820206
  batch 400 loss: 0.7328471904993057
  batch 450 loss: 0.7433698087930679
  batch 500 loss: 0.7105798470973969
  batch 550 loss: 0.7245013964176178
  batch 600 loss: 0.7123197507858277
  batch 650 loss: 0.7232656073570252
  batch 700 loss: 0.7105563056468963
  batch 750 loss: 0.7304962587356567
  batch 800 loss: 0.7410221248865128
  batch 850 loss: 0.7162282264232636
  batch 900 loss: 0.7387616610527039
LOSS train 0.73876 valid 0.94252, valid PER 28.86%
EPOCH 17:
  batch 50 loss: 0.6882807278633117
  batch 100 loss: 0.6968718898296357
  batch 150 loss: 0.6803642177581787
  batch 200 loss: 0.6799431103467941
  batch 250 loss: 0.7021751189231873
  batch 300 loss: 0.7166782492399215
  batch 350 loss: 0.689012600183487
  batch 400 loss: 0.7577261197566986
  batch 450 loss: 0.7560162144899368
  batch 500 loss: 0.6973692697286605
  batch 550 loss: 0.7379541277885437
  batch 600 loss: 0.7565162098407745
  batch 650 loss: 0.7168852174282074
  batch 700 loss: 0.7008311116695404
  batch 750 loss: 0.6996649980545044
  batch 800 loss: 0.7391443216800689
  batch 850 loss: 0.7466934859752655
  batch 900 loss: 0.7224748879671097
LOSS train 0.72247 valid 0.93980, valid PER 28.06%
EPOCH 18:
  batch 50 loss: 0.6774365383386612
  batch 100 loss: 0.6848152774572372
  batch 150 loss: 0.7123404097557068
  batch 200 loss: 0.679544380903244
  batch 250 loss: 0.6854298913478851
  batch 300 loss: 0.6759954631328583
  batch 350 loss: 0.6946000176668167
  batch 400 loss: 0.6924057704210281
  batch 450 loss: 0.7042956954240799
  batch 500 loss: 0.7144061881303787
  batch 550 loss: 0.7077773451805115
  batch 600 loss: 0.6779659426212311
  batch 650 loss: 0.6811453765630722
  batch 700 loss: 0.7275531327724457
  batch 750 loss: 0.6939643353223801
  batch 800 loss: 0.6944447857141495
  batch 850 loss: 0.6921267020702362
  batch 900 loss: 0.7304253590106964
LOSS train 0.73043 valid 0.94924, valid PER 28.82%
EPOCH 19:
  batch 50 loss: 0.6304336225986481
  batch 100 loss: 0.6490406596660614
  batch 150 loss: 0.6477249574661255
  batch 200 loss: 0.6756322729587555
  batch 250 loss: 0.670478321313858
  batch 300 loss: 0.6743048417568207
  batch 350 loss: 0.6760864162445068
  batch 400 loss: 0.6861142545938492
  batch 450 loss: 0.7182801800966263
  batch 500 loss: 0.7146061849594116
  batch 550 loss: 0.6800763928890228
  batch 600 loss: 0.6746822011470794
  batch 650 loss: 0.7269339120388031
  batch 700 loss: 0.6638295447826386
  batch 750 loss: 0.6686742329597473
  batch 800 loss: 0.7253574287891388
  batch 850 loss: 0.711532473564148
  batch 900 loss: 0.6977645218372345
LOSS train 0.69776 valid 0.95393, valid PER 28.18%
EPOCH 20:
  batch 50 loss: 0.6462916767597199
  batch 100 loss: 0.6571339547634125
  batch 150 loss: 0.6367232131958008
  batch 200 loss: 0.668133824467659
  batch 250 loss: 0.6524460470676422
  batch 300 loss: 0.6515330535173416
  batch 350 loss: 0.6471697020530701
  batch 400 loss: 0.6544218528270721
  batch 450 loss: 0.6673598229885102
  batch 500 loss: 0.6397630870342255
  batch 550 loss: 0.7076840245723724
  batch 600 loss: 0.652406952381134
  batch 650 loss: 0.6911018121242524
  batch 700 loss: 0.663420376777649
  batch 750 loss: 0.6528940850496292
  batch 800 loss: 0.6926690208911895
  batch 850 loss: 0.6896646785736084
  batch 900 loss: 0.7029461336135864
LOSS train 0.70295 valid 0.95760, valid PER 28.12%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230117_192112/model_17
Loading model from checkpoints/20230117_192112/model_17
SUB: 16.88%, DEL: 10.32%, INS: 3.00%, COR: 72.81%, PER: 30.20%
