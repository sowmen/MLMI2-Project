Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.5)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 6.110213375091552
  batch 100 loss: 3.2719945526123047
  batch 150 loss: 3.1998745727539064
  batch 200 loss: 3.044102921485901
  batch 250 loss: 2.839325399398804
  batch 300 loss: 2.6471128940582274
  batch 350 loss: 2.5191472434997557
  batch 400 loss: 2.456362419128418
  batch 450 loss: 2.3782086896896364
  batch 500 loss: 2.268981251716614
  batch 550 loss: 2.203433530330658
  batch 600 loss: 2.169023108482361
  batch 650 loss: 2.087567100524902
  batch 700 loss: 2.0758355259895325
  batch 750 loss: 2.025920431613922
  batch 800 loss: 1.9920486426353454
  batch 850 loss: 1.9568952083587647
  batch 900 loss: 1.9342581796646119
LOSS train 1.93426 valid 1.90268, valid PER 70.24%
EPOCH 2:
  batch 50 loss: 1.8841280841827392
  batch 100 loss: 1.8283077359199524
  batch 150 loss: 1.7783770179748535
  batch 200 loss: 1.785475299358368
  batch 250 loss: 1.7907009649276733
  batch 300 loss: 1.7643795490264893
  batch 350 loss: 1.674557843208313
  batch 400 loss: 1.6869333434104918
  batch 450 loss: 1.65440575838089
  batch 500 loss: 1.6692113494873047
  batch 550 loss: 1.6654244875907898
  batch 600 loss: 1.6181659245491027
  batch 650 loss: 1.6466585874557496
  batch 700 loss: 1.6111185050010681
  batch 750 loss: 1.600027446746826
  batch 800 loss: 1.5442924213409424
  batch 850 loss: 1.5551704597473144
  batch 900 loss: 1.5820741629600525
LOSS train 1.58207 valid 1.53092, valid PER 56.63%
EPOCH 3:
  batch 50 loss: 1.530534462928772
  batch 100 loss: 1.4886953401565552
  batch 150 loss: 1.499168698787689
  batch 200 loss: 1.4737996292114257
  batch 250 loss: 1.4558176398277283
  batch 300 loss: 1.4518201971054077
  batch 350 loss: 1.4868262577056885
  batch 400 loss: 1.4710403060913086
  batch 450 loss: 1.4128743529319763
  batch 500 loss: 1.4126188516616822
  batch 550 loss: 1.4191076707839967
  batch 600 loss: 1.3833036780357362
  batch 650 loss: 1.353622407913208
  batch 700 loss: 1.3753658628463745
  batch 750 loss: 1.423048162460327
  batch 800 loss: 1.338412766456604
  batch 850 loss: 1.3684969902038575
  batch 900 loss: 1.3117873930931092
LOSS train 1.31179 valid 1.34757, valid PER 41.07%
EPOCH 4:
  batch 50 loss: 1.291991662979126
  batch 100 loss: 1.3146349358558655
  batch 150 loss: 1.2834464025497436
  batch 200 loss: 1.3167488527297975
  batch 250 loss: 1.301086106300354
  batch 300 loss: 1.3047757267951965
  batch 350 loss: 1.226337915658951
  batch 400 loss: 1.2768979668617249
  batch 450 loss: 1.2478341579437255
  batch 500 loss: 1.228488985300064
  batch 550 loss: 1.2484793186187744
  batch 600 loss: 1.2718439924716949
  batch 650 loss: 1.2542386913299561
  batch 700 loss: 1.2153821980953217
  batch 750 loss: 1.1998066627979278
  batch 800 loss: 1.1768079257011415
  batch 850 loss: 1.2120499908924103
  batch 900 loss: 1.2533639669418335
LOSS train 1.25336 valid 1.22494, valid PER 37.40%
EPOCH 5:
  batch 50 loss: 1.1780543792247773
  batch 100 loss: 1.1730258870124817
  batch 150 loss: 1.2113291323184967
  batch 200 loss: 1.153418927192688
  batch 250 loss: 1.1546896994113922
  batch 300 loss: 1.1754772984981536
  batch 350 loss: 1.1581491756439208
  batch 400 loss: 1.1669715285301208
  batch 450 loss: 1.1497929787635803
  batch 500 loss: 1.1659396255016328
  batch 550 loss: 1.1121798825263978
  batch 600 loss: 1.1902416801452638
  batch 650 loss: 1.1477940785884857
  batch 700 loss: 1.176093077659607
  batch 750 loss: 1.1190004408359528
  batch 800 loss: 1.1473666775226592
  batch 850 loss: 1.1279472696781159
  batch 900 loss: 1.146687115430832
LOSS train 1.14669 valid 1.16315, valid PER 36.38%
EPOCH 6:
  batch 50 loss: 1.1560329139232635
  batch 100 loss: 1.0978516805171967
  batch 150 loss: 1.0810087537765503
  batch 200 loss: 1.0938382613658906
  batch 250 loss: 1.127570379972458
  batch 300 loss: 1.1122772789001465
  batch 350 loss: 1.10121808886528
  batch 400 loss: 1.0804597878456115
  batch 450 loss: 1.1041491389274598
  batch 500 loss: 1.0906743919849395
  batch 550 loss: 1.1015340411663055
  batch 600 loss: 1.096447504758835
  batch 650 loss: 1.0875524222850799
  batch 700 loss: 1.0938914716243744
  batch 750 loss: 1.0696510756015778
  batch 800 loss: 1.072690280675888
  batch 850 loss: 1.0576483643054961
  batch 900 loss: 1.0809725737571716
LOSS train 1.08097 valid 1.12706, valid PER 34.90%
EPOCH 7:
  batch 50 loss: 1.0653473448753357
  batch 100 loss: 1.078299777507782
  batch 150 loss: 1.0483140170574188
  batch 200 loss: 1.0375974035263063
  batch 250 loss: 1.0436327064037323
  batch 300 loss: 1.0238699698448182
  batch 350 loss: 1.041425039768219
  batch 400 loss: 1.0457899498939514
  batch 450 loss: 1.0423199868202209
  batch 500 loss: 1.0286402189731598
  batch 550 loss: 1.0309023690223693
  batch 600 loss: 1.0563747847080232
  batch 650 loss: 1.0259600758552552
  batch 700 loss: 1.0394206941127777
  batch 750 loss: 1.0156678545475006
  batch 800 loss: 1.0274019598960877
  batch 850 loss: 1.056656278371811
  batch 900 loss: 1.0767135226726532
LOSS train 1.07671 valid 1.07739, valid PER 33.97%
EPOCH 8:
  batch 50 loss: 1.012281926870346
  batch 100 loss: 0.9959687530994416
  batch 150 loss: 0.9973969221115112
  batch 200 loss: 0.9735620450973511
  batch 250 loss: 1.0128911185264586
  batch 300 loss: 0.9471702933311462
  batch 350 loss: 1.0178472566604615
  batch 400 loss: 0.9968975818157196
  batch 450 loss: 1.0096006965637208
  batch 500 loss: 1.0478044593334197
  batch 550 loss: 0.9771797931194306
  batch 600 loss: 1.0038056707382201
  batch 650 loss: 1.0235943818092346
  batch 700 loss: 0.9824511504173279
  batch 750 loss: 0.9809529948234558
  batch 800 loss: 1.0013943791389466
  batch 850 loss: 1.0100690722465515
  batch 900 loss: 1.0096058452129364
LOSS train 1.00961 valid 1.05590, valid PER 32.21%
EPOCH 9:
  batch 50 loss: 0.9285228228569031
  batch 100 loss: 0.9758699214458466
  batch 150 loss: 0.9596661126613617
  batch 200 loss: 0.9427717256546021
  batch 250 loss: 0.9709298288822175
  batch 300 loss: 0.9762219727039337
  batch 350 loss: 0.9954959571361541
  batch 400 loss: 0.9786904203891754
  batch 450 loss: 0.9726760828495026
  batch 500 loss: 0.9410124909877777
  batch 550 loss: 0.9830566561222076
  batch 600 loss: 0.9654721748828888
  batch 650 loss: 0.9457365453243256
  batch 700 loss: 0.9448227727413178
  batch 750 loss: 0.9602911531925201
  batch 800 loss: 0.9851093399524689
  batch 850 loss: 0.9809907448291778
  batch 900 loss: 0.9394368016719818
LOSS train 0.93944 valid 1.02569, valid PER 31.79%
EPOCH 10:
  batch 50 loss: 0.8988756108283996
  batch 100 loss: 0.926093887090683
  batch 150 loss: 0.9467293202877045
  batch 200 loss: 0.9465749382972717
  batch 250 loss: 0.956697860956192
  batch 300 loss: 0.9104321384429932
  batch 350 loss: 0.9427862107753754
  batch 400 loss: 0.9160770761966706
  batch 450 loss: 0.9008526873588562
  batch 500 loss: 0.9506033062934875
  batch 550 loss: 0.9550981616973877
  batch 600 loss: 0.9376865649223327
  batch 650 loss: 0.9166831588745117
  batch 700 loss: 0.9339719820022583
  batch 750 loss: 0.9193456780910492
  batch 800 loss: 0.9431649029254914
  batch 850 loss: 0.9330261707305908
  batch 900 loss: 0.9562048137187957
LOSS train 0.95620 valid 1.00655, valid PER 31.70%
EPOCH 11:
  batch 50 loss: 0.8805277371406555
  batch 100 loss: 0.8603418648242951
  batch 150 loss: 0.8802503418922424
  batch 200 loss: 0.9255225825309753
  batch 250 loss: 0.9153158354759217
  batch 300 loss: 0.8811364793777465
  batch 350 loss: 0.9025232696533203
  batch 400 loss: 0.9110358810424805
  batch 450 loss: 0.9140993106365204
  batch 500 loss: 0.8936517214775086
  batch 550 loss: 0.9073009324073792
  batch 600 loss: 0.906243726015091
  batch 650 loss: 0.9523423492908478
  batch 700 loss: 0.8616471254825592
  batch 750 loss: 0.8920676434040069
  batch 800 loss: 0.92477743268013
  batch 850 loss: 0.9376715874671936
  batch 900 loss: 0.9323846268653869
LOSS train 0.93238 valid 1.00799, valid PER 31.10%
EPOCH 12:
  batch 50 loss: 0.891857932806015
  batch 100 loss: 0.8740409040451049
  batch 150 loss: 0.853916083574295
  batch 200 loss: 0.8615227282047272
  batch 250 loss: 0.8821862185001373
  batch 300 loss: 0.8778131532669068
  batch 350 loss: 0.868426856994629
  batch 400 loss: 0.9016971385478973
  batch 450 loss: 0.8978927445411682
  batch 500 loss: 0.9024209237098694
  batch 550 loss: 0.8260356462001801
  batch 600 loss: 0.8622404932975769
  batch 650 loss: 0.9032873547077179
  batch 700 loss: 0.8960009396076203
  batch 750 loss: 0.8788678693771362
  batch 800 loss: 0.8673834788799286
  batch 850 loss: 0.9156540679931641
  batch 900 loss: 0.904048547744751
LOSS train 0.90405 valid 0.98310, valid PER 30.17%
EPOCH 13:
  batch 50 loss: 0.8385281193256379
  batch 100 loss: 0.874644216299057
  batch 150 loss: 0.8298346781730652
  batch 200 loss: 0.8615790247917176
  batch 250 loss: 0.8548013532161712
  batch 300 loss: 0.8366433376073837
  batch 350 loss: 0.8676793503761292
  batch 400 loss: 0.871384162902832
  batch 450 loss: 0.8702244007587433
  batch 500 loss: 0.8371538448333741
  batch 550 loss: 0.8742841601371765
  batch 600 loss: 0.8490178513526917
  batch 650 loss: 0.8686258268356323
  batch 700 loss: 0.8728249490261077
  batch 750 loss: 0.8280760431289673
  batch 800 loss: 0.8479729616641998
  batch 850 loss: 0.8837360501289367
  batch 900 loss: 0.8646941494941711
LOSS train 0.86469 valid 0.97900, valid PER 30.44%
EPOCH 14:
  batch 50 loss: 0.8368067932128906
  batch 100 loss: 0.8295503401756287
  batch 150 loss: 0.8385762643814086
  batch 200 loss: 0.8242931270599365
  batch 250 loss: 0.8264751470088959
  batch 300 loss: 0.8634729039669037
  batch 350 loss: 0.8158955347537994
  batch 400 loss: 0.8163810932636261
  batch 450 loss: 0.8201178443431855
  batch 500 loss: 0.8312143695354461
  batch 550 loss: 0.8594271755218506
  batch 600 loss: 0.8146128231287002
  batch 650 loss: 0.8410342621803284
  batch 700 loss: 0.8754857528209686
  batch 750 loss: 0.8190298128128052
  batch 800 loss: 0.7956983947753906
  batch 850 loss: 0.8539264237880707
  batch 900 loss: 0.8473743653297424
LOSS train 0.84737 valid 0.96827, valid PER 30.05%
EPOCH 15:
  batch 50 loss: 0.8261912322044372
  batch 100 loss: 0.7994137758016586
  batch 150 loss: 0.8203794121742248
  batch 200 loss: 0.8285430252552033
  batch 250 loss: 0.8348277914524078
  batch 300 loss: 0.8135012930631638
  batch 350 loss: 0.8046280443668365
  batch 400 loss: 0.8026449632644653
  batch 450 loss: 0.8073726391792297
  batch 500 loss: 0.7964308893680573
  batch 550 loss: 0.8212670636177063
  batch 600 loss: 0.8381347990036011
  batch 650 loss: 0.838571617603302
  batch 700 loss: 0.8274424707889557
  batch 750 loss: 0.8324606335163116
  batch 800 loss: 0.8102399003505707
  batch 850 loss: 0.8004861557483673
  batch 900 loss: 0.8139264810085297
LOSS train 0.81393 valid 0.97091, valid PER 30.14%
EPOCH 16:
  batch 50 loss: 0.8227336573600769
  batch 100 loss: 0.764383515715599
  batch 150 loss: 0.7718486708402633
  batch 200 loss: 0.7931712889671325
  batch 250 loss: 0.8141504430770874
  batch 300 loss: 0.7950745391845703
  batch 350 loss: 0.8097378718852997
  batch 400 loss: 0.8019683170318603
  batch 450 loss: 0.8185789835453033
  batch 500 loss: 0.7658389616012573
  batch 550 loss: 0.8029325127601623
  batch 600 loss: 0.7962046313285828
  batch 650 loss: 0.8178118878602981
  batch 700 loss: 0.7799885821342468
  batch 750 loss: 0.8242662143707276
  batch 800 loss: 0.8043548047542572
  batch 850 loss: 0.7872611552476882
  batch 900 loss: 0.7932777690887451
LOSS train 0.79328 valid 0.94981, valid PER 28.80%
EPOCH 17:
  batch 50 loss: 0.7752668750286102
  batch 100 loss: 0.7859670686721801
  batch 150 loss: 0.7706421947479248
  batch 200 loss: 0.7614910852909088
  batch 250 loss: 0.7835295987129212
  batch 300 loss: 0.789290941953659
  batch 350 loss: 0.7627521347999573
  batch 400 loss: 0.8049150264263153
  batch 450 loss: 0.7995717096328735
  batch 500 loss: 0.7735425448417663
  batch 550 loss: 0.7855272674560547
  batch 600 loss: 0.8163318061828613
  batch 650 loss: 0.7744168877601624
  batch 700 loss: 0.7758134829998017
  batch 750 loss: 0.7526947748661041
  batch 800 loss: 0.7638030278682709
  batch 850 loss: 0.7842355102300644
  batch 900 loss: 0.7595323717594147
LOSS train 0.75953 valid 0.94179, valid PER 28.62%
EPOCH 18:
  batch 50 loss: 0.7575629687309265
  batch 100 loss: 0.780912184715271
  batch 150 loss: 0.7879472947120667
  batch 200 loss: 0.7617044854164123
  batch 250 loss: 0.7550984239578247
  batch 300 loss: 0.7495649164915085
  batch 350 loss: 0.7761638993024826
  batch 400 loss: 0.7274284660816193
  batch 450 loss: 0.793137217760086
  batch 500 loss: 0.7665309691429139
  batch 550 loss: 0.7808404648303986
  batch 600 loss: 0.7399610996246337
  batch 650 loss: 0.739797146320343
  batch 700 loss: 0.7734352040290833
  batch 750 loss: 0.7514059543609619
  batch 800 loss: 0.7485448616743088
  batch 850 loss: 0.7601047050952912
  batch 900 loss: 0.7902494692802429
LOSS train 0.79025 valid 0.95290, valid PER 29.50%
EPOCH 19:
  batch 50 loss: 0.7118218600749969
  batch 100 loss: 0.712715522646904
  batch 150 loss: 0.7315705972909927
  batch 200 loss: 0.7580831438302994
  batch 250 loss: 0.7495144659280777
  batch 300 loss: 0.7503276777267456
  batch 350 loss: 0.7344022428989411
  batch 400 loss: 0.7471880221366882
  batch 450 loss: 0.7586415052413941
  batch 500 loss: 0.752470097541809
  batch 550 loss: 0.7310495901107789
  batch 600 loss: 0.7456129097938538
  batch 650 loss: 0.790197280049324
  batch 700 loss: 0.7269337683916092
  batch 750 loss: 0.7231093150377274
  batch 800 loss: 0.7634921360015869
  batch 850 loss: 0.7650066649913788
  batch 900 loss: 0.7617394971847534
LOSS train 0.76174 valid 0.94422, valid PER 28.72%
EPOCH 20:
  batch 50 loss: 0.727129448056221
  batch 100 loss: 0.7146907991170883
  batch 150 loss: 0.7156860035657883
  batch 200 loss: 0.7494043922424316
  batch 250 loss: 0.7217060899734498
  batch 300 loss: 0.7561029827594757
  batch 350 loss: 0.7188782638311386
  batch 400 loss: 0.72006487429142
  batch 450 loss: 0.7307730430364608
  batch 500 loss: 0.701924951672554
  batch 550 loss: 0.7693218243122101
  batch 600 loss: 0.7112083148956299
  batch 650 loss: 0.7378925013542176
  batch 700 loss: 0.7427719247341156
  batch 750 loss: 0.7255167472362518
  batch 800 loss: 0.7613331305980683
  batch 850 loss: 0.7662675344944
  batch 900 loss: 0.7661235797405243
LOSS train 0.76612 valid 0.93628, valid PER 28.44%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_192615/model_20
Loading model from checkpoints/20230117_192615/model_20
SUB: 17.09%, DEL: 10.37%, INS: 2.85%, COR: 72.54%, PER: 30.31%
