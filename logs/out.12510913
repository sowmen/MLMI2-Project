Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=0.5)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 6.0870250749588015
  batch 100 loss: 3.2657937812805176
  batch 150 loss: 3.192840313911438
  batch 200 loss: 3.0304185581207275
  batch 250 loss: 2.813919801712036
  batch 300 loss: 2.6118216848373415
  batch 350 loss: 2.4834017658233645
  batch 400 loss: 2.4328352546691896
  batch 450 loss: 2.356369638442993
  batch 500 loss: 2.2534717869758607
  batch 550 loss: 2.1920794248580933
  batch 600 loss: 2.159743187427521
  batch 650 loss: 2.0782594418525697
  batch 700 loss: 2.074317066669464
  batch 750 loss: 2.016185419559479
  batch 800 loss: 1.9803840565681456
  batch 850 loss: 1.949890501499176
  batch 900 loss: 1.9315297746658324
LOSS train 1.93153 valid 1.89706, valid PER 69.70%
EPOCH 2:
  batch 50 loss: 1.878754322528839
  batch 100 loss: 1.8218237996101379
  batch 150 loss: 1.7717728519439697
  batch 200 loss: 1.7862144303321839
  batch 250 loss: 1.7900950050354003
  batch 300 loss: 1.7592855787277222
  batch 350 loss: 1.6756955361366273
  batch 400 loss: 1.6871561884880066
  batch 450 loss: 1.6571564173698425
  batch 500 loss: 1.666248881816864
  batch 550 loss: 1.665391743183136
  batch 600 loss: 1.618250346183777
  batch 650 loss: 1.6440530562400817
  batch 700 loss: 1.61315110206604
  batch 750 loss: 1.597959508895874
  batch 800 loss: 1.5437796664237977
  batch 850 loss: 1.5521443462371827
  batch 900 loss: 1.5717569589614868
LOSS train 1.57176 valid 1.53416, valid PER 55.39%
EPOCH 3:
  batch 50 loss: 1.5251914000511169
  batch 100 loss: 1.4923132371902466
  batch 150 loss: 1.4961624193191527
  batch 200 loss: 1.4780591893196107
  batch 250 loss: 1.4488960194587708
  batch 300 loss: 1.453916609287262
  batch 350 loss: 1.4888407945632935
  batch 400 loss: 1.4687197661399842
  batch 450 loss: 1.4299311566352844
  batch 500 loss: 1.4197084546089171
  batch 550 loss: 1.4149829745292664
  batch 600 loss: 1.3967668771743775
  batch 650 loss: 1.3598055601119996
  batch 700 loss: 1.3981233954429626
  batch 750 loss: 1.438990077972412
  batch 800 loss: 1.3580894327163697
  batch 850 loss: 1.4010313391685485
  batch 900 loss: 1.3206398868560791
LOSS train 1.32064 valid 1.37034, valid PER 43.26%
EPOCH 4:
  batch 50 loss: 1.3263500571250915
  batch 100 loss: 1.3333535027503967
  batch 150 loss: 1.3008491277694703
  batch 200 loss: 1.3331555891036988
  batch 250 loss: 1.3277295279502868
  batch 300 loss: 1.323708176612854
  batch 350 loss: 1.2490896368026734
  batch 400 loss: 1.2876183331012725
  batch 450 loss: 1.270954623222351
  batch 500 loss: 1.2504654848575592
  batch 550 loss: 1.2805381584167481
  batch 600 loss: 1.2901936602592468
  batch 650 loss: 1.2658959496021271
  batch 700 loss: 1.2475180888175965
  batch 750 loss: 1.2139001178741455
  batch 800 loss: 1.1826003193855286
  batch 850 loss: 1.2255236506462097
  batch 900 loss: 1.2467677187919617
LOSS train 1.24677 valid 1.23852, valid PER 38.60%
EPOCH 5:
  batch 50 loss: 1.1980581188201904
  batch 100 loss: 1.1758686423301696
  batch 150 loss: 1.2217798626422882
  batch 200 loss: 1.1481905221939086
  batch 250 loss: 1.1698465478420257
  batch 300 loss: 1.1771221232414246
  batch 350 loss: 1.1836027717590332
  batch 400 loss: 1.1773619723320008
  batch 450 loss: 1.1710170602798462
  batch 500 loss: 1.1781071615219116
  batch 550 loss: 1.1248203480243684
  batch 600 loss: 1.205454831123352
  batch 650 loss: 1.1558983552455901
  batch 700 loss: 1.2040703463554383
  batch 750 loss: 1.126724967956543
  batch 800 loss: 1.1612896704673767
  batch 850 loss: 1.1490136218070983
  batch 900 loss: 1.1682161223888397
LOSS train 1.16822 valid 1.18270, valid PER 36.46%
EPOCH 6:
  batch 50 loss: 1.1599258685112
  batch 100 loss: 1.113678343296051
  batch 150 loss: 1.0970651173591615
  batch 200 loss: 1.0945541369915008
  batch 250 loss: 1.1334483301639557
  batch 300 loss: 1.1205806505680085
  batch 350 loss: 1.110980134010315
  batch 400 loss: 1.0986932015419006
  batch 450 loss: 1.1267766404151915
  batch 500 loss: 1.0933424425125122
  batch 550 loss: 1.122052744626999
  batch 600 loss: 1.0789567852020263
  batch 650 loss: 1.0967347037792206
  batch 700 loss: 1.109216558933258
  batch 750 loss: 1.0816597390174865
  batch 800 loss: 1.0787705302238464
  batch 850 loss: 1.0648701691627502
  batch 900 loss: 1.0922622382640839
LOSS train 1.09226 valid 1.13444, valid PER 35.45%
EPOCH 7:
  batch 50 loss: 1.081935044527054
  batch 100 loss: 1.0828351545333863
  batch 150 loss: 1.0462378203868865
  batch 200 loss: 1.0422193169593812
  batch 250 loss: 1.050111838579178
  batch 300 loss: 1.0306632566452025
  batch 350 loss: 1.044136791229248
  batch 400 loss: 1.0581566405296325
  batch 450 loss: 1.048580869436264
  batch 500 loss: 1.0418287587165833
  batch 550 loss: 1.0448056399822234
  batch 600 loss: 1.0641239511966705
  batch 650 loss: 1.031387140750885
  batch 700 loss: 1.0667345917224884
  batch 750 loss: 1.012695562839508
  batch 800 loss: 1.0296571695804595
  batch 850 loss: 1.0725205612182618
  batch 900 loss: 1.0896320688724517
LOSS train 1.08963 valid 1.08473, valid PER 33.92%
EPOCH 8:
  batch 50 loss: 1.0225898444652557
  batch 100 loss: 1.001390552520752
  batch 150 loss: 1.0068987214565277
  batch 200 loss: 0.9772029197216034
  batch 250 loss: 1.0222937786579132
  batch 300 loss: 0.9556223464012146
  batch 350 loss: 1.0299579310417175
  batch 400 loss: 0.9974774074554443
  batch 450 loss: 1.0138161396980285
  batch 500 loss: 1.0521679055690765
  batch 550 loss: 0.9806193697452545
  batch 600 loss: 1.025862443447113
  batch 650 loss: 1.047905868291855
  batch 700 loss: 0.9875621378421784
  batch 750 loss: 0.9941322600841522
  batch 800 loss: 1.017867500782013
  batch 850 loss: 1.0264618647098542
  batch 900 loss: 1.0227186095714569
LOSS train 1.02272 valid 1.05845, valid PER 32.54%
EPOCH 9:
  batch 50 loss: 0.9361326467990875
  batch 100 loss: 0.998196839094162
  batch 150 loss: 0.9708182740211487
  batch 200 loss: 0.9461539268493653
  batch 250 loss: 0.96762775182724
  batch 300 loss: 1.0074306917190552
  batch 350 loss: 1.0145400702953338
  batch 400 loss: 0.9929185128211975
  batch 450 loss: 0.9775642824172973
  batch 500 loss: 0.9437225270271301
  batch 550 loss: 0.9860992527008057
  batch 600 loss: 0.9765729224681854
  batch 650 loss: 0.9537796008586884
  batch 700 loss: 0.9397529697418213
  batch 750 loss: 0.9737811660766602
  batch 800 loss: 0.9874035060405731
  batch 850 loss: 0.9952204346656799
  batch 900 loss: 0.9313628435134887
LOSS train 0.93136 valid 1.03231, valid PER 31.98%
EPOCH 10:
  batch 50 loss: 0.9098527181148529
  batch 100 loss: 0.9392143046855926
  batch 150 loss: 0.9570621263980865
  batch 200 loss: 0.9553100514411926
  batch 250 loss: 0.9603838610649109
  batch 300 loss: 0.9200356793403626
  batch 350 loss: 0.9578480243682861
  batch 400 loss: 0.9123950171470642
  batch 450 loss: 0.9176990902423858
  batch 500 loss: 0.9731294095516205
  batch 550 loss: 0.974916752576828
  batch 600 loss: 0.9338497519493103
  batch 650 loss: 0.9316294622421265
  batch 700 loss: 0.9543003427982331
  batch 750 loss: 0.9302788138389587
  batch 800 loss: 0.9418091905117035
  batch 850 loss: 0.9512697136402131
  batch 900 loss: 0.9558121883869171
LOSS train 0.95581 valid 1.03087, valid PER 33.06%
EPOCH 11:
  batch 50 loss: 0.9014032447338104
  batch 100 loss: 0.8819204175472259
  batch 150 loss: 0.8931628513336182
  batch 200 loss: 0.9552165520191193
  batch 250 loss: 0.9238036739826202
  batch 300 loss: 0.8935758566856384
  batch 350 loss: 0.9210856831073762
  batch 400 loss: 0.9246659016609192
  batch 450 loss: 0.9390392625331878
  batch 500 loss: 0.8997907853126526
  batch 550 loss: 0.9162486875057221
  batch 600 loss: 0.8949357032775879
  batch 650 loss: 0.9520907258987427
  batch 700 loss: 0.8723302209377288
  batch 750 loss: 0.8897018647193908
  batch 800 loss: 0.9426677584648132
  batch 850 loss: 0.9436171245574951
  batch 900 loss: 0.9414648485183715
LOSS train 0.94146 valid 1.01063, valid PER 31.16%
EPOCH 12:
  batch 50 loss: 0.900197913646698
  batch 100 loss: 0.8789138996601105
  batch 150 loss: 0.8609095358848572
  batch 200 loss: 0.873620731830597
  batch 250 loss: 0.9016806507110595
  batch 300 loss: 0.8905853533744812
  batch 350 loss: 0.8917054629325867
  batch 400 loss: 0.9109682738780975
  batch 450 loss: 0.9039024937152863
  batch 500 loss: 0.9123345601558686
  batch 550 loss: 0.8364258742332459
  batch 600 loss: 0.8705645871162414
  batch 650 loss: 0.9162228369712829
  batch 700 loss: 0.9077267467975616
  batch 750 loss: 0.8803320121765137
  batch 800 loss: 0.8628598141670227
  batch 850 loss: 0.9209678900241852
  batch 900 loss: 0.9189388406276703
LOSS train 0.91894 valid 0.98565, valid PER 30.59%
EPOCH 13:
  batch 50 loss: 0.8547045016288757
  batch 100 loss: 0.8763117897510528
  batch 150 loss: 0.8363500487804413
  batch 200 loss: 0.8560126590728759
  batch 250 loss: 0.8606111419200897
  batch 300 loss: 0.8506730085611344
  batch 350 loss: 0.8608596587181091
  batch 400 loss: 0.8817812502384186
  batch 450 loss: 0.8830596804618835
  batch 500 loss: 0.8449936902523041
  batch 550 loss: 0.8864100205898285
  batch 600 loss: 0.8655710005760193
  batch 650 loss: 0.8936911183595657
  batch 700 loss: 0.8798000299930573
  batch 750 loss: 0.8326257038116455
  batch 800 loss: 0.8642009222507476
  batch 850 loss: 0.8866463708877563
  batch 900 loss: 0.87761838555336
LOSS train 0.87762 valid 0.96874, valid PER 30.04%
EPOCH 14:
  batch 50 loss: 0.8358980882167816
  batch 100 loss: 0.8457819759845734
  batch 150 loss: 0.848891988992691
  batch 200 loss: 0.824833619594574
  batch 250 loss: 0.8384675192832947
  batch 300 loss: 0.8716354143619537
  batch 350 loss: 0.8256063663959503
  batch 400 loss: 0.8377097427845002
  batch 450 loss: 0.833236038684845
  batch 500 loss: 0.84364501953125
  batch 550 loss: 0.8752131593227387
  batch 600 loss: 0.814771733880043
  batch 650 loss: 0.8622297024726868
  batch 700 loss: 0.8770447170734406
  batch 750 loss: 0.8363372445106506
  batch 800 loss: 0.8058141636848449
  batch 850 loss: 0.8588586628437043
  batch 900 loss: 0.8436860692501068
LOSS train 0.84369 valid 0.99386, valid PER 31.18%
EPOCH 15:
  batch 50 loss: 0.8317809951305389
  batch 100 loss: 0.8269163525104523
  batch 150 loss: 0.8240213322639466
  batch 200 loss: 0.8597681510448456
  batch 250 loss: 0.8393758225440979
  batch 300 loss: 0.8134114396572113
  batch 350 loss: 0.8097443950176239
  batch 400 loss: 0.8252875077724456
  batch 450 loss: 0.8217125606536865
  batch 500 loss: 0.7880903923511505
  batch 550 loss: 0.8064152145385742
  batch 600 loss: 0.8390824949741363
  batch 650 loss: 0.8515910315513611
  batch 700 loss: 0.8494856584072114
  batch 750 loss: 0.8484470558166504
  batch 800 loss: 0.813853006362915
  batch 850 loss: 0.7990048229694366
  batch 900 loss: 0.8146944046020508
LOSS train 0.81469 valid 0.98104, valid PER 30.07%
EPOCH 16:
  batch 50 loss: 0.8306433594226837
  batch 100 loss: 0.7645256876945495
  batch 150 loss: 0.7856174349784851
  batch 200 loss: 0.810211216211319
  batch 250 loss: 0.8309854626655578
  batch 300 loss: 0.8056859219074249
  batch 350 loss: 0.8074236977100372
  batch 400 loss: 0.8136220598220825
  batch 450 loss: 0.825691876411438
  batch 500 loss: 0.7806759345531463
  batch 550 loss: 0.8227787923812866
  batch 600 loss: 0.8043347477912903
  batch 650 loss: 0.8297133028507233
  batch 700 loss: 0.7835661673545837
  batch 750 loss: 0.8152555191516876
  batch 800 loss: 0.8274344074726104
  batch 850 loss: 0.8089902901649475
  batch 900 loss: 0.7951881623268128
LOSS train 0.79519 valid 0.96182, valid PER 29.03%
EPOCH 17:
  batch 50 loss: 0.7895059275627136
  batch 100 loss: 0.7854806977510452
  batch 150 loss: 0.7822335755825043
  batch 200 loss: 0.7576315188407898
  batch 250 loss: 0.7951290929317474
  batch 300 loss: 0.8128274691104889
  batch 350 loss: 0.7677304232120514
  batch 400 loss: 0.8123524034023285
  batch 450 loss: 0.8035643517971038
  batch 500 loss: 0.783722882270813
  batch 550 loss: 0.7813649487495422
  batch 600 loss: 0.8275641167163849
  batch 650 loss: 0.7826738226413726
  batch 700 loss: 0.7763971054553985
  batch 750 loss: 0.7619254571199418
  batch 800 loss: 0.7779667538404464
  batch 850 loss: 0.8030384051799774
  batch 900 loss: 0.7893395835161209
LOSS train 0.78934 valid 0.95049, valid PER 29.02%
EPOCH 18:
  batch 50 loss: 0.7622377663850785
  batch 100 loss: 0.7699203634262085
  batch 150 loss: 0.7942746841907501
  batch 200 loss: 0.7660141849517822
  batch 250 loss: 0.7725202643871307
  batch 300 loss: 0.7599352431297303
  batch 350 loss: 0.7632214403152466
  batch 400 loss: 0.7596935945749282
  batch 450 loss: 0.797005912065506
  batch 500 loss: 0.7812597668170929
  batch 550 loss: 0.7840286386013031
  batch 600 loss: 0.7584592747688294
  batch 650 loss: 0.7466683727502823
  batch 700 loss: 0.7766718864440918
  batch 750 loss: 0.7614611786603928
  batch 800 loss: 0.7771472728252411
  batch 850 loss: 0.7513876926898956
  batch 900 loss: 0.8013622009754181
LOSS train 0.80136 valid 0.95087, valid PER 29.07%
EPOCH 19:
  batch 50 loss: 0.717812305688858
  batch 100 loss: 0.7147537589073181
  batch 150 loss: 0.7440534305572509
  batch 200 loss: 0.7510285931825638
  batch 250 loss: 0.7570734977722168
  batch 300 loss: 0.745993424654007
  batch 350 loss: 0.751328399181366
  batch 400 loss: 0.7528020215034484
  batch 450 loss: 0.7708244997262955
  batch 500 loss: 0.7585407876968384
  batch 550 loss: 0.7437125313282013
  batch 600 loss: 0.7548582708835602
  batch 650 loss: 0.8223967289924622
  batch 700 loss: 0.7485281854867936
  batch 750 loss: 0.7239027428627014
  batch 800 loss: 0.7717333459854125
  batch 850 loss: 0.7733150851726532
  batch 900 loss: 0.766594603061676
LOSS train 0.76659 valid 0.95474, valid PER 29.17%
EPOCH 20:
  batch 50 loss: 0.727372875213623
  batch 100 loss: 0.7210614717006684
  batch 150 loss: 0.7121925318241119
  batch 200 loss: 0.7446056938171387
  batch 250 loss: 0.7259777796268463
  batch 300 loss: 0.7494119870662689
  batch 350 loss: 0.7248667496442794
  batch 400 loss: 0.7355081057548523
  batch 450 loss: 0.7273047626018524
  batch 500 loss: 0.7125828921794891
  batch 550 loss: 0.7835773968696594
  batch 600 loss: 0.7229546737670899
  batch 650 loss: 0.7586343777179718
  batch 700 loss: 0.7570096719264984
  batch 750 loss: 0.7280536127090455
  batch 800 loss: 0.7783011031150818
  batch 850 loss: 0.762543386220932
  batch 900 loss: 0.762143275141716
LOSS train 0.76214 valid 0.95061, valid PER 28.72%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_193128/model_17
Loading model from checkpoints/20230117_193128/model_17
SUB: 16.70%, DEL: 11.48%, INS: 2.41%, COR: 71.82%, PER: 30.59%
