Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.654552292823792
  batch 100 loss: 3.275118498802185
  batch 150 loss: 3.2277695178985595
  batch 200 loss: 3.193031282424927
  batch 250 loss: 3.1407716035842896
  batch 300 loss: 3.0641412496566773
  batch 350 loss: 2.9942203044891356
  batch 400 loss: 2.943965349197388
  batch 450 loss: 2.8855372619628907
  batch 500 loss: 2.7956251764297484
  batch 550 loss: 2.739081172943115
  batch 600 loss: 2.6934809160232542
  batch 650 loss: 2.6275054025650024
  batch 700 loss: 2.6090343523025514
  batch 750 loss: 2.5589335536956788
  batch 800 loss: 2.5334606981277465
  batch 850 loss: 2.5064989042282106
  batch 900 loss: 2.4603355312347412
LOSS train 2.46034 valid 2.43274, valid PER 80.97%
EPOCH 2:
  batch 50 loss: 2.4306021165847778
  batch 100 loss: 2.380953702926636
  batch 150 loss: 2.3242642307281494
  batch 200 loss: 2.3081329822540284
  batch 250 loss: 2.303949222564697
  batch 300 loss: 2.278643479347229
  batch 350 loss: 2.2055635690689086
  batch 400 loss: 2.2135469198226927
  batch 450 loss: 2.1752903032302857
  batch 500 loss: 2.159650242328644
  batch 550 loss: 2.162758891582489
  batch 600 loss: 2.10901504278183
  batch 650 loss: 2.124723358154297
  batch 700 loss: 2.091093456745148
  batch 750 loss: 2.0866453719139098
  batch 800 loss: 2.027711567878723
  batch 850 loss: 2.0136933374404906
  batch 900 loss: 2.0198548865318298
LOSS train 2.01985 valid 1.99423, valid PER 75.34%
EPOCH 3:
  batch 50 loss: 1.9966760230064393
  batch 100 loss: 1.9395565176010132
  batch 150 loss: 1.9507115817070007
  batch 200 loss: 1.9313198828697205
  batch 250 loss: 1.9003169894218446
  batch 300 loss: 1.8876821851730348
  batch 350 loss: 1.9166775417327881
  batch 400 loss: 1.880744128227234
  batch 450 loss: 1.823509316444397
  batch 500 loss: 1.8223735547065736
  batch 550 loss: 1.8070654058456421
  batch 600 loss: 1.78387309551239
  batch 650 loss: 1.754413251876831
  batch 700 loss: 1.7857362914085388
  batch 750 loss: 1.802115068435669
  batch 800 loss: 1.7312890005111694
  batch 850 loss: 1.7471589040756226
  batch 900 loss: 1.6841286444664
LOSS train 1.68413 valid 1.73585, valid PER 63.76%
EPOCH 4:
  batch 50 loss: 1.7022615718841552
  batch 100 loss: 1.711578643321991
  batch 150 loss: 1.6520737195014954
  batch 200 loss: 1.6939133262634278
  batch 250 loss: 1.6719829154014587
  batch 300 loss: 1.676156678199768
  batch 350 loss: 1.5973874950408935
  batch 400 loss: 1.6457956194877625
  batch 450 loss: 1.6246900796890258
  batch 500 loss: 1.5897568821907044
  batch 550 loss: 1.6108159852027892
  batch 600 loss: 1.6265040349960327
  batch 650 loss: 1.6135948586463928
  batch 700 loss: 1.5739582586288452
  batch 750 loss: 1.534239444732666
  batch 800 loss: 1.5032200813293457
  batch 850 loss: 1.5469804859161378
  batch 900 loss: 1.5743656396865844
LOSS train 1.57437 valid 1.53564, valid PER 56.49%
EPOCH 5:
  batch 50 loss: 1.5194181060791017
  batch 100 loss: 1.502240653038025
  batch 150 loss: 1.521416494846344
  batch 200 loss: 1.4654545879364014
  batch 250 loss: 1.475076868534088
  batch 300 loss: 1.4765420699119567
  batch 350 loss: 1.4846504616737366
  batch 400 loss: 1.462644999027252
  batch 450 loss: 1.454419333934784
  batch 500 loss: 1.455711510181427
  batch 550 loss: 1.383736753463745
  batch 600 loss: 1.4739465880393983
  batch 650 loss: 1.4071386408805848
  batch 700 loss: 1.4526851630210877
  batch 750 loss: 1.3648197054862976
  batch 800 loss: 1.4143404078483581
  batch 850 loss: 1.4052602291107177
  batch 900 loss: 1.4041410303115844
LOSS train 1.40414 valid 1.40756, valid PER 43.49%
EPOCH 6:
  batch 50 loss: 1.4156808423995972
  batch 100 loss: 1.3442827010154723
  batch 150 loss: 1.3423501443862915
  batch 200 loss: 1.3424362540245056
  batch 250 loss: 1.3721387672424317
  batch 300 loss: 1.3320683455467224
  batch 350 loss: 1.3348140001296998
  batch 400 loss: 1.3102658104896545
  batch 450 loss: 1.354211459159851
  batch 500 loss: 1.3216389775276185
  batch 550 loss: 1.337851996421814
  batch 600 loss: 1.3023918640613557
  batch 650 loss: 1.3055268716812134
  batch 700 loss: 1.2983478903770447
  batch 750 loss: 1.286079684495926
  batch 800 loss: 1.2814946818351745
  batch 850 loss: 1.2759810757637025
  batch 900 loss: 1.293144598007202
LOSS train 1.29314 valid 1.30546, valid PER 41.80%
EPOCH 7:
  batch 50 loss: 1.27417316198349
  batch 100 loss: 1.281426055431366
  batch 150 loss: 1.2612173414230348
  batch 200 loss: 1.246847403049469
  batch 250 loss: 1.2544978308677672
  batch 300 loss: 1.2296549546718598
  batch 350 loss: 1.2319891369342804
  batch 400 loss: 1.2378169679641724
  batch 450 loss: 1.2310265386104584
  batch 500 loss: 1.2231653666496276
  batch 550 loss: 1.2360279393196105
  batch 600 loss: 1.2542393958568574
  batch 650 loss: 1.2251480329036712
  batch 700 loss: 1.2262813353538513
  batch 750 loss: 1.2086763739585877
  batch 800 loss: 1.206005095243454
  batch 850 loss: 1.2244171357154847
  batch 900 loss: 1.2560293412208556
LOSS train 1.25603 valid 1.24145, valid PER 39.75%
EPOCH 8:
  batch 50 loss: 1.2020322537422181
  batch 100 loss: 1.1930887758731843
  batch 150 loss: 1.1941878008842468
  batch 200 loss: 1.153710126876831
  batch 250 loss: 1.2001243126392365
  batch 300 loss: 1.1195836222171784
  batch 350 loss: 1.1968747222423552
  batch 400 loss: 1.1658657670021058
  batch 450 loss: 1.1779691302776336
  batch 500 loss: 1.219537422657013
  batch 550 loss: 1.1426313877105714
  batch 600 loss: 1.1847365736961364
  batch 650 loss: 1.206637318134308
  batch 700 loss: 1.1445626258850097
  batch 750 loss: 1.1530284833908082
  batch 800 loss: 1.1698162746429444
  batch 850 loss: 1.1631389498710631
  batch 900 loss: 1.158964068889618
LOSS train 1.15896 valid 1.17521, valid PER 37.28%
EPOCH 9:
  batch 50 loss: 1.096707799434662
  batch 100 loss: 1.1447503912448882
  batch 150 loss: 1.1377881896495818
  batch 200 loss: 1.1033525991439819
  batch 250 loss: 1.1345967304706575
  batch 300 loss: 1.1452485024929047
  batch 350 loss: 1.158986588716507
  batch 400 loss: 1.1392173147201539
  batch 450 loss: 1.120747641324997
  batch 500 loss: 1.100345914363861
  batch 550 loss: 1.1400247859954833
  batch 600 loss: 1.1477403008937836
  batch 650 loss: 1.0943658649921417
  batch 700 loss: 1.1017971813678742
  batch 750 loss: 1.1186664938926696
  batch 800 loss: 1.1374372637271881
  batch 850 loss: 1.1484151446819306
  batch 900 loss: 1.0850714921951294
LOSS train 1.08507 valid 1.15676, valid PER 35.74%
EPOCH 10:
  batch 50 loss: 1.067516874074936
  batch 100 loss: 1.091828510761261
  batch 150 loss: 1.1125263094902038
  batch 200 loss: 1.1114022564888
  batch 250 loss: 1.118581281900406
  batch 300 loss: 1.0614762151241302
  batch 350 loss: 1.1039158368110658
  batch 400 loss: 1.0606734240055085
  batch 450 loss: 1.0549406456947326
  batch 500 loss: 1.0930205810070037
  batch 550 loss: 1.1138828444480895
  batch 600 loss: 1.0752796411514283
  batch 650 loss: 1.0656618392467498
  batch 700 loss: 1.0826832044124604
  batch 750 loss: 1.064284292459488
  batch 800 loss: 1.0808278155326843
  batch 850 loss: 1.0771044921875
  batch 900 loss: 1.0840603041648864
LOSS train 1.08406 valid 1.15030, valid PER 38.10%
EPOCH 11:
  batch 50 loss: 1.0449570763111113
  batch 100 loss: 1.0223954868316651
  batch 150 loss: 1.026741522550583
  batch 200 loss: 1.0784263694286347
  batch 250 loss: 1.04915651679039
  batch 300 loss: 1.0337292850017548
  batch 350 loss: 1.044589866399765
  batch 400 loss: 1.0736775660514832
  batch 450 loss: 1.0580796217918396
  batch 500 loss: 1.0394415414333344
  batch 550 loss: 1.0335969924926758
  batch 600 loss: 1.0383521652221679
  batch 650 loss: 1.0765669167041778
  batch 700 loss: 1.015638016462326
  batch 750 loss: 1.0176313734054565
  batch 800 loss: 1.0812206053733826
  batch 850 loss: 1.0620678424835206
  batch 900 loss: 1.0620251989364624
LOSS train 1.06203 valid 1.08763, valid PER 34.59%
EPOCH 12:
  batch 50 loss: 1.028094049692154
  batch 100 loss: 1.0103473734855652
  batch 150 loss: 1.0002202153205872
  batch 200 loss: 0.9927923834323883
  batch 250 loss: 1.03562273979187
  batch 300 loss: 1.0153512644767761
  batch 350 loss: 1.0267733144760132
  batch 400 loss: 1.036950844526291
  batch 450 loss: 1.0345132780075073
  batch 500 loss: 1.0542281043529511
  batch 550 loss: 0.9488302373886108
  batch 600 loss: 0.993664311170578
  batch 650 loss: 1.0404130005836487
  batch 700 loss: 1.0113809895515442
  batch 750 loss: 1.014326686859131
  batch 800 loss: 0.9874770772457123
  batch 850 loss: 1.0384096086025238
  batch 900 loss: 1.028973673582077
LOSS train 1.02897 valid 1.05567, valid PER 34.17%
EPOCH 13:
  batch 50 loss: 0.9822532999515533
  batch 100 loss: 1.0003405845165252
  batch 150 loss: 0.9651867628097535
  batch 200 loss: 0.9683604729175568
  batch 250 loss: 0.9897108387947082
  batch 300 loss: 0.962562724351883
  batch 350 loss: 0.9815531027317047
  batch 400 loss: 0.9984808027744293
  batch 450 loss: 1.003438231945038
  batch 500 loss: 0.9677189016342163
  batch 550 loss: 1.008371458053589
  batch 600 loss: 0.9754138767719269
  batch 650 loss: 1.0016105854511261
  batch 700 loss: 1.0075176751613617
  batch 750 loss: 0.9541373181343079
  batch 800 loss: 0.9796448945999146
  batch 850 loss: 1.0228220546245574
  batch 900 loss: 0.9931650161743164
LOSS train 0.99317 valid 1.05742, valid PER 32.78%
EPOCH 14:
  batch 50 loss: 0.9706212317943573
  batch 100 loss: 0.9706012833118439
  batch 150 loss: 0.9709120953083038
  batch 200 loss: 0.9644856607913971
  batch 250 loss: 0.9634679341316223
  batch 300 loss: 0.9906481575965881
  batch 350 loss: 0.9360433113574982
  batch 400 loss: 0.955274703502655
  batch 450 loss: 0.9415127265453339
  batch 500 loss: 0.9592953133583069
  batch 550 loss: 0.9869472503662109
  batch 600 loss: 0.945905442237854
  batch 650 loss: 0.9722521221637725
  batch 700 loss: 0.9892231512069702
  batch 750 loss: 0.9406098556518555
  batch 800 loss: 0.9143719220161438
  batch 850 loss: 0.9914048504829407
  batch 900 loss: 0.9724664950370788
LOSS train 0.97247 valid 1.04972, valid PER 33.57%
EPOCH 15:
  batch 50 loss: 0.9466249573230744
  batch 100 loss: 0.9261280250549316
  batch 150 loss: 0.9220935285091401
  batch 200 loss: 0.9525309908390045
  batch 250 loss: 0.9658238887786865
  batch 300 loss: 0.9174668490886688
  batch 350 loss: 0.9410351145267487
  batch 400 loss: 0.9356582021713257
  batch 450 loss: 0.9321197319030762
  batch 500 loss: 0.9165580320358276
  batch 550 loss: 0.9440707075595856
  batch 600 loss: 0.9595041418075562
  batch 650 loss: 0.972061927318573
  batch 700 loss: 0.953106894493103
  batch 750 loss: 0.9473749876022339
  batch 800 loss: 0.9311616969108581
  batch 850 loss: 0.9028548347949982
  batch 900 loss: 0.9331672739982605
LOSS train 0.93317 valid 1.03168, valid PER 32.42%
EPOCH 16:
  batch 50 loss: 0.941798632144928
  batch 100 loss: 0.8851452350616456
  batch 150 loss: 0.9051120686531067
  batch 200 loss: 0.9101045870780945
  batch 250 loss: 0.9327804899215698
  batch 300 loss: 0.9206687450408936
  batch 350 loss: 0.9509968471527099
  batch 400 loss: 0.9317995727062225
  batch 450 loss: 0.9478440141677856
  batch 500 loss: 0.8813915348052979
  batch 550 loss: 0.9512251627445221
  batch 600 loss: 0.9141034877300263
  batch 650 loss: 0.922712779045105
  batch 700 loss: 0.8913935279846191
  batch 750 loss: 0.9157555389404297
  batch 800 loss: 0.9301604437828064
  batch 850 loss: 0.8981467890739441
  batch 900 loss: 0.8962600183486938
LOSS train 0.89626 valid 1.00622, valid PER 30.94%
EPOCH 17:
  batch 50 loss: 0.898695148229599
  batch 100 loss: 0.9014709031581879
  batch 150 loss: 0.8768943417072296
  batch 200 loss: 0.8933488416671753
  batch 250 loss: 0.9123387157917022
  batch 300 loss: 0.9098075151443481
  batch 350 loss: 0.8629134333133698
  batch 400 loss: 0.9260212123394013
  batch 450 loss: 0.914770575761795
  batch 500 loss: 0.8772588348388672
  batch 550 loss: 0.9071422493457795
  batch 600 loss: 0.9408119761943817
  batch 650 loss: 0.8894052481651307
  batch 700 loss: 0.8960973227024078
  batch 750 loss: 0.8623305583000183
  batch 800 loss: 0.8802348363399506
  batch 850 loss: 0.8864010155200959
  batch 900 loss: 0.871733570098877
LOSS train 0.87173 valid 1.01281, valid PER 31.16%
EPOCH 18:
  batch 50 loss: 0.8807101321220397
  batch 100 loss: 0.884820683002472
  batch 150 loss: 0.8830895483493805
  batch 200 loss: 0.8951150619983673
  batch 250 loss: 0.8693615114688873
  batch 300 loss: 0.8600462257862092
  batch 350 loss: 0.8805164837837219
  batch 400 loss: 0.8496511435508728
  batch 450 loss: 0.8969489300251007
  batch 500 loss: 0.8750143468379974
  batch 550 loss: 0.8966116368770599
  batch 600 loss: 0.8621767771244049
  batch 650 loss: 0.8630707252025605
  batch 700 loss: 0.9007838749885559
  batch 750 loss: 0.866561188697815
  batch 800 loss: 0.8516064822673798
  batch 850 loss: 0.8685950219631196
  batch 900 loss: 0.8993555462360382
LOSS train 0.89936 valid 1.00251, valid PER 31.72%
EPOCH 19:
  batch 50 loss: 0.8258113479614257
  batch 100 loss: 0.8346522355079651
  batch 150 loss: 0.8577227103710174
  batch 200 loss: 0.8610110116004944
  batch 250 loss: 0.8669162917137146
  batch 300 loss: 0.8514949429035187
  batch 350 loss: 0.8583007001876831
  batch 400 loss: 0.8780697464942933
  batch 450 loss: 0.8790901732444764
  batch 500 loss: 0.8645755136013031
  batch 550 loss: 0.8520454776287079
  batch 600 loss: 0.8677451574802398
  batch 650 loss: 0.9102802634239197
  batch 700 loss: 0.8457921910285949
  batch 750 loss: 0.8327432930469513
  batch 800 loss: 0.8736301386356353
  batch 850 loss: 0.8833699655532837
  batch 900 loss: 0.8729166316986084
LOSS train 0.87292 valid 0.98358, valid PER 30.69%
EPOCH 20:
  batch 50 loss: 0.8228758704662323
  batch 100 loss: 0.8342366909980774
  batch 150 loss: 0.8251946985721588
  batch 200 loss: 0.8546711814403534
  batch 250 loss: 0.8366405582427978
  batch 300 loss: 0.8523653316497802
  batch 350 loss: 0.8234181582927704
  batch 400 loss: 0.8203867149353027
  batch 450 loss: 0.8282422661781311
  batch 500 loss: 0.8080331957340241
  batch 550 loss: 0.8862725484371186
  batch 600 loss: 0.8156999802589416
  batch 650 loss: 0.8555884742736817
  batch 700 loss: 0.8683205819129944
  batch 750 loss: 0.8441313493251801
  batch 800 loss: 0.8803883421421052
  batch 850 loss: 0.8664069223403931
  batch 900 loss: 0.8802055609226227
LOSS train 0.88021 valid 0.98632, valid PER 30.12%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230117_201200/model_19
Loading model from checkpoints/20230117_201200/model_19
SUB: 16.81%, DEL: 13.22%, INS: 2.22%, COR: 69.97%, PER: 32.25%
