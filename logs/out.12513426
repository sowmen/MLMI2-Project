Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.139325194358825
  batch 100 loss: 3.3162196493148803
  batch 150 loss: 3.276068186759949
  batch 200 loss: 3.164022650718689
  batch 250 loss: 3.014246549606323
  batch 300 loss: 2.820645604133606
  batch 350 loss: 2.688029408454895
  batch 400 loss: 2.6009356451034544
  batch 450 loss: 2.536863079071045
  batch 500 loss: 2.426254687309265
  batch 550 loss: 2.364060807228088
  batch 600 loss: 2.3054960632324217
  batch 650 loss: 2.233699309825897
  batch 700 loss: 2.219901616573334
  batch 750 loss: 2.168796546459198
  batch 800 loss: 2.1295291328430177
  batch 850 loss: 2.0771934270858763
  batch 900 loss: 2.0697640347480775
LOSS train 2.06976 valid 1.97557, valid PER 76.85%
EPOCH 2:
  batch 50 loss: 1.9922964596748352
  batch 100 loss: 1.9097113156318664
  batch 150 loss: 1.9180713486671448
  batch 200 loss: 1.8877284193038941
  batch 250 loss: 1.8903493690490722
  batch 300 loss: 1.8240880870819092
  batch 350 loss: 1.7390718865394592
  batch 400 loss: 1.7454438281059266
  batch 450 loss: 1.7141016960144042
  batch 500 loss: 1.7181202340126038
  batch 550 loss: 1.7134522318840026
  batch 600 loss: 1.651574237346649
  batch 650 loss: 1.70999192237854
  batch 700 loss: 1.6348529314994813
  batch 750 loss: 1.6351365876197814
  batch 800 loss: 1.568444981575012
  batch 850 loss: 1.5791452240943908
  batch 900 loss: 1.5990680861473083
LOSS train 1.59907 valid 1.56130, valid PER 58.29%
EPOCH 3:
  batch 50 loss: 1.5740351819992064
  batch 100 loss: 1.5142492055892944
  batch 150 loss: 1.508821952342987
  batch 200 loss: 1.5093292999267578
  batch 250 loss: 1.4631147933006288
  batch 300 loss: 1.4758077049255371
  batch 350 loss: 1.5125379157066345
  batch 400 loss: 1.4859383034706115
  batch 450 loss: 1.4496180367469789
  batch 500 loss: 1.455376386642456
  batch 550 loss: 1.4579811716079711
  batch 600 loss: 1.4017478179931642
  batch 650 loss: 1.3897696471214294
  batch 700 loss: 1.4098038053512574
  batch 750 loss: 1.470306258201599
  batch 800 loss: 1.3695645070075988
  batch 850 loss: 1.4021670961380004
  batch 900 loss: 1.3485325288772583
LOSS train 1.34853 valid 1.44619, valid PER 42.57%
EPOCH 4:
  batch 50 loss: 1.3329961538314818
  batch 100 loss: 1.3389873147010802
  batch 150 loss: 1.3027523374557495
  batch 200 loss: 1.3302905678749084
  batch 250 loss: 1.3319338274002075
  batch 300 loss: 1.3469391775131225
  batch 350 loss: 1.2388538241386413
  batch 400 loss: 1.2878133583068847
  batch 450 loss: 1.2650742995738984
  batch 500 loss: 1.2526990425586702
  batch 550 loss: 1.2790677809715272
  batch 600 loss: 1.2894254398345948
  batch 650 loss: 1.2646993291378021
  batch 700 loss: 1.2201483392715453
  batch 750 loss: 1.1996479582786561
  batch 800 loss: 1.1838867032527924
  batch 850 loss: 1.2103989160060882
  batch 900 loss: 1.2635180926322938
LOSS train 1.26352 valid 1.26057, valid PER 39.55%
EPOCH 5:
  batch 50 loss: 1.1710599994659423
  batch 100 loss: 1.1648852157592773
  batch 150 loss: 1.2376297366619111
  batch 200 loss: 1.1525177490711211
  batch 250 loss: 1.1503884994983673
  batch 300 loss: 1.1612292337417602
  batch 350 loss: 1.1732347345352172
  batch 400 loss: 1.1727455520629884
  batch 450 loss: 1.1414834427833558
  batch 500 loss: 1.1636446952819823
  batch 550 loss: 1.1045441484451295
  batch 600 loss: 1.1838388931751251
  batch 650 loss: 1.1465266394615172
  batch 700 loss: 1.179847139120102
  batch 750 loss: 1.1037624967098236
  batch 800 loss: 1.1410020840168
  batch 850 loss: 1.1451647698879241
  batch 900 loss: 1.143458949327469
LOSS train 1.14346 valid 1.16019, valid PER 35.90%
EPOCH 6:
  batch 50 loss: 1.1439114665985108
  batch 100 loss: 1.0701848912239074
  batch 150 loss: 1.0604022669792175
  batch 200 loss: 1.0825583517551423
  batch 250 loss: 1.125065358877182
  batch 300 loss: 1.0967200708389282
  batch 350 loss: 1.096637750864029
  batch 400 loss: 1.0749340975284576
  batch 450 loss: 1.1302277255058288
  batch 500 loss: 1.1072751367092133
  batch 550 loss: 1.0960133612155913
  batch 600 loss: 1.058260556459427
  batch 650 loss: 1.0899108147621155
  batch 700 loss: 1.081194624900818
  batch 750 loss: 1.0584916543960572
  batch 800 loss: 1.0574177515506744
  batch 850 loss: 1.0467900824546814
  batch 900 loss: 1.0780660033226013
LOSS train 1.07807 valid 1.13056, valid PER 35.39%
EPOCH 7:
  batch 50 loss: 1.0525109088420868
  batch 100 loss: 1.0521381032466888
  batch 150 loss: 1.043420580625534
  batch 200 loss: 1.0196952211856842
  batch 250 loss: 1.0328997576236725
  batch 300 loss: 1.015874866247177
  batch 350 loss: 1.0284159445762635
  batch 400 loss: 1.0216729736328125
  batch 450 loss: 1.020962144136429
  batch 500 loss: 1.0222962474822999
  batch 550 loss: 1.0084939312934875
  batch 600 loss: 1.0434201657772064
  batch 650 loss: 1.0038282668590546
  batch 700 loss: 1.0299830889701844
  batch 750 loss: 0.9897487437725068
  batch 800 loss: 1.0003717684745788
  batch 850 loss: 1.0278381145000457
  batch 900 loss: 1.0456937849521637
LOSS train 1.04569 valid 1.08152, valid PER 34.46%
EPOCH 8:
  batch 50 loss: 0.9871738314628601
  batch 100 loss: 0.9714257299900055
  batch 150 loss: 0.9663809502124786
  batch 200 loss: 0.9411776840686799
  batch 250 loss: 0.9781352877616882
  batch 300 loss: 0.9306659209728241
  batch 350 loss: 0.9995797538757324
  batch 400 loss: 0.9539939033985138
  batch 450 loss: 0.9749878132343293
  batch 500 loss: 1.0154049563407899
  batch 550 loss: 0.9589590883255005
  batch 600 loss: 0.9945891714096069
  batch 650 loss: 0.9960960209369659
  batch 700 loss: 0.9552590036392212
  batch 750 loss: 0.9641765367984771
  batch 800 loss: 0.9923797190189362
  batch 850 loss: 0.98603285074234
  batch 900 loss: 0.9815573346614838
LOSS train 0.98156 valid 1.03902, valid PER 31.82%
EPOCH 9:
  batch 50 loss: 0.8992740941047669
  batch 100 loss: 0.9677702260017395
  batch 150 loss: 0.940608549118042
  batch 200 loss: 0.9140716230869294
  batch 250 loss: 0.9398517274856567
  batch 300 loss: 0.9700437068939209
  batch 350 loss: 0.96575164437294
  batch 400 loss: 0.9486988961696625
  batch 450 loss: 0.9432257330417633
  batch 500 loss: 0.9168131530284882
  batch 550 loss: 0.9564956641197204
  batch 600 loss: 0.9656390035152436
  batch 650 loss: 0.9226066255569458
  batch 700 loss: 0.9244079375267029
  batch 750 loss: 0.9442036199569702
  batch 800 loss: 0.9612886738777161
  batch 850 loss: 0.9773233282566071
  batch 900 loss: 0.9335216426849365
LOSS train 0.93352 valid 1.02536, valid PER 31.81%
EPOCH 10:
  batch 50 loss: 0.8749362754821778
  batch 100 loss: 0.8966338956356048
  batch 150 loss: 0.9097359311580658
  batch 200 loss: 0.9163452792167663
  batch 250 loss: 0.933667802810669
  batch 300 loss: 0.8787931275367736
  batch 350 loss: 0.8945724785327911
  batch 400 loss: 0.8617976117134094
  batch 450 loss: 0.8871836400032044
  batch 500 loss: 0.9119242906570435
  batch 550 loss: 0.9286017620563507
  batch 600 loss: 0.8988929522037507
  batch 650 loss: 0.8993193757534027
  batch 700 loss: 0.9096628904342652
  batch 750 loss: 0.892913225889206
  batch 800 loss: 0.9098621737957001
  batch 850 loss: 0.9165035486221313
  batch 900 loss: 0.9137484741210937
LOSS train 0.91375 valid 1.02016, valid PER 32.51%
EPOCH 11:
  batch 50 loss: 0.8445653057098389
  batch 100 loss: 0.8282638430595398
  batch 150 loss: 0.847216340303421
  batch 200 loss: 0.8864158153533935
  batch 250 loss: 0.8871622180938721
  batch 300 loss: 0.8584529507160187
  batch 350 loss: 0.8931989359855652
  batch 400 loss: 0.9012685787677764
  batch 450 loss: 0.8946756660938263
  batch 500 loss: 0.8710864496231079
  batch 550 loss: 0.8591143846511841
  batch 600 loss: 0.8501720678806305
  batch 650 loss: 0.9200096786022186
  batch 700 loss: 0.848447413444519
  batch 750 loss: 0.8674053347110748
  batch 800 loss: 0.898420375585556
  batch 850 loss: 0.9226581478118896
  batch 900 loss: 0.9114500308036804
LOSS train 0.91145 valid 0.98979, valid PER 30.52%
EPOCH 12:
  batch 50 loss: 0.8522790503501892
  batch 100 loss: 0.8394975996017456
  batch 150 loss: 0.8144941127300263
  batch 200 loss: 0.8384876811504364
  batch 250 loss: 0.8466647303104401
  batch 300 loss: 0.8555736756324768
  batch 350 loss: 0.8269512355327606
  batch 400 loss: 0.8611120891571045
  batch 450 loss: 0.8538080561161041
  batch 500 loss: 0.8824660587310791
  batch 550 loss: 0.8045198333263397
  batch 600 loss: 0.8237998473644257
  batch 650 loss: 0.8689881122112274
  batch 700 loss: 0.8520325803756714
  batch 750 loss: 0.8406139242649079
  batch 800 loss: 0.8363147991895675
  batch 850 loss: 0.8785295104980468
  batch 900 loss: 0.8660302680730819
LOSS train 0.86603 valid 0.98092, valid PER 30.44%
EPOCH 13:
  batch 50 loss: 0.8006191253662109
  batch 100 loss: 0.8177638339996338
  batch 150 loss: 0.8012153017520904
  batch 200 loss: 0.8171930599212647
  batch 250 loss: 0.8123682475090027
  batch 300 loss: 0.7936992740631104
  batch 350 loss: 0.7982392823696136
  batch 400 loss: 0.8333686482906342
  batch 450 loss: 0.832387136220932
  batch 500 loss: 0.7856031322479248
  batch 550 loss: 0.8382266867160797
  batch 600 loss: 0.7871652483940125
  batch 650 loss: 0.8290526217222214
  batch 700 loss: 0.8379108142852784
  batch 750 loss: 0.802768530845642
  batch 800 loss: 0.8177376246452331
  batch 850 loss: 0.8606251788139343
  batch 900 loss: 0.8418987250328064
LOSS train 0.84190 valid 1.00057, valid PER 31.02%
EPOCH 14:
  batch 50 loss: 0.792245979309082
  batch 100 loss: 0.8258000493049622
  batch 150 loss: 0.7933209943771362
  batch 200 loss: 0.7983198231458664
  batch 250 loss: 0.7833866024017334
  batch 300 loss: 0.8299170577526093
  batch 350 loss: 0.7623411834239959
  batch 400 loss: 0.7832793545722961
  batch 450 loss: 0.7828917956352234
  batch 500 loss: 0.8038346898555756
  batch 550 loss: 0.8131619417667388
  batch 600 loss: 0.7860525274276733
  batch 650 loss: 0.8244911646842956
  batch 700 loss: 0.8431946861743927
  batch 750 loss: 0.7903968572616578
  batch 800 loss: 0.7659339380264282
  batch 850 loss: 0.8104537880420685
  batch 900 loss: 0.8075936710834504
LOSS train 0.80759 valid 0.97350, valid PER 29.62%
EPOCH 15:
  batch 50 loss: 0.7705816376209259
  batch 100 loss: 0.7686521130800247
  batch 150 loss: 0.7712243103981018
  batch 200 loss: 0.7969398665428161
  batch 250 loss: 0.7864137077331543
  batch 300 loss: 0.7586717355251312
  batch 350 loss: 0.7648223960399627
  batch 400 loss: 0.7669110357761383
  batch 450 loss: 0.7728097277879715
  batch 500 loss: 0.735047721862793
  batch 550 loss: 0.7873373234272003
  batch 600 loss: 0.7996964359283447
  batch 650 loss: 0.7989246499538422
  batch 700 loss: 0.7951079154014588
  batch 750 loss: 0.790730413198471
  batch 800 loss: 0.7751329505443573
  batch 850 loss: 0.7416987133026123
  batch 900 loss: 0.7826056694984436
LOSS train 0.78261 valid 0.96696, valid PER 29.61%
EPOCH 16:
  batch 50 loss: 0.7662719714641572
  batch 100 loss: 0.7242447757720947
  batch 150 loss: 0.7315468060970306
  batch 200 loss: 0.7484661567211152
  batch 250 loss: 0.7650736963748932
  batch 300 loss: 0.7554411888122559
  batch 350 loss: 0.7814476865530015
  batch 400 loss: 0.7828135943412781
  batch 450 loss: 0.7763762438297271
  batch 500 loss: 0.7550311613082886
  batch 550 loss: 0.7734093689918518
  batch 600 loss: 0.7563288748264313
  batch 650 loss: 0.770282838344574
  batch 700 loss: 0.7425093901157379
  batch 750 loss: 0.772897287607193
  batch 800 loss: 0.7622391891479492
  batch 850 loss: 0.7705519115924835
  batch 900 loss: 0.7544006454944611
LOSS train 0.75440 valid 0.95930, valid PER 28.67%
EPOCH 17:
  batch 50 loss: 0.716138778924942
  batch 100 loss: 0.7309909093379975
  batch 150 loss: 0.7149571913480759
  batch 200 loss: 0.7292510288953781
  batch 250 loss: 0.7329162329435348
  batch 300 loss: 0.7313706207275391
  batch 350 loss: 0.7127850985527039
  batch 400 loss: 0.7775426006317139
  batch 450 loss: 0.7547580134868622
  batch 500 loss: 0.738832539319992
  batch 550 loss: 0.7443964147567749
  batch 600 loss: 0.7849939465522766
  batch 650 loss: 0.7366057014465333
  batch 700 loss: 0.7471369397640228
  batch 750 loss: 0.7275247919559479
  batch 800 loss: 0.732784959077835
  batch 850 loss: 0.7485885274410248
  batch 900 loss: 0.7301890248060227
LOSS train 0.73019 valid 0.95769, valid PER 28.44%
EPOCH 18:
  batch 50 loss: 0.7156152737140655
  batch 100 loss: 0.7249173676967621
  batch 150 loss: 0.7313765436410904
  batch 200 loss: 0.7092866396903992
  batch 250 loss: 0.7239881217479706
  batch 300 loss: 0.7006759589910507
  batch 350 loss: 0.7201219701766968
  batch 400 loss: 0.6947565907239914
  batch 450 loss: 0.7272558951377869
  batch 500 loss: 0.7249116861820221
  batch 550 loss: 0.730535832643509
  batch 600 loss: 0.6875736349821091
  batch 650 loss: 0.7077706003189087
  batch 700 loss: 0.7371348392963409
  batch 750 loss: 0.7147256445884704
  batch 800 loss: 0.7180874490737915
  batch 850 loss: 0.7259119868278503
  batch 900 loss: 0.7547778582572937
LOSS train 0.75478 valid 0.96967, valid PER 29.43%
EPOCH 19:
  batch 50 loss: 0.6547264665365219
  batch 100 loss: 0.6708765530586243
  batch 150 loss: 0.7028280580043793
  batch 200 loss: 0.7101559311151504
  batch 250 loss: 0.6997031033039093
  batch 300 loss: 0.7004838955402374
  batch 350 loss: 0.7005158632993698
  batch 400 loss: 0.6907734376192093
  batch 450 loss: 0.72727629840374
  batch 500 loss: 0.7139034187793731
  batch 550 loss: 0.6868588519096375
  batch 600 loss: 0.715991353392601
  batch 650 loss: 0.7778630387783051
  batch 700 loss: 0.7127984690666199
  batch 750 loss: 0.7078244197368622
  batch 800 loss: 0.7278872120380402
  batch 850 loss: 0.7257693707942963
  batch 900 loss: 0.713113534450531
LOSS train 0.71311 valid 0.95645, valid PER 28.94%
EPOCH 20:
  batch 50 loss: 0.6548688393831253
  batch 100 loss: 0.6560976779460908
  batch 150 loss: 0.6756375205516815
  batch 200 loss: 0.6883042812347412
  batch 250 loss: 0.6878354895114899
  batch 300 loss: 0.6960646605491638
  batch 350 loss: 0.661455237865448
  batch 400 loss: 0.6930317223072052
  batch 450 loss: 0.7002852380275726
  batch 500 loss: 0.6718618386983871
  batch 550 loss: 0.720017957687378
  batch 600 loss: 0.6670829671621322
  batch 650 loss: 0.7103922337293624
  batch 700 loss: 0.7150278103351593
  batch 750 loss: 0.6747651302814484
  batch 800 loss: 0.717233943939209
  batch 850 loss: 0.7119759404659272
  batch 900 loss: 0.7142247343063355
LOSS train 0.71422 valid 0.97440, valid PER 29.20%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_203533/model_19
Loading model from checkpoints/20230117_203533/model_19
SUB: 17.06%, DEL: 11.87%, INS: 2.27%, COR: 71.07%, PER: 31.20%
