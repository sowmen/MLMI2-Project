Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.138687491416931
  batch 100 loss: 3.2381810045242307
  batch 150 loss: 3.1396832513809203
  batch 200 loss: 2.9125371742248536
  batch 250 loss: 2.8462927865982057
  batch 300 loss: 2.5349031925201415
  batch 350 loss: 2.4085906982421874
  batch 400 loss: 2.2944309735298156
  batch 450 loss: 2.170878188610077
  batch 500 loss: 2.035708320140839
  batch 550 loss: 1.9624896335601807
  batch 600 loss: 1.9306001853942871
  batch 650 loss: 1.841752862930298
  batch 700 loss: 1.8084449672698975
  batch 750 loss: 1.8123153257369995
  batch 800 loss: 1.7503753089904786
  batch 850 loss: 1.7232358074188232
  batch 900 loss: 1.6909483504295348
LOSS train 1.69095 valid 1.66023, valid PER 54.63%
EPOCH 2:
  batch 50 loss: 1.6076171112060547
  batch 100 loss: 1.5512422704696656
  batch 150 loss: 1.5883077716827392
  batch 200 loss: 1.5832173919677734
  batch 250 loss: 1.5972897148132323
  batch 300 loss: 1.522060408592224
  batch 350 loss: 1.449689702987671
  batch 400 loss: 1.4584537315368653
  batch 450 loss: 1.4053669738769532
  batch 500 loss: 1.43987087726593
  batch 550 loss: 1.4434369707107544
  batch 600 loss: 1.3923448395729066
  batch 650 loss: 1.4223504543304444
  batch 700 loss: 1.4127475357055663
  batch 750 loss: 1.3856507396698
  batch 800 loss: 1.3080910992622377
  batch 850 loss: 1.3326651108264924
  batch 900 loss: 1.3652587676048278
LOSS train 1.36526 valid 1.36719, valid PER 42.53%
EPOCH 3:
  batch 50 loss: 1.2800623893737793
  batch 100 loss: 1.2774451100826263
  batch 150 loss: 1.270505164861679
  batch 200 loss: 1.242851482629776
  batch 250 loss: 1.2411985325813293
  batch 300 loss: 1.2373582601547242
  batch 350 loss: 1.2954923391342164
  batch 400 loss: 1.2470950484275818
  batch 450 loss: 1.2449262595176698
  batch 500 loss: 1.2237247622013092
  batch 550 loss: 1.2359041011333465
  batch 600 loss: 1.2261710095405578
  batch 650 loss: 1.1891440224647523
  batch 700 loss: 1.2167716026306152
  batch 750 loss: 1.2470904064178467
  batch 800 loss: 1.179351762533188
  batch 850 loss: 1.2284972035884858
  batch 900 loss: 1.153226363658905
LOSS train 1.15323 valid 1.21727, valid PER 36.85%
EPOCH 4:
  batch 50 loss: 1.142786866426468
  batch 100 loss: 1.1846375131607056
  batch 150 loss: 1.0987609481811524
  batch 200 loss: 1.1392859780788422
  batch 250 loss: 1.145788825750351
  batch 300 loss: 1.1837705230712892
  batch 350 loss: 1.083988720178604
  batch 400 loss: 1.147782974243164
  batch 450 loss: 1.1130878281593324
  batch 500 loss: 1.1180871963500976
  batch 550 loss: 1.1417161691188813
  batch 600 loss: 1.1678767609596252
  batch 650 loss: 1.1560156393051146
  batch 700 loss: 1.1254967868328094
  batch 750 loss: 1.0864499521255493
  batch 800 loss: 1.0532295620441436
  batch 850 loss: 1.1005303490161895
  batch 900 loss: 1.1309391140937806
LOSS train 1.13094 valid 1.13971, valid PER 36.30%
EPOCH 5:
  batch 50 loss: 1.0479830288887024
  batch 100 loss: 1.0478824138641358
  batch 150 loss: 1.091430948972702
  batch 200 loss: 1.04167973279953
  batch 250 loss: 1.055751632452011
  batch 300 loss: 1.0726202368736266
  batch 350 loss: 1.0404462242126464
  batch 400 loss: 1.0619475734233856
  batch 450 loss: 1.0598685312271119
  batch 500 loss: 1.0591385543346405
  batch 550 loss: 1.030874570608139
  batch 600 loss: 1.098252067565918
  batch 650 loss: 1.0339503347873689
  batch 700 loss: 1.067457798719406
  batch 750 loss: 1.018293137550354
  batch 800 loss: 1.0906331980228423
  batch 850 loss: 1.1039235985279083
  batch 900 loss: 1.05568186044693
LOSS train 1.05568 valid 1.10433, valid PER 33.80%
EPOCH 6:
  batch 50 loss: 1.0489135217666625
  batch 100 loss: 1.0044377875328063
  batch 150 loss: 1.005129985809326
  batch 200 loss: 1.0397675776481627
  batch 250 loss: 1.0692598724365234
  batch 300 loss: 1.02729083776474
  batch 350 loss: 1.012162709236145
  batch 400 loss: 1.006851464509964
  batch 450 loss: 1.0200027203559876
  batch 500 loss: 1.0215822219848634
  batch 550 loss: 1.0413107860088349
  batch 600 loss: 0.9916045665740967
  batch 650 loss: 1.0593958163261414
  batch 700 loss: 1.0788208603858949
  batch 750 loss: 1.0529226386547088
  batch 800 loss: 1.0409255313873291
  batch 850 loss: 1.0154825615882874
  batch 900 loss: 1.1815583968162537
LOSS train 1.18156 valid 1.23637, valid PER 36.50%
EPOCH 7:
  batch 50 loss: 1.073800939321518
  batch 100 loss: 1.0567428183555603
  batch 150 loss: 0.9964488375186921
  batch 200 loss: 0.973616350889206
  batch 250 loss: 0.9775950014591217
  batch 300 loss: 0.9905322456359863
  batch 350 loss: 0.9633994889259339
  batch 400 loss: 0.9843765878677369
  batch 450 loss: 0.9712509143352509
  batch 500 loss: 0.9779961013793945
  batch 550 loss: 0.9846447849273682
  batch 600 loss: 1.025868581533432
  batch 650 loss: 0.9868546628952026
  batch 700 loss: 1.0222480118274688
  batch 750 loss: 0.9701596915721893
  batch 800 loss: 0.9635894358158111
  batch 850 loss: 1.010251795053482
  batch 900 loss: 1.011216584444046
LOSS train 1.01122 valid 1.04274, valid PER 31.80%
EPOCH 8:
  batch 50 loss: 0.9295648348331451
  batch 100 loss: 0.9132525730133056
  batch 150 loss: 0.9143608212471008
  batch 200 loss: 0.9359369874000549
  batch 250 loss: 0.9670296323299408
  batch 300 loss: 0.9523032593727112
  batch 350 loss: 0.9909343409538269
  batch 400 loss: 0.9251697504520416
  batch 450 loss: 0.976377991437912
  batch 500 loss: 1.0324550890922546
  batch 550 loss: 0.9605482172966003
  batch 600 loss: 0.9954034113883972
  batch 650 loss: 1.0223933494091033
  batch 700 loss: 0.9395944821834564
  batch 750 loss: 0.9587761616706848
  batch 800 loss: 0.9768601632118226
  batch 850 loss: 0.9564792513847351
  batch 900 loss: 0.9521824038028717
LOSS train 0.95218 valid 1.03189, valid PER 31.52%
EPOCH 9:
  batch 50 loss: 0.8731195890903473
  batch 100 loss: 0.9244840002059936
  batch 150 loss: 0.9810329282283783
  batch 200 loss: 0.9672873437404632
  batch 250 loss: 0.9381723737716675
  batch 300 loss: 0.9536063468456268
  batch 350 loss: 0.9829309511184693
  batch 400 loss: 0.9354860973358154
  batch 450 loss: 0.9523620629310607
  batch 500 loss: 0.9114646422863006
  batch 550 loss: 0.964381639957428
  batch 600 loss: 0.9597910666465759
  batch 650 loss: 0.9417343962192536
  batch 700 loss: 0.9168374252319336
  batch 750 loss: 0.9384156548976899
  batch 800 loss: 0.9401997399330139
  batch 850 loss: 0.9497463798522949
  batch 900 loss: 0.9064462661743165
LOSS train 0.90645 valid 1.05654, valid PER 31.96%
EPOCH 10:
  batch 50 loss: 0.8544196271896363
  batch 100 loss: 0.8808459794521332
  batch 150 loss: 0.931578893661499
  batch 200 loss: 0.89798126578331
  batch 250 loss: 0.946048401594162
  batch 300 loss: 0.8879009056091308
  batch 350 loss: 0.9914699470996857
  batch 400 loss: 1.227604340314865
  batch 450 loss: 1.0028535509109497
  batch 500 loss: 1.0585682046413423
  batch 550 loss: 1.0238083958625794
  batch 600 loss: 1.0579070591926574
  batch 650 loss: 1.011234325170517
  batch 700 loss: 1.078012933731079
  batch 750 loss: 1.0066415405273437
  batch 800 loss: 1.0179113161563873
  batch 850 loss: 1.0446050143241883
  batch 900 loss: 1.0358435368537904
LOSS train 1.03584 valid 1.13945, valid PER 35.21%
EPOCH 11:
  batch 50 loss: 0.9573284661769867
  batch 100 loss: 0.9241454732418061
  batch 150 loss: 0.9653201055526733
  batch 200 loss: 0.9780457019805908
  batch 250 loss: 0.9332354927062988
  batch 300 loss: 0.9046887171268463
  batch 350 loss: 0.9470654571056366
  batch 400 loss: 0.9767441284656525
  batch 450 loss: 0.9239647734165192
  batch 500 loss: 0.9122973370552063
  batch 550 loss: 0.9392853689193725
  batch 600 loss: 0.9413656079769135
  batch 650 loss: 1.0497244238853454
  batch 700 loss: 0.940369154214859
  batch 750 loss: 0.9777571070194244
  batch 800 loss: 1.0270186388492584
  batch 850 loss: 1.00507164478302
  batch 900 loss: 0.9890260934829712
LOSS train 0.98903 valid 1.06450, valid PER 32.01%
EPOCH 12:
  batch 50 loss: 0.9776860213279724
  batch 100 loss: 0.9536997497081756
  batch 150 loss: 0.9409339559078217
  batch 200 loss: 0.9312993550300598
  batch 250 loss: 0.9955617392063141
  batch 300 loss: 0.9405656373500824
  batch 350 loss: 0.9426285767555237
  batch 400 loss: 0.9566953372955322
  batch 450 loss: 0.9956117987632751
  batch 500 loss: 0.9984003293514252
  batch 550 loss: 0.9100635862350464
  batch 600 loss: 0.9088505780696869
  batch 650 loss: 1.056397248506546
  batch 700 loss: 1.0369730603694916
  batch 750 loss: 0.9468752348423004
  batch 800 loss: 0.9210180819034577
  batch 850 loss: 0.986737174987793
  batch 900 loss: 0.9586036622524261
LOSS train 0.95860 valid 1.03162, valid PER 31.34%
EPOCH 13:
  batch 50 loss: 0.8630464363098145
  batch 100 loss: 0.8997070717811585
  batch 150 loss: 0.896998039484024
  batch 200 loss: 0.8764121508598328
  batch 250 loss: 0.8567001104354859
  batch 300 loss: 0.8708543705940247
  batch 350 loss: 0.9198485386371612
  batch 400 loss: 0.9337783241271973
  batch 450 loss: 0.9030132794380188
  batch 500 loss: 0.895597962141037
  batch 550 loss: 0.8829312300682068
  batch 600 loss: 0.8779033708572388
  batch 650 loss: 0.8942072892189026
  batch 700 loss: 0.9003973627090454
  batch 750 loss: 0.9237791562080383
  batch 800 loss: 0.9705073595046997
  batch 850 loss: 0.9926034474372863
  batch 900 loss: 0.9897347629070282
LOSS train 0.98973 valid 1.07584, valid PER 32.04%
EPOCH 14:
  batch 50 loss: 0.9020023345947266
  batch 100 loss: 0.9283372461795807
  batch 150 loss: 0.9055301260948181
  batch 200 loss: 1.1746469604969025
  batch 250 loss: 1.2597614324092865
  batch 300 loss: 1.166222048997879
  batch 350 loss: 1.1022226083278657
  batch 400 loss: 1.049928094148636
  batch 450 loss: 1.0517005348205566
  batch 500 loss: 1.0407805395126344
  batch 550 loss: 1.0469288337230682
  batch 600 loss: 0.9754825961589814
  batch 650 loss: 1.0450853776931763
  batch 700 loss: 1.0453732335567474
  batch 750 loss: 0.9739136230945588
  batch 800 loss: 0.9208228409290313
  batch 850 loss: 0.9883054065704345
  batch 900 loss: 1.0074464654922486
LOSS train 1.00745 valid 1.20918, valid PER 35.50%
EPOCH 15:
  batch 50 loss: 1.0373872566223143
  batch 100 loss: 1.0109285068511964
  batch 150 loss: 0.9447739565372467
  batch 200 loss: 0.9870499205589295
  batch 250 loss: 0.9448561012744904
  batch 300 loss: 0.9382112884521484
  batch 350 loss: 0.9595166361331939
  batch 400 loss: 0.9613299930095672
  batch 450 loss: 0.9722374629974365
  batch 500 loss: 0.9566067719459533
  batch 550 loss: 1.07153009057045
  batch 600 loss: 1.0839080548286437
  batch 650 loss: 1.0876275193691254
  batch 700 loss: 1.0702744626998901
  batch 750 loss: 1.017794806957245
  batch 800 loss: 1.1145302391052245
  batch 850 loss: 1.1783517289161682
  batch 900 loss: 1.117424682378769
LOSS train 1.11742 valid 1.28518, valid PER 36.32%
EPOCH 16:
  batch 50 loss: 1.0995610332489014
  batch 100 loss: 1.0254355335235597
  batch 150 loss: 0.9836035263538361
  batch 200 loss: 1.01271404504776
  batch 250 loss: 1.036725311279297
  batch 300 loss: 1.0134774160385132
  batch 350 loss: 1.0413393187522888
  batch 400 loss: 1.027648879289627
  batch 450 loss: 1.0254621386528016
  batch 500 loss: 0.9639189052581787
  batch 550 loss: 1.0279183673858643
  batch 600 loss: 0.9782901811599731
  batch 650 loss: 0.9781939256191253
  batch 700 loss: 0.9556161224842071
  batch 750 loss: 0.9733845949172973
  batch 800 loss: 0.9767456388473511
  batch 850 loss: 0.9733935570716858
  batch 900 loss: 0.950958251953125
LOSS train 0.95096 valid 1.08373, valid PER 31.94%
EPOCH 17:
  batch 50 loss: 0.934581469297409
  batch 100 loss: 0.9253966498374939
  batch 150 loss: 0.933233586549759
  batch 200 loss: 0.9485169637203217
  batch 250 loss: 0.9462467217445374
  batch 300 loss: 0.9521013963222503
  batch 350 loss: 0.9238470268249511
  batch 400 loss: 0.9812913322448731
  batch 450 loss: 0.9735503005981445
  batch 500 loss: 0.8977346014976502
  batch 550 loss: 0.9732471764087677
  batch 600 loss: 0.9804670667648315
  batch 650 loss: 0.9532045102119446
  batch 700 loss: 1.0154903030395508
  batch 750 loss: 0.9456044244766235
  batch 800 loss: 0.970724904537201
  batch 850 loss: 0.9795598697662353
  batch 900 loss: 0.9368578362464904
LOSS train 0.93686 valid 1.11499, valid PER 33.28%
EPOCH 18:
  batch 50 loss: 0.9407798874378205
  batch 100 loss: 0.9913400173187256
  batch 150 loss: 0.9812097799777985
  batch 200 loss: 0.9826298975944519
  batch 250 loss: 0.9464612662792206
  batch 300 loss: 0.9382014012336731
  batch 350 loss: 0.9411097085475921
  batch 400 loss: 0.8988422310352325
  batch 450 loss: 0.9401295006275177
  batch 500 loss: 0.9246684086322784
  batch 550 loss: 0.9754370188713074
  batch 600 loss: 0.9277145648002625
  batch 650 loss: 0.9448022830486298
  batch 700 loss: 0.9656698834896088
  batch 750 loss: 0.9268968689441681
  batch 800 loss: 0.9310525679588317
  batch 850 loss: 1.0123440253734588
  batch 900 loss: 1.0253691828250886
LOSS train 1.02537 valid 1.11305, valid PER 34.20%
EPOCH 19:
  batch 50 loss: 0.8537991380691529
  batch 100 loss: 0.8880519664287567
  batch 150 loss: 0.8720803213119507
  batch 200 loss: 0.9007741904258728
  batch 250 loss: 0.904765076637268
  batch 300 loss: 0.9382772290706635
  batch 350 loss: 0.9129376196861267
  batch 400 loss: 0.9038482689857483
  batch 450 loss: 0.9203019881248474
  batch 500 loss: 1.0016159105300904
  batch 550 loss: 0.983963371515274
  batch 600 loss: 1.07754558801651
  batch 650 loss: 1.0922014951705932
  batch 700 loss: 0.9836092984676361
  batch 750 loss: 0.9597204530239105
  batch 800 loss: 0.9892502522468567
  batch 850 loss: 0.9544023132324219
  batch 900 loss: 0.9409991908073425
LOSS train 0.94100 valid 1.13808, valid PER 33.82%
EPOCH 20:
  batch 50 loss: 0.91805823802948
  batch 100 loss: 0.8987933707237243
  batch 150 loss: 0.8757658922672271
  batch 200 loss: 0.9077970790863037
  batch 250 loss: 0.9040381133556366
  batch 300 loss: 0.9540234088897706
  batch 350 loss: 0.9196244013309479
  batch 400 loss: 0.9161907589435577
  batch 450 loss: 0.9145785009860993
  batch 500 loss: 0.8949258077144623
  batch 550 loss: 0.9352652060985566
  batch 600 loss: 0.8903575670719147
  batch 650 loss: 0.9036375188827515
  batch 700 loss: 0.9170370280742646
  batch 750 loss: 0.8842998945713043
  batch 800 loss: 0.9185908102989196
  batch 850 loss: 0.8867599201202393
  batch 900 loss: 0.8977486395835876
LOSS train 0.89775 valid 1.06525, valid PER 31.96%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230117_204925/model_12
Loading model from checkpoints/20230117_204925/model_12
SUB: 18.14%, DEL: 12.21%, INS: 2.21%, COR: 69.65%, PER: 32.56%
