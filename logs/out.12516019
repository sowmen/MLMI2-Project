Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.236863489151001
  batch 100 loss: 3.192497673034668
  batch 150 loss: 3.2139860010147094
  batch 200 loss: 2.959187970161438
  batch 250 loss: 2.8395617485046385
  batch 300 loss: 2.613133201599121
  batch 350 loss: 2.4737174463272096
  batch 400 loss: 2.4326459121704103
  batch 450 loss: 2.196788294315338
  batch 500 loss: 2.0979318594932557
  batch 550 loss: 1.979937856197357
  batch 600 loss: 1.9429132390022277
  batch 650 loss: 1.8295525455474853
  batch 700 loss: 1.8234847378730774
  batch 750 loss: 1.7848580288887024
  batch 800 loss: 1.7703850769996643
  batch 850 loss: 1.721325011253357
  batch 900 loss: 1.6674096703529357
LOSS train 1.66741 valid 1.65434, valid PER 53.18%
EPOCH 2:
  batch 50 loss: 1.5983361744880675
  batch 100 loss: 1.559209177494049
  batch 150 loss: 1.5585521697998046
  batch 200 loss: 1.5676939511299133
  batch 250 loss: 1.5608001232147217
  batch 300 loss: 1.5090800023078919
  batch 350 loss: 1.4449041509628295
  batch 400 loss: 1.47034770488739
  batch 450 loss: 1.3845580053329467
  batch 500 loss: 1.4328321933746337
  batch 550 loss: 1.449337170124054
  batch 600 loss: 1.3940055871009827
  batch 650 loss: 1.401316635608673
  batch 700 loss: 1.3925817823410034
  batch 750 loss: 1.3605945372581483
  batch 800 loss: 1.2900416088104247
  batch 850 loss: 1.336983231306076
  batch 900 loss: 1.372494215965271
LOSS train 1.37249 valid 1.31900, valid PER 40.31%
EPOCH 3:
  batch 50 loss: 1.272040729522705
  batch 100 loss: 1.2776702451705932
  batch 150 loss: 1.295533103942871
  batch 200 loss: 1.2696544086933137
  batch 250 loss: 1.2579851830005646
  batch 300 loss: 1.2466475582122802
  batch 350 loss: 1.2823902893066406
  batch 400 loss: 1.2406651496887207
  batch 450 loss: 1.2428843998908996
  batch 500 loss: 1.249937973022461
  batch 550 loss: 1.2376381063461304
  batch 600 loss: 1.236130130290985
  batch 650 loss: 1.2025184476375579
  batch 700 loss: 1.2359471452236175
  batch 750 loss: 1.2667883288860322
  batch 800 loss: 1.1693309772014617
  batch 850 loss: 1.2236455833911897
  batch 900 loss: 1.1559634900093079
LOSS train 1.15596 valid 1.25123, valid PER 37.43%
EPOCH 4:
  batch 50 loss: 1.1489511609077454
  batch 100 loss: 1.1813814151287079
  batch 150 loss: 1.126081520318985
  batch 200 loss: 1.156223884820938
  batch 250 loss: 1.1905001926422119
  batch 300 loss: 1.1789836966991425
  batch 350 loss: 1.108785560131073
  batch 400 loss: 1.1722061383724212
  batch 450 loss: 1.1352529895305634
  batch 500 loss: 1.1203931307792663
  batch 550 loss: 1.1417939054965973
  batch 600 loss: 1.1590771234035493
  batch 650 loss: 1.1492869865894317
  batch 700 loss: 1.1550487756729126
  batch 750 loss: 1.1161739861965179
  batch 800 loss: 1.0839262592792511
  batch 850 loss: 1.1207294905185698
  batch 900 loss: 1.163371467590332
LOSS train 1.16337 valid 1.16366, valid PER 35.26%
EPOCH 5:
  batch 50 loss: 1.0702362525463105
  batch 100 loss: 1.0622089207172394
  batch 150 loss: 1.128202613592148
  batch 200 loss: 1.052374941110611
  batch 250 loss: 1.0542280292510986
  batch 300 loss: 1.0568712449073792
  batch 350 loss: 1.077555958032608
  batch 400 loss: 1.0762047827243806
  batch 450 loss: 1.0659324741363525
  batch 500 loss: 1.0968840432167053
  batch 550 loss: 1.0487959921360015
  batch 600 loss: 1.1257098948955535
  batch 650 loss: 1.0844165062904358
  batch 700 loss: 1.1223040580749513
  batch 750 loss: 1.044954229593277
  batch 800 loss: 1.0751940965652467
  batch 850 loss: 1.0732050263881683
  batch 900 loss: 1.0686041152477264
