Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.01, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.3470496702194215
  batch 100 loss: 2.858823285102844
  batch 150 loss: 2.4858498859405516
  batch 200 loss: 2.2625669169425966
  batch 250 loss: 2.1199458694458007
  batch 300 loss: 1.9935922884941102
  batch 350 loss: 1.8840070819854737
  batch 400 loss: 1.8854168486595153
  batch 450 loss: 1.8055125999450683
  batch 500 loss: 1.7369621467590333
  batch 550 loss: 1.681798357963562
  batch 600 loss: 1.6781461453437805
  batch 650 loss: 1.5667613458633423
  batch 700 loss: 1.5850433897972107
  batch 750 loss: 1.5496288561820983
  batch 800 loss: 1.5353136348724366
  batch 850 loss: 1.501384952068329
  batch 900 loss: 1.4777170157432555
LOSS train 1.47772 valid 1.47205, valid PER 46.66%
EPOCH 2:
  batch 50 loss: 1.4434292757511138
  batch 100 loss: 1.420512282848358
  batch 150 loss: 1.3953817701339721
  batch 200 loss: 1.4503949546813966
  batch 250 loss: 1.433541555404663
  batch 300 loss: 1.4204373025894166
  batch 350 loss: 1.3570526885986327
  batch 400 loss: 1.3886346888542176
  batch 450 loss: 1.3466488862037658
  batch 500 loss: 1.3982006788253785
  batch 550 loss: 1.4294222569465638
  batch 600 loss: 1.3476669788360596
  batch 650 loss: 1.3712751626968385
  batch 700 loss: 1.3452326774597168
  batch 750 loss: 1.333988962173462
  batch 800 loss: 1.3316511487960816
  batch 850 loss: 1.3341701328754425
  batch 900 loss: 1.364996681213379
LOSS train 1.36500 valid 1.33093, valid PER 42.27%
EPOCH 3:
  batch 50 loss: 1.3174569201469422
  batch 100 loss: 1.2645757913589477
  batch 150 loss: 1.2526723456382751
  batch 200 loss: 1.2659630751609803
  batch 250 loss: 1.282949012517929
  batch 300 loss: 1.2386031067371368
  batch 350 loss: 1.3131092548370362
  batch 400 loss: 1.2800223267078399
  batch 450 loss: 1.2809266757965088
  batch 500 loss: 1.2516911554336547
  batch 550 loss: 1.2919146394729615
  batch 600 loss: 1.2830706906318665
  batch 650 loss: 1.2672918248176575
  batch 700 loss: 1.239347698688507
  batch 750 loss: 1.3142010521888734
  batch 800 loss: 1.2682723951339723
  batch 850 loss: 1.3304049229621888
  batch 900 loss: 1.267568016052246
LOSS train 1.26757 valid 1.31914, valid PER 40.71%
EPOCH 4:
  batch 50 loss: 1.216251859664917
  batch 100 loss: 1.2276122379302978
  batch 150 loss: 1.2044872665405273
  batch 200 loss: 1.2383317124843598
  batch 250 loss: 1.2602979362010955
  batch 300 loss: 1.2704549288749696
  batch 350 loss: 1.1884472572803497
  batch 400 loss: 1.2349389719963073
  batch 450 loss: 1.2060422146320342
  batch 500 loss: 1.2026462471485138
  batch 550 loss: 1.2649277186393737
  batch 600 loss: 1.2955376482009888
  batch 650 loss: 1.2380887281894684
  batch 700 loss: 1.2280575716495514
  batch 750 loss: 1.2021886754035949
  batch 800 loss: 1.1732117199897767
  batch 850 loss: 1.2305263912677764
  batch 900 loss: 1.2328312909603119
LOSS train 1.23283 valid 1.22606, valid PER 37.77%
EPOCH 5:
  batch 50 loss: 1.1655909037590027
  batch 100 loss: 1.1691602075099945
  batch 150 loss: 1.1972399044036866
  batch 200 loss: 1.1219008445739747
  batch 250 loss: 1.1390591037273408
  batch 300 loss: 1.165705533027649
  batch 350 loss: 1.1997756659984589
  batch 400 loss: 1.2079124701023103
  batch 450 loss: 1.199622858762741
  batch 500 loss: 1.2140077018737794
  batch 550 loss: 1.1331671810150146
  batch 600 loss: 1.2041383516788482
  batch 650 loss: 1.2779425060749054
  batch 700 loss: 1.5510016107559204
  batch 750 loss: 1.64936194896698
  batch 800 loss: 1.466748869419098
  batch 850 loss: 1.453310034275055
  batch 900 loss: 1.4094909811019898
LOSS train 1.40949 valid 1.41999, valid PER 43.08%
EPOCH 6:
  batch 50 loss: 1.3720738124847411
  batch 100 loss: 1.3271636366844177
  batch 150 loss: 1.2846197962760926
  batch 200 loss: 1.312401852607727
  batch 250 loss: 1.3300439858436583
  batch 300 loss: 1.309012552499771
  batch 350 loss: 1.3089888620376586
  batch 400 loss: 1.2882560324668884
  batch 450 loss: 1.326381222009659
  batch 500 loss: 1.3017858028411866
  batch 550 loss: 1.3091264510154723
  batch 600 loss: 1.3008089411258696
  batch 650 loss: 1.293566451072693
  batch 700 loss: 1.2697013807296753
  batch 750 loss: 1.282048946619034
  batch 800 loss: 1.259007830619812
  batch 850 loss: 1.2627883756160736
  batch 900 loss: 1.284513771533966
LOSS train 1.28451 valid 1.32168, valid PER 41.21%
EPOCH 7:
  batch 50 loss: 1.2322356009483337
  batch 100 loss: 1.239343593120575
  batch 150 loss: 1.2175440895557403
  batch 200 loss: 1.218669250011444
  batch 250 loss: 1.2566401910781861
  batch 300 loss: 1.244740755558014
  batch 350 loss: 1.2331886672973633
  batch 400 loss: 1.2336407911777496
  batch 450 loss: 1.225257247686386
  batch 500 loss: 1.2147687256336213
  batch 550 loss: 1.2208516156673432
  batch 600 loss: 1.2222916853427888
  batch 650 loss: 1.225224882364273
  batch 700 loss: 1.1880173921585082
  batch 750 loss: 1.1928865265846254
  batch 800 loss: 1.183625670671463
  batch 850 loss: 1.2185317623615264
  batch 900 loss: 1.2419927096366883
LOSS train 1.24199 valid 1.27237, valid PER 39.72%
EPOCH 8:
  batch 50 loss: 1.2873132276535033
  batch 100 loss: 1.225099768638611
  batch 150 loss: 1.204032051563263
  batch 200 loss: 1.148488657474518
  batch 250 loss: 1.2351029372215272
  batch 300 loss: 1.1321283960342408
  batch 350 loss: 1.1881605231761931
  batch 400 loss: 1.1589486753940583
  batch 450 loss: 1.171126778125763
  batch 500 loss: 1.2207157361507415
  batch 550 loss: 1.1773472452163696
  batch 600 loss: 1.1975628459453582
  batch 650 loss: 1.2492965376377105
  batch 700 loss: 1.197872976064682
  batch 750 loss: 1.2064344716072082
  batch 800 loss: 1.1961578917503357
  batch 850 loss: 1.1945780777931214
  batch 900 loss: 1.179304324388504
LOSS train 1.17930 valid 1.22303, valid PER 37.77%
EPOCH 9:
  batch 50 loss: 1.0894552433490754
  batch 100 loss: 1.1604016518592835
  batch 150 loss: 1.1105840921401977
  batch 200 loss: 1.0968128514289857
  batch 250 loss: 1.153907926082611
  batch 300 loss: 1.149434082508087
  batch 350 loss: 1.1589110052585603
  batch 400 loss: 1.1618877172470092
  batch 450 loss: 1.1509117996692657
  batch 500 loss: 1.1142279660701753
  batch 550 loss: 1.1499120354652406
  batch 600 loss: 1.1772455632686616
  batch 650 loss: 1.1209865272045136
  batch 700 loss: 1.1278630602359772
  batch 750 loss: 1.1989094710350037
  batch 800 loss: 1.1713223004341125
  batch 850 loss: 1.1723876583576203
  batch 900 loss: 1.1218727838993072
LOSS train 1.12187 valid 1.22067, valid PER 38.03%
EPOCH 10:
  batch 50 loss: 1.0762097001075746
  batch 100 loss: 1.139841333627701
  batch 150 loss: 1.141659208536148
  batch 200 loss: 1.1619229781627656
  batch 250 loss: 1.1334504389762878
  batch 300 loss: 1.0855666708946228
  batch 350 loss: 1.1544057369232177
  batch 400 loss: 1.1301777064800262
  batch 450 loss: 1.107661590576172
  batch 500 loss: 1.1834097754955293
  batch 550 loss: 1.2002941024303437
  batch 600 loss: 1.172508542537689
  batch 650 loss: 1.1584932327270507
  batch 700 loss: 1.1812352597713471
  batch 750 loss: 1.1221911013126373
  batch 800 loss: 1.1564884495735168
  batch 850 loss: 1.207597861289978
  batch 900 loss: 1.1893876588344574
LOSS train 1.18939 valid 1.27208, valid PER 39.00%
EPOCH 11:
  batch 50 loss: 1.1408353447914124
  batch 100 loss: 1.1189546811580657
  batch 150 loss: 1.112510746717453
  batch 200 loss: 1.156790177822113
  batch 250 loss: 1.1486369359493256
  batch 300 loss: 1.1114591538906098
  batch 350 loss: 1.119809376001358
  batch 400 loss: 1.109018211364746
  batch 450 loss: 1.1602878415584563
  batch 500 loss: 1.1199646949768067
  batch 550 loss: 1.123910541534424
  batch 600 loss: 1.1327896010875702
  batch 650 loss: 1.1820196497440338
  batch 700 loss: 1.0701285672187806
  batch 750 loss: 1.0890269076824188
  batch 800 loss: 1.137853833436966
  batch 850 loss: 1.1847565615177154
  batch 900 loss: 1.149311351776123
LOSS train 1.14931 valid 1.20335, valid PER 36.58%
EPOCH 12:
  batch 50 loss: 1.114422289133072
  batch 100 loss: 1.0893590366840362
  batch 150 loss: 1.0527865445613862
  batch 200 loss: 1.0555624043941498
  batch 250 loss: 1.095069898366928
  batch 300 loss: 1.0926062488555908
  batch 350 loss: 1.0931005454063416
  batch 400 loss: 1.1292939829826354
  batch 450 loss: 1.1176021945476533
  batch 500 loss: 1.153718363046646
  batch 550 loss: 1.049722799062729
  batch 600 loss: 1.075473426580429
  batch 650 loss: 1.122209243774414
  batch 700 loss: 1.1106047308444977
  batch 750 loss: 1.1307566308975219
  batch 800 loss: 1.095490381717682
  batch 850 loss: 1.126556408405304
  batch 900 loss: 1.1161765718460084
LOSS train 1.11618 valid 1.17725, valid PER 37.06%
EPOCH 13:
  batch 50 loss: 1.047670065164566
  batch 100 loss: 1.0833434629440308
  batch 150 loss: 1.0344738817214967
  batch 200 loss: 1.0578915750980378
  batch 250 loss: 1.0903078520298004
  batch 300 loss: 1.0834759545326234
  batch 350 loss: 1.113590168952942
  batch 400 loss: 1.14807519197464
  batch 450 loss: 1.1368028342723846
  batch 500 loss: 1.071768569946289
  batch 550 loss: 1.1131276381015778
  batch 600 loss: 1.0844136941432954
  batch 650 loss: 1.125028191804886
  batch 700 loss: 1.1070172238349913
  batch 750 loss: 1.0778609585762025
  batch 800 loss: 1.1431119525432587
  batch 850 loss: 1.1474952328205108
  batch 900 loss: 1.1171784484386444
LOSS train 1.11718 valid 1.20133, valid PER 37.03%
EPOCH 14:
  batch 50 loss: 1.0409850943088532
  batch 100 loss: 1.0963820946216583
  batch 150 loss: 1.088056844472885
  batch 200 loss: 1.0720358848571778
  batch 250 loss: 1.0556314873695374
  batch 300 loss: 1.115965905189514
  batch 350 loss: 1.0511243963241577
  batch 400 loss: 1.0712609899044037
  batch 450 loss: 1.083339867591858
  batch 500 loss: 1.0874760150909424
  batch 550 loss: 1.1180650866031647
  batch 600 loss: 1.0741782212257385
  batch 650 loss: 1.1337158870697022
  batch 700 loss: 1.1241186368465423
  batch 750 loss: 1.0915223145484925
  batch 800 loss: 1.050208076238632
  batch 850 loss: 1.123849937915802
  batch 900 loss: 1.0981244111061097
LOSS train 1.09812 valid 1.19172, valid PER 36.01%
EPOCH 15:
  batch 50 loss: 1.0641006112098694
  batch 100 loss: 1.0685710847377776
  batch 150 loss: 1.0486488044261932
  batch 200 loss: 1.0820613300800324
  batch 250 loss: 1.073633326292038
  batch 300 loss: 1.0635797238349916
  batch 350 loss: 1.0687606739997864
  batch 400 loss: 1.0718075835704803
  batch 450 loss: 1.0814663612842559
  batch 500 loss: 1.0815215826034545
  batch 550 loss: 1.086755896806717
  batch 600 loss: 1.1138921093940735
  batch 650 loss: 1.1337111902236938
  batch 700 loss: 1.1050226271152497
  batch 750 loss: 1.099241886138916
  batch 800 loss: 1.0862115287780763
  batch 850 loss: 1.0792251801490784
  batch 900 loss: 1.1275150060653687
LOSS train 1.12752 valid 1.22632, valid PER 37.08%
EPOCH 16:
  batch 50 loss: 1.1466117012500763
  batch 100 loss: 1.0434009444713592
  batch 150 loss: 1.0543741047382356
  batch 200 loss: 1.0804840970039367
  batch 250 loss: 1.0997568595409393
  batch 300 loss: 1.0662675881385804
  batch 350 loss: 1.0833389472961426
  batch 400 loss: 1.0877732968330383
  batch 450 loss: 1.116039788722992
  batch 500 loss: 1.0298535633087158
  batch 550 loss: 1.0723426449298858
  batch 600 loss: 1.072198941707611
  batch 650 loss: 1.0666266560554505
  batch 700 loss: 1.062249481678009
  batch 750 loss: 1.0885450184345244
  batch 800 loss: 1.0987853121757507
  batch 850 loss: 1.0987015116214751
  batch 900 loss: 1.0772116410732269
LOSS train 1.07721 valid 1.18530, valid PER 36.22%
EPOCH 17:
  batch 50 loss: 1.0681595408916473
  batch 100 loss: 1.0701805233955384
  batch 150 loss: 1.0207501661777496
  batch 200 loss: 1.0378549551963807
  batch 250 loss: 1.085893372297287
  batch 300 loss: 1.1419431710243224
  batch 350 loss: 1.0830902814865113
  batch 400 loss: 1.1188001096248628
  batch 450 loss: 1.1093339669704436
  batch 500 loss: 1.0725965666770936
  batch 550 loss: 1.0884334135055542
  batch 600 loss: 1.14290109872818
  batch 650 loss: 1.0477269279956818
  batch 700 loss: 1.0618256652355194
  batch 750 loss: 1.0612426233291625
  batch 800 loss: 1.0545427525043487
  batch 850 loss: 1.0549699401855468
  batch 900 loss: 1.05194886803627
LOSS train 1.05195 valid 1.18270, valid PER 35.35%
EPOCH 18:
  batch 50 loss: 1.055699691772461
  batch 100 loss: 1.073040269613266
  batch 150 loss: 1.0611270236968995
  batch 200 loss: 1.0807270097732544
  batch 250 loss: 1.082583236694336
  batch 300 loss: 1.0683108162879944
  batch 350 loss: 1.0963854336738585
  batch 400 loss: 1.0208693599700929
  batch 450 loss: 1.0957603800296782
  batch 500 loss: 1.0499752295017242
  batch 550 loss: 1.0652910602092742
  batch 600 loss: 1.0670468235015869
  batch 650 loss: 1.0464671218395234
  batch 700 loss: 1.0868506145477295
  batch 750 loss: 1.0277751648426057
  batch 800 loss: 1.0284156036376952
  batch 850 loss: 1.0458044123649597
  batch 900 loss: 1.1081860756874085
LOSS train 1.10819 valid 1.21087, valid PER 36.46%
EPOCH 19:
  batch 50 loss: 1.0347083079814912
  batch 100 loss: 1.0826602983474731
  batch 150 loss: 1.055692721605301
  batch 200 loss: 1.0696571362018585
  batch 250 loss: 1.0773420977592467
  batch 300 loss: 1.094178900718689
  batch 350 loss: 1.0614576733112335
  batch 400 loss: 1.0598551833629608
  batch 450 loss: 1.086257017850876
  batch 500 loss: 1.0758844852447509
  batch 550 loss: 1.032604864835739
  batch 600 loss: 1.0618681585788727
  batch 650 loss: 1.1190356647968291
  batch 700 loss: 1.0647832036018372
  batch 750 loss: 1.0754783296585082
  batch 800 loss: 1.1038608753681183
  batch 850 loss: 1.1165619659423829
  batch 900 loss: 1.1037186908721923
LOSS train 1.10372 valid 1.18715, valid PER 36.39%
EPOCH 20:
  batch 50 loss: 1.0618847584724427
  batch 100 loss: 1.094961267709732
  batch 150 loss: 1.0644270694255829
  batch 200 loss: 1.090108244419098
  batch 250 loss: 1.0638547897338868
  batch 300 loss: 1.0979181051254272
  batch 350 loss: 1.0748993849754334
  batch 400 loss: 1.0737543821334838
  batch 450 loss: 1.0749343073368072
  batch 500 loss: 1.0714911770820619
  batch 550 loss: 1.1203366720676422
  batch 600 loss: 1.062896386384964
  batch 650 loss: 1.0661000943183898
  batch 700 loss: 1.1068161952495574
  batch 750 loss: 1.098875995874405
  batch 800 loss: 1.1175521790981293
  batch 850 loss: 1.076078304052353
  batch 900 loss: 1.1109022569656373
LOSS train 1.11090 valid 1.19051, valid PER 35.90%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230117_220145/model_12
Loading model from checkpoints/20230117_220145/model_12
SUB: 20.58%, DEL: 16.50%, INS: 1.42%, COR: 62.92%, PER: 38.50%
