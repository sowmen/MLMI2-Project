Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 8.953179583549499
  batch 100 loss: 3.186504988670349
  batch 150 loss: 3.091751465797424
  batch 200 loss: 3.0168603086471557
  batch 250 loss: 2.9684641933441163
  batch 300 loss: 2.847211465835571
  batch 350 loss: 2.7330988836288452
  batch 400 loss: 2.6746996355056765
  batch 450 loss: 2.5987943601608277
  batch 500 loss: 2.462528762817383
  batch 550 loss: 2.3654325008392334
  batch 600 loss: 2.2933440828323364
  batch 650 loss: 2.2158341121673586
  batch 700 loss: 2.1739149689674377
  batch 750 loss: 2.095051290988922
  batch 800 loss: 2.062231571674347
  batch 850 loss: 2.006864788532257
  batch 900 loss: 1.9650423836708069
LOSS train 1.96504 valid 1.93016, valid PER 73.32%
EPOCH 2:
  batch 50 loss: 1.9082206583023071
  batch 100 loss: 1.8679647898674012
  batch 150 loss: 1.8227230381965638
  batch 200 loss: 1.8112277436256408
  batch 250 loss: 1.8059561920166016
  batch 300 loss: 1.7783035802841187
  batch 350 loss: 1.6849909687042237
  batch 400 loss: 1.6923203778266906
  batch 450 loss: 1.6605205130577088
  batch 500 loss: 1.6641053748130799
  batch 550 loss: 1.6803794622421264
  batch 600 loss: 1.6119754910469055
  batch 650 loss: 1.6348391604423522
  batch 700 loss: 1.6245797634124757
  batch 750 loss: 1.5859288883209228
  batch 800 loss: 1.5206860542297362
  batch 850 loss: 1.5221046948432921
  batch 900 loss: 1.5417379450798034
LOSS train 1.54174 valid 1.53160, valid PER 54.35%
EPOCH 3:
  batch 50 loss: 1.5052064704895018
  batch 100 loss: 1.4654464101791382
  batch 150 loss: 1.457318398952484
  batch 200 loss: 1.4359650826454162
  batch 250 loss: 1.4238021206855773
  batch 300 loss: 1.39951420545578
  batch 350 loss: 1.4451279401779176
  batch 400 loss: 1.4011575031280517
  batch 450 loss: 1.3776863837242126
  batch 500 loss: 1.3691796278953552
  batch 550 loss: 1.3724247336387634
  batch 600 loss: 1.350559206008911
  batch 650 loss: 1.3253863883018493
  batch 700 loss: 1.337618169784546
  batch 750 loss: 1.3980318021774292
  batch 800 loss: 1.3349247908592223
  batch 850 loss: 1.3449370002746581
  batch 900 loss: 1.2901006245613098
LOSS train 1.29010 valid 1.34267, valid PER 42.52%
EPOCH 4:
  batch 50 loss: 1.2811230635643005
  batch 100 loss: 1.2667473697662353
  batch 150 loss: 1.2346560406684874
  batch 200 loss: 1.2629592680931092
  batch 250 loss: 1.2675904548168182
  batch 300 loss: 1.2727213048934936
  batch 350 loss: 1.2059115183353424
  batch 400 loss: 1.2367788815498353
  batch 450 loss: 1.2398214614391327
  batch 500 loss: 1.2316814768314361
  batch 550 loss: 1.2444595992565155
  batch 600 loss: 1.2511791718006133
  batch 650 loss: 1.2385607862472534
  batch 700 loss: 1.2081611061096191
  batch 750 loss: 1.180836979150772
  batch 800 loss: 1.1511616897583008
  batch 850 loss: 1.2133790516853333
  batch 900 loss: 1.2298889398574828
LOSS train 1.22989 valid 1.20132, valid PER 38.15%
EPOCH 5:
  batch 50 loss: 1.1425316107273102
  batch 100 loss: 1.1522881317138671
  batch 150 loss: 1.2016001808643342
  batch 200 loss: 1.1205707597732544
  batch 250 loss: 1.1397154653072357
  batch 300 loss: 1.148913526535034
  batch 350 loss: 1.1502872812747955
  batch 400 loss: 1.1530114150047301
  batch 450 loss: 1.1333655416965485
  batch 500 loss: 1.164535439014435
  batch 550 loss: 1.1093961572647095
  batch 600 loss: 1.1767661666870117
  batch 650 loss: 1.1340676355361938
  batch 700 loss: 1.154877690076828
  batch 750 loss: 1.0926513862609863
  batch 800 loss: 1.1247351360321045
  batch 850 loss: 1.1269640219211579
  batch 900 loss: 1.136978613138199
LOSS train 1.13698 valid 1.19484, valid PER 37.20%
EPOCH 6:
  batch 50 loss: 1.1333438634872437
  batch 100 loss: 1.0648895907402038
  batch 150 loss: 1.0382909905910491
  batch 200 loss: 1.0881088948249817
  batch 250 loss: 1.0973151576519014
  batch 300 loss: 1.07279904961586
  batch 350 loss: 1.0878300499916076
  batch 400 loss: 1.056241385936737
  batch 450 loss: 1.1050553262233733
  batch 500 loss: 1.0821334791183472
  batch 550 loss: 1.0812919700145722
  batch 600 loss: 1.0421186888217926
  batch 650 loss: 1.0755643022060395
  batch 700 loss: 1.0675879955291747
  batch 750 loss: 1.055374355316162
  batch 800 loss: 1.0415267145633698
  batch 850 loss: 1.0392006647586822
  batch 900 loss: 1.0588647270202636
LOSS train 1.05886 valid 1.10159, valid PER 35.18%
EPOCH 7:
  batch 50 loss: 1.0214173018932342
  batch 100 loss: 1.0388789427280427
  batch 150 loss: 1.030877901315689
  batch 200 loss: 1.002542209625244
  batch 250 loss: 1.0233693385124207
  batch 300 loss: 1.0131967997550964
  batch 350 loss: 1.0120608294010163
  batch 400 loss: 1.028673859834671
  batch 450 loss: 1.0075994145870208
  batch 500 loss: 1.0061615908145904
  batch 550 loss: 1.0051406228542328
  batch 600 loss: 1.0388466823101044
  batch 650 loss: 0.9930678820610046
  batch 700 loss: 1.028000293970108
  batch 750 loss: 0.9998290300369262
  batch 800 loss: 1.0082033705711364
  batch 850 loss: 1.0255725932121278
  batch 900 loss: 1.0759527432918548
LOSS train 1.07595 valid 1.07484, valid PER 34.76%
EPOCH 8:
  batch 50 loss: 0.9850471067428589
  batch 100 loss: 0.9788237166404724
  batch 150 loss: 0.9772804975509644
  batch 200 loss: 0.9555250597000122
  batch 250 loss: 0.9791262686252594
  batch 300 loss: 0.9342596840858459
  batch 350 loss: 0.9960400235652923
  batch 400 loss: 0.9601111018657684
  batch 450 loss: 1.0017564499378204
  batch 500 loss: 1.012509171962738
  batch 550 loss: 0.9521394050121308
  batch 600 loss: 1.0032756984233857
  batch 650 loss: 1.011925879716873
  batch 700 loss: 0.9690488207340241
  batch 750 loss: 0.9946238791942597
  batch 800 loss: 0.9832305920124054
  batch 850 loss: 0.9586786615848542
  batch 900 loss: 0.9447709846496583
LOSS train 0.94477 valid 1.03621, valid PER 32.68%
EPOCH 9:
  batch 50 loss: 0.9276677787303924
  batch 100 loss: 0.9578160834312439
  batch 150 loss: 0.9507398676872253
  batch 200 loss: 0.9105407786369324
  batch 250 loss: 0.9495649874210358
  batch 300 loss: 0.9546411108970642
  batch 350 loss: 0.9572542810440063
  batch 400 loss: 0.9485296082496643
  batch 450 loss: 0.956200715303421
  batch 500 loss: 0.9087319958209992
  batch 550 loss: 0.9586093533039093
  batch 600 loss: 0.9658180272579193
  batch 650 loss: 0.9231152653694152
  batch 700 loss: 0.9339187252521515
  batch 750 loss: 0.9123442959785462
  batch 800 loss: 0.9423765683174133
  batch 850 loss: 0.9400342094898224
  batch 900 loss: 0.9118558013439179
LOSS train 0.91186 valid 1.03558, valid PER 32.68%
EPOCH 10:
  batch 50 loss: 0.8947879421710968
  batch 100 loss: 0.8860719573497772
  batch 150 loss: 0.9253223586082459
  batch 200 loss: 0.9182283103466033
  batch 250 loss: 0.9243080401420594
  batch 300 loss: 0.8785943484306336
  batch 350 loss: 0.899530497789383
  batch 400 loss: 0.8710367846488952
  batch 450 loss: 0.8710781180858612
  batch 500 loss: 0.9181364679336548
  batch 550 loss: 0.9214958667755127
  batch 600 loss: 0.893068253993988
  batch 650 loss: 0.892754567861557
  batch 700 loss: 0.9155118298530579
  batch 750 loss: 0.9049506068229676
  batch 800 loss: 0.899497766494751
  batch 850 loss: 0.9154439353942871
  batch 900 loss: 0.9174914383888244
LOSS train 0.91749 valid 1.00738, valid PER 31.38%
EPOCH 11:
  batch 50 loss: 0.8590534353256225
  batch 100 loss: 0.8294590044021607
  batch 150 loss: 0.8433242273330689
  batch 200 loss: 0.8955342543125152
  batch 250 loss: 0.8909377920627594
  batch 300 loss: 0.8615407240390778
  batch 350 loss: 0.8725076031684875
  batch 400 loss: 0.9019152629375458
  batch 450 loss: 0.8735881447792053
  batch 500 loss: 0.8792032384872437
  batch 550 loss: 0.8615143597126007
  batch 600 loss: 0.8457844138145447
  batch 650 loss: 0.928030754327774
  batch 700 loss: 0.8343322014808655
  batch 750 loss: 0.8818938910961152
  batch 800 loss: 0.8934821248054504
  batch 850 loss: 0.9020515155792236
  batch 900 loss: 0.8934981417655945
LOSS train 0.89350 valid 0.98847, valid PER 30.62%
EPOCH 12:
  batch 50 loss: 0.8625042760372161
  batch 100 loss: 0.8327970516681671
  batch 150 loss: 0.8139778387546539
  batch 200 loss: 0.8369320446252823
  batch 250 loss: 0.8625614833831787
  batch 300 loss: 0.8338021290302277
  batch 350 loss: 0.8344230473041534
  batch 400 loss: 0.8422328639030456
  batch 450 loss: 0.845848149061203
  batch 500 loss: 0.8834750497341156
  batch 550 loss: 0.8035179388523102
  batch 600 loss: 0.8323753273487091
  batch 650 loss: 0.890170955657959
  batch 700 loss: 0.8655510306358337
  batch 750 loss: 0.8160623669624328
  batch 800 loss: 0.8506659144163131
  batch 850 loss: 0.894668493270874
  batch 900 loss: 0.8549293661117554
LOSS train 0.85493 valid 0.96799, valid PER 31.40%
EPOCH 13:
  batch 50 loss: 0.8049830532073975
  batch 100 loss: 0.8272671127319335
  batch 150 loss: 0.8052375745773316
  batch 200 loss: 0.8130142402648926
  batch 250 loss: 0.8112676072120667
  batch 300 loss: 0.820434975028038
  batch 350 loss: 0.7991611206531525
  batch 400 loss: 0.8153356409072876
  batch 450 loss: 0.8101866328716278
  batch 500 loss: 0.8055295014381408
  batch 550 loss: 0.840559264421463
  batch 600 loss: 0.8302538931369782
  batch 650 loss: 0.8371699059009552
  batch 700 loss: 0.8449549663066864
  batch 750 loss: 0.789486882686615
  batch 800 loss: 0.8105572617053985
  batch 850 loss: 0.8481771433353424
  batch 900 loss: 0.8493337976932526
LOSS train 0.84933 valid 0.94843, valid PER 30.22%
EPOCH 14:
  batch 50 loss: 0.7810643494129181
  batch 100 loss: 0.7827245873212815
  batch 150 loss: 0.7777123296260834
  batch 200 loss: 0.7906552958488464
  batch 250 loss: 0.8038129603862763
  batch 300 loss: 0.8459700107574463
  batch 350 loss: 0.7773427188396453
  batch 400 loss: 0.8065448546409607
  batch 450 loss: 0.79268390417099
  batch 500 loss: 0.8151897478103638
  batch 550 loss: 0.8339649140834808
  batch 600 loss: 0.773241183757782
  batch 650 loss: 0.81501669049263
  batch 700 loss: 0.8153096354007721
  batch 750 loss: 0.7797691893577575
  batch 800 loss: 0.772668788433075
  batch 850 loss: 0.8384778094291687
  batch 900 loss: 0.8132525336742401
LOSS train 0.81325 valid 0.95111, valid PER 30.20%
EPOCH 15:
  batch 50 loss: 0.7661764019727707
  batch 100 loss: 0.7593661969900132
  batch 150 loss: 0.7886045920848846
  batch 200 loss: 0.8149904119968414
  batch 250 loss: 0.7910830414295197
  batch 300 loss: 0.765787997841835
  batch 350 loss: 0.765309522151947
  batch 400 loss: 0.7654563760757447
  batch 450 loss: 0.777429050207138
  batch 500 loss: 0.744881136417389
  batch 550 loss: 0.7831030678749085
  batch 600 loss: 0.8007326638698578
  batch 650 loss: 0.7984336316585541
  batch 700 loss: 0.7906868839263916
  batch 750 loss: 0.7984856796264649
  batch 800 loss: 0.7838612937927246
  batch 850 loss: 0.7673232877254486
  batch 900 loss: 0.804138759970665
LOSS train 0.80414 valid 0.94760, valid PER 29.65%
EPOCH 16:
  batch 50 loss: 0.7730362784862518
  batch 100 loss: 0.7398356145620346
  batch 150 loss: 0.7558717012405396
  batch 200 loss: 0.7564176118373871
  batch 250 loss: 0.7624542939662934
  batch 300 loss: 0.7641906869411469
  batch 350 loss: 0.7660144364833832
  batch 400 loss: 0.7741743379831314
  batch 450 loss: 0.7740370762348175
  batch 500 loss: 0.7343283355236053
  batch 550 loss: 0.7712804961204529
  batch 600 loss: 0.7485158932209015
  batch 650 loss: 0.7813466584682465
  batch 700 loss: 0.7633519279956817
  batch 750 loss: 0.758732293844223
  batch 800 loss: 0.7840610682964325
  batch 850 loss: 0.76017418384552
  batch 900 loss: 0.7582484877109528
LOSS train 0.75825 valid 0.94578, valid PER 29.64%
EPOCH 17:
  batch 50 loss: 0.7358969140052796
  batch 100 loss: 0.7337538534402848
  batch 150 loss: 0.7261704778671265
  batch 200 loss: 0.7190558564662933
  batch 250 loss: 0.7423480832576752
  batch 300 loss: 0.7369382125139237
  batch 350 loss: 0.7049486827850342
  batch 400 loss: 0.7646138274669647
  batch 450 loss: 0.7682227659225463
  batch 500 loss: 0.7290755701065064
  batch 550 loss: 0.7548627305030823
  batch 600 loss: 0.8728978049755096
  batch 650 loss: 0.7932324099540711
  batch 700 loss: 0.7622299265861511
  batch 750 loss: 0.751362909078598
  batch 800 loss: 0.7562479639053344
  batch 850 loss: 0.7453285706043243
  batch 900 loss: 0.7312516224384308
LOSS train 0.73125 valid 0.92854, valid PER 28.90%
EPOCH 18:
  batch 50 loss: 0.7121864640712738
  batch 100 loss: 0.7306946247816086
  batch 150 loss: 0.7343332076072693
  batch 200 loss: 0.7224460214376449
  batch 250 loss: 0.7327938652038575
  batch 300 loss: 0.6998250424861908
  batch 350 loss: 0.711616369485855
  batch 400 loss: 0.6909888935089111
  batch 450 loss: 0.7349756455421448
  batch 500 loss: 0.7137656956911087
  batch 550 loss: 0.735615668296814
  batch 600 loss: 0.7147477465867996
  batch 650 loss: 0.7119605964422226
  batch 700 loss: 0.749504337310791
  batch 750 loss: 0.7309073400497437
  batch 800 loss: 0.7305007994174957
  batch 850 loss: 0.7302581930160522
  batch 900 loss: 0.7478904759883881
LOSS train 0.74789 valid 0.92601, valid PER 28.94%
EPOCH 19:
  batch 50 loss: 0.6843178939819335
  batch 100 loss: 0.6553671157360077
  batch 150 loss: 0.6778570610284805
  batch 200 loss: 0.6992770624160767
  batch 250 loss: 0.7224451470375061
  batch 300 loss: 0.7162325900793075
  batch 350 loss: 0.697564457654953
  batch 400 loss: 0.7043340009450912
  batch 450 loss: 0.7148021131753921
  batch 500 loss: 0.7274702417850495
  batch 550 loss: 0.6950988638401031
  batch 600 loss: 0.7010890436172486
  batch 650 loss: 0.742076290845871
  batch 700 loss: 0.6906474244594574
  batch 750 loss: 0.6913280725479126
  batch 800 loss: 0.7309628748893737
  batch 850 loss: 0.7249000602960587
  batch 900 loss: 0.6859082567691803
LOSS train 0.68591 valid 0.92377, valid PER 28.17%
EPOCH 20:
  batch 50 loss: 0.6672521728277206
  batch 100 loss: 0.6742187714576722
  batch 150 loss: 0.6597984087467194
  batch 200 loss: 0.6637644463777542
  batch 250 loss: 0.6748540103435516
  batch 300 loss: 0.711737676858902
  batch 350 loss: 0.6801437824964524
  batch 400 loss: 0.68997274518013
  batch 450 loss: 0.6696078753471375
  batch 500 loss: 0.6550558459758759
  batch 550 loss: 0.7308964610099793
  batch 600 loss: 0.680056534409523
  batch 650 loss: 0.709798879623413
  batch 700 loss: 0.7017550051212311
  batch 750 loss: 0.7065562283992768
  batch 800 loss: 0.7319252574443817
  batch 850 loss: 0.7225586724281311
  batch 900 loss: 0.7060679405927658
LOSS train 0.70607 valid 0.91703, valid PER 28.23%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_220619/model_20
Loading model from checkpoints/20230117_220619/model_20
SUB: 16.25%, DEL: 11.50%, INS: 2.31%, COR: 72.25%, PER: 30.06%
