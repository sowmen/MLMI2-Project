Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.0001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 24.711413764953612
  batch 100 loss: 18.56735761642456
  batch 150 loss: 6.115986342430115
  batch 200 loss: 3.5875825691223144
  batch 250 loss: 3.492273726463318
  batch 300 loss: 3.37549458026886
  batch 350 loss: 3.3350398111343384
  batch 400 loss: 3.3371757316589354
  batch 450 loss: 3.328781180381775
  batch 500 loss: 3.3101300954818726
  batch 550 loss: 3.3033329820632935
  batch 600 loss: 3.2897872734069824
  batch 650 loss: 3.2895656776428224
  batch 700 loss: 3.305470895767212
  batch 750 loss: 3.288676586151123
  batch 800 loss: 3.2968387365341187
  batch 850 loss: 3.2962242794036865
  batch 900 loss: 3.275878081321716
LOSS train 3.27588 valid 3.29528, valid PER 100.00%
EPOCH 2:
  batch 50 loss: 3.2723243331909178
  batch 100 loss: 3.2655309867858886
  batch 150 loss: 3.2641660261154173
  batch 200 loss: 3.253533887863159
  batch 250 loss: 3.253529224395752
  batch 300 loss: 3.2508712005615235
  batch 350 loss: 3.238770771026611
  batch 400 loss: 3.2315093326568602
  batch 450 loss: 3.2329888582229613
  batch 500 loss: 3.222084231376648
  batch 550 loss: 3.218738331794739
  batch 600 loss: 3.2073272466659546
  batch 650 loss: 3.1989612007141113
  batch 700 loss: 3.1774397230148317
  batch 750 loss: 3.1884930181503295
  batch 800 loss: 3.1599763107299803
  batch 850 loss: 3.150290870666504
  batch 900 loss: 3.1412187957763673
LOSS train 3.14122 valid 3.12818, valid PER 97.33%
EPOCH 3:
  batch 50 loss: 3.115733723640442
  batch 100 loss: 3.08752236366272
  batch 150 loss: 3.0789603281021116
  batch 200 loss: 3.0450703477859498
  batch 250 loss: 3.0089334297180175
  batch 300 loss: 2.9903048133850096
  batch 350 loss: 2.9782315635681154
  batch 400 loss: 2.938572096824646
  batch 450 loss: 2.9004573106765745
  batch 500 loss: 2.8678317308425902
  batch 550 loss: 2.832192907333374
  batch 600 loss: 2.7842569017410277
  batch 650 loss: 2.7419023656845094
  batch 700 loss: 2.7039241933822633
  batch 750 loss: 2.7190542125701906
  batch 800 loss: 2.665978960990906
  batch 850 loss: 2.6411196851730345
  batch 900 loss: 2.612395849227905
LOSS train 2.61240 valid 2.60400, valid PER 85.06%
EPOCH 4:
  batch 50 loss: 2.589119381904602
  batch 100 loss: 2.5615461111068725
  batch 150 loss: 2.5038473176956177
  batch 200 loss: 2.5322600317001345
  batch 250 loss: 2.4852047634124754
  batch 300 loss: 2.4837508010864258
  batch 350 loss: 2.422613763809204
  batch 400 loss: 2.4330048608779906
  batch 450 loss: 2.4270428800582886
  batch 500 loss: 2.365461230278015
  batch 550 loss: 2.387640151977539
  batch 600 loss: 2.3750901889801024
  batch 650 loss: 2.358257155418396
  batch 700 loss: 2.335968942642212
  batch 750 loss: 2.293715867996216
  batch 800 loss: 2.272200651168823
  batch 850 loss: 2.27254204750061
  batch 900 loss: 2.28138569355011
LOSS train 2.28139 valid 2.26044, valid PER 79.08%
EPOCH 5:
  batch 50 loss: 2.24777569770813
  batch 100 loss: 2.210502164363861
  batch 150 loss: 2.222830891609192
  batch 200 loss: 2.192427701950073
  batch 250 loss: 2.1895529294013976
  batch 300 loss: 2.1803930521011354
  batch 350 loss: 2.1766801738739012
  batch 400 loss: 2.167585892677307
  batch 450 loss: 2.151567347049713
  batch 500 loss: 2.154353213310242
  batch 550 loss: 2.089093658924103
  batch 600 loss: 2.1293070220947268
  batch 650 loss: 2.1025011253356936
  batch 700 loss: 2.110141637325287
  batch 750 loss: 2.069503490924835
  batch 800 loss: 2.083121180534363
  batch 850 loss: 2.074614818096161
  batch 900 loss: 2.0743664860725404
LOSS train 2.07437 valid 2.06218, valid PER 74.54%
EPOCH 6:
  batch 50 loss: 2.0607307887077333
  batch 100 loss: 2.0374084591865538
  batch 150 loss: 1.9966268944740295
  batch 200 loss: 2.0083977031707763
  batch 250 loss: 2.0335665464401247
  batch 300 loss: 1.9738631749153137
  batch 350 loss: 2.000316972732544
  batch 400 loss: 1.9637752079963684
  batch 450 loss: 1.9976726746559144
  batch 500 loss: 1.962645766735077
  batch 550 loss: 1.9650296211242675
  batch 600 loss: 1.9389703059196473
  batch 650 loss: 1.955879683494568
  batch 700 loss: 1.9333381676673889
  batch 750 loss: 1.9246452641487122
  batch 800 loss: 1.9107015895843507
  batch 850 loss: 1.9153264355659485
  batch 900 loss: 1.913581154346466
LOSS train 1.91358 valid 1.91967, valid PER 69.75%
EPOCH 7:
  batch 50 loss: 1.9225769829750061
  batch 100 loss: 1.9112325859069825
  batch 150 loss: 1.8771826195716859
  batch 200 loss: 1.8658711647987365
  batch 250 loss: 1.8666231584548951
  batch 300 loss: 1.846971046924591
  batch 350 loss: 1.8473722004890443
  batch 400 loss: 1.860060348510742
  batch 450 loss: 1.8436751818656922
  batch 500 loss: 1.8484523677825928
  batch 550 loss: 1.8450750350952148
  batch 600 loss: 1.8448155570030211
  batch 650 loss: 1.8112398767471314
  batch 700 loss: 1.8255179142951965
  batch 750 loss: 1.7965232753753662
  batch 800 loss: 1.8007433581352235
  batch 850 loss: 1.8196240043640137
  batch 900 loss: 1.8271759128570557
LOSS train 1.82718 valid 1.82165, valid PER 66.47%
EPOCH 8:
  batch 50 loss: 1.79701425075531
  batch 100 loss: 1.7955833554267884
  batch 150 loss: 1.780880901813507
  batch 200 loss: 1.7494929790496827
  batch 250 loss: 1.7760916972160339
  batch 300 loss: 1.727263057231903
  batch 350 loss: 1.7673703026771546
  batch 400 loss: 1.7576802825927735
  batch 450 loss: 1.7704293632507324
  batch 500 loss: 1.7827914237976075
  batch 550 loss: 1.7220205950737
  batch 600 loss: 1.7640167450904847
  batch 650 loss: 1.7594773888587951
  batch 700 loss: 1.723775987625122
  batch 750 loss: 1.7274846053123474
  batch 800 loss: 1.7335996079444884
  batch 850 loss: 1.7390049576759339
  batch 900 loss: 1.7178234362602234
LOSS train 1.71782 valid 1.74013, valid PER 63.09%
EPOCH 9:
  batch 50 loss: 1.6729688882827758
  batch 100 loss: 1.7318931221961975
  batch 150 loss: 1.7026294326782228
  batch 200 loss: 1.6683546686172486
  batch 250 loss: 1.6967269921302794
  batch 300 loss: 1.6988586592674255
  batch 350 loss: 1.6953786301612854
  batch 400 loss: 1.6776317954063416
  batch 450 loss: 1.6757239699363708
  batch 500 loss: 1.6616870069503784
  batch 550 loss: 1.6743645071983337
  batch 600 loss: 1.6881847953796387
  batch 650 loss: 1.654559154510498
  batch 700 loss: 1.675235378742218
  batch 750 loss: 1.661715030670166
  batch 800 loss: 1.666791272163391
  batch 850 loss: 1.6854878401756286
  batch 900 loss: 1.6541238832473755
LOSS train 1.65412 valid 1.67162, valid PER 61.38%
EPOCH 10:
  batch 50 loss: 1.6316919136047363
  batch 100 loss: 1.6306770873069762
  batch 150 loss: 1.6448464608192443
  batch 200 loss: 1.650981743335724
  batch 250 loss: 1.6269031929969788
  batch 300 loss: 1.583553810119629
  batch 350 loss: 1.6117966747283936
  batch 400 loss: 1.6046009039878846
  batch 450 loss: 1.5724107551574706
  batch 500 loss: 1.6256833291053772
  batch 550 loss: 1.6240927624702453
  batch 600 loss: 1.6021710634231567
  batch 650 loss: 1.5777987575531005
  batch 700 loss: 1.5853373169898988
  batch 750 loss: 1.575153431892395
  batch 800 loss: 1.6033216762542724
  batch 850 loss: 1.581531264781952
  batch 900 loss: 1.6052586698532105
LOSS train 1.60526 valid 1.61025, valid PER 59.23%
EPOCH 11:
  batch 50 loss: 1.5557509231567384
  batch 100 loss: 1.580315442085266
  batch 150 loss: 1.5527592587471009
  batch 200 loss: 1.5822018480300903
  batch 250 loss: 1.5713125133514405
  batch 300 loss: 1.5373739719390869
  batch 350 loss: 1.5494108891487122
  batch 400 loss: 1.5676839518547059
  batch 450 loss: 1.546315152645111
  batch 500 loss: 1.5270956873893737
  batch 550 loss: 1.5472589492797852
  batch 600 loss: 1.540889446735382
  batch 650 loss: 1.589749915599823
  batch 700 loss: 1.5005949592590333
  batch 750 loss: 1.5245066499710083
  batch 800 loss: 1.5574229669570923
  batch 850 loss: 1.5619260382652282
  batch 900 loss: 1.5360558915138245
LOSS train 1.53606 valid 1.56109, valid PER 57.19%
EPOCH 12:
  batch 50 loss: 1.5319591307640075
  batch 100 loss: 1.517527117729187
  batch 150 loss: 1.501792278289795
  batch 200 loss: 1.5140670323371888
  batch 250 loss: 1.5424672341346741
  batch 300 loss: 1.5121752142906189
  batch 350 loss: 1.511870572566986
  batch 400 loss: 1.5341470956802368
  batch 450 loss: 1.5190430903434753
  batch 500 loss: 1.5226714849472045
  batch 550 loss: 1.443124852180481
  batch 600 loss: 1.472728145122528
  batch 650 loss: 1.5300300407409668
  batch 700 loss: 1.5012313365936278
  batch 750 loss: 1.4783967876434325
  batch 800 loss: 1.4631910872459413
  batch 850 loss: 1.5023791432380675
  batch 900 loss: 1.518772373199463
LOSS train 1.51877 valid 1.52855, valid PER 56.00%
EPOCH 13:
  batch 50 loss: 1.4670439171791076
  batch 100 loss: 1.4896894812583923
  batch 150 loss: 1.457460412979126
  batch 200 loss: 1.4703274321556092
  batch 250 loss: 1.4790660643577576
  batch 300 loss: 1.4620974278450012
  batch 350 loss: 1.4797186279296874
  batch 400 loss: 1.4786894774436952
  batch 450 loss: 1.4962308835983276
  batch 500 loss: 1.455032675266266
  batch 550 loss: 1.4565673613548278
  batch 600 loss: 1.460667498111725
  batch 650 loss: 1.4497195148468018
  batch 700 loss: 1.46698237657547
  batch 750 loss: 1.43996728181839
  batch 800 loss: 1.456251609325409
  batch 850 loss: 1.4800504922866822
  batch 900 loss: 1.4721584486961365
LOSS train 1.47216 valid 1.49301, valid PER 53.97%
EPOCH 14:
  batch 50 loss: 1.434071273803711
  batch 100 loss: 1.4600718760490417
  batch 150 loss: 1.4367494106292724
  batch 200 loss: 1.448228838443756
  batch 250 loss: 1.4191184163093566
  batch 300 loss: 1.4411018514633178
  batch 350 loss: 1.4058758306503296
  batch 400 loss: 1.4342345929145812
  batch 450 loss: 1.41364529132843
  batch 500 loss: 1.4440174508094787
  batch 550 loss: 1.4701563954353332
  batch 600 loss: 1.4182020711898804
  batch 650 loss: 1.461369388103485
  batch 700 loss: 1.4458981275558471
  batch 750 loss: 1.4064360737800599
  batch 800 loss: 1.3750137686729431
  batch 850 loss: 1.4420625805854796
  batch 900 loss: 1.4365091848373412
LOSS train 1.43651 valid 1.47264, valid PER 53.14%
EPOCH 15:
  batch 50 loss: 1.4255428957939147
  batch 100 loss: 1.397155900001526
  batch 150 loss: 1.3992848229408263
  batch 200 loss: 1.4251923227310181
  batch 250 loss: 1.4055435132980347
  batch 300 loss: 1.4082965230941773
  batch 350 loss: 1.4150203824043275
  batch 400 loss: 1.3926496148109435
  batch 450 loss: 1.404509744644165
  batch 500 loss: 1.3785121107101441
  batch 550 loss: 1.4126116752624511
  batch 600 loss: 1.4137644863128662
  batch 650 loss: 1.4338537096977233
  batch 700 loss: 1.4136001324653626
  batch 750 loss: 1.3976334476470946
  batch 800 loss: 1.3861295175552368
  batch 850 loss: 1.3706750202178954
  batch 900 loss: 1.393108127117157
LOSS train 1.39311 valid 1.43951, valid PER 50.89%
EPOCH 16:
  batch 50 loss: 1.4122542500495912
  batch 100 loss: 1.3436780023574828
  batch 150 loss: 1.377884316444397
  batch 200 loss: 1.3784087467193604
  batch 250 loss: 1.4138130974769592
  batch 300 loss: 1.3906023478507996
  batch 350 loss: 1.3822268104553224
  batch 400 loss: 1.381323103904724
  batch 450 loss: 1.400558819770813
  batch 500 loss: 1.3521570825576783
  batch 550 loss: 1.4169179463386536
  batch 600 loss: 1.377701780796051
  batch 650 loss: 1.3811498737335206
  batch 700 loss: 1.3536330199241637
  batch 750 loss: 1.3647205710411072
  batch 800 loss: 1.3750563621520997
  batch 850 loss: 1.3556250524520874
  batch 900 loss: 1.352528579235077
LOSS train 1.35253 valid 1.42100, valid PER 50.31%
EPOCH 17:
  batch 50 loss: 1.3630216646194457
  batch 100 loss: 1.3544009137153625
  batch 150 loss: 1.3363805651664733
  batch 200 loss: 1.3447486305236815
  batch 250 loss: 1.3706524896621703
  batch 300 loss: 1.347028465270996
  batch 350 loss: 1.3293142008781433
  batch 400 loss: 1.3950480794906617
  batch 450 loss: 1.3722616720199585
  batch 500 loss: 1.3366445064544679
  batch 550 loss: 1.360277945995331
  batch 600 loss: 1.408097095489502
  batch 650 loss: 1.334641194343567
  batch 700 loss: 1.3332953000068664
  batch 750 loss: 1.3283306312561036
  batch 800 loss: 1.3319055151939392
  batch 850 loss: 1.341069233417511
  batch 900 loss: 1.3277161836624145
LOSS train 1.32772 valid 1.40283, valid PER 48.00%
EPOCH 18:
  batch 50 loss: 1.3357533502578736
  batch 100 loss: 1.3539622449874877
  batch 150 loss: 1.3307259345054627
  batch 200 loss: 1.3348126530647277
  batch 250 loss: 1.3251302719116211
  batch 300 loss: 1.3258607602119445
  batch 350 loss: 1.3516887497901917
  batch 400 loss: 1.3222339487075805
  batch 450 loss: 1.3592983818054198
  batch 500 loss: 1.3372460317611694
  batch 550 loss: 1.3064642500877381
  batch 600 loss: 1.30661682844162
  batch 650 loss: 1.310525517463684
  batch 700 loss: 1.3507772159576417
  batch 750 loss: 1.309773564338684
  batch 800 loss: 1.31485515832901
  batch 850 loss: 1.3026520037651061
  batch 900 loss: 1.3531884503364564
LOSS train 1.35319 valid 1.37904, valid PER 47.11%
EPOCH 19:
  batch 50 loss: 1.2691701745986939
  batch 100 loss: 1.291103949546814
  batch 150 loss: 1.3166220784187317
  batch 200 loss: 1.3147202968597411
  batch 250 loss: 1.3308481621742247
  batch 300 loss: 1.3215677070617675
  batch 350 loss: 1.2986541056632996
  batch 400 loss: 1.3185272884368897
  batch 450 loss: 1.3200101089477538
  batch 500 loss: 1.3233164572715759
  batch 550 loss: 1.2997591948509217
  batch 600 loss: 1.3093360984325408
  batch 650 loss: 1.3501696181297302
  batch 700 loss: 1.2900060319900513
  batch 750 loss: 1.269730863571167
  batch 800 loss: 1.315136375427246
  batch 850 loss: 1.3190168142318726
  batch 900 loss: 1.287840075492859
LOSS train 1.28784 valid 1.36675, valid PER 45.05%
EPOCH 20:
  batch 50 loss: 1.2791324532032013
  batch 100 loss: 1.30364319562912
  batch 150 loss: 1.287338296175003
  batch 200 loss: 1.2911436200141906
  batch 250 loss: 1.3022347617149352
  batch 300 loss: 1.3048040628433228
  batch 350 loss: 1.264923439025879
  batch 400 loss: 1.2783290553092956
  batch 450 loss: 1.2776153886318207
  batch 500 loss: 1.2498543071746826
  batch 550 loss: 1.3356814539432527
  batch 600 loss: 1.2563525700569154
  batch 650 loss: 1.2994221544265747
  batch 700 loss: 1.2949763178825378
  batch 750 loss: 1.2832916224002837
  batch 800 loss: 1.3170523524284363
  batch 850 loss: 1.298488941192627
  batch 900 loss: 1.2921950364112853
LOSS train 1.29220 valid 1.34288, valid PER 46.27%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_221131/model_20
Loading model from checkpoints/20230117_221131/model_20
SUB: 13.58%, DEL: 32.59%, INS: 0.76%, COR: 53.83%, PER: 46.93%
