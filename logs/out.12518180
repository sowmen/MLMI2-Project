Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.249265093803405
  batch 100 loss: 3.181393837928772
  batch 150 loss: 2.9252290678024293
  batch 200 loss: 2.616034255027771
  batch 250 loss: 2.3950412702560424
  batch 300 loss: 2.2007591342926025
  batch 350 loss: 2.0485131573677062
  batch 400 loss: 2.0172336149215697
  batch 450 loss: 1.9211060976982117
  batch 500 loss: 1.820275275707245
  batch 550 loss: 1.7694134306907654
  batch 600 loss: 1.7299236679077148
  batch 650 loss: 1.6296050238609314
  batch 700 loss: 1.6780457520484924
  batch 750 loss: 1.6125112199783325
  batch 800 loss: 1.607893433570862
  batch 850 loss: 1.5653509712219238
  batch 900 loss: 1.5422642850875854
LOSS train 1.54226 valid 1.51927, valid PER 52.31%
EPOCH 2:
  batch 50 loss: 1.4747409057617187
  batch 100 loss: 1.4525954294204713
  batch 150 loss: 1.4139022517204285
  batch 200 loss: 1.4620610141754151
  batch 250 loss: 1.4489746260643006
  batch 300 loss: 1.4174805808067321
  batch 350 loss: 1.332220230102539
  batch 400 loss: 1.3889765787124633
  batch 450 loss: 1.31385604262352
  batch 500 loss: 1.3384695029258729
  batch 550 loss: 1.3518248176574708
  batch 600 loss: 1.3119223368167878
  batch 650 loss: 1.332294294834137
  batch 700 loss: 1.3311235988140107
  batch 750 loss: 1.3163044834136963
  batch 800 loss: 1.2459672796726227
  batch 850 loss: 1.283088434934616
  batch 900 loss: 1.2831435441970824
LOSS train 1.28314 valid 1.28052, valid PER 40.63%
EPOCH 3:
  batch 50 loss: 1.2604273557662964
  batch 100 loss: 1.2242585039138794
  batch 150 loss: 1.1972042059898376
  batch 200 loss: 1.2212113642692566
  batch 250 loss: 1.2115964925289153
  batch 300 loss: 1.18765722990036
  batch 350 loss: 1.2427738499641419
  batch 400 loss: 1.2156745457649232
  batch 450 loss: 1.2126759815216064
  batch 500 loss: 1.1775225925445556
  batch 550 loss: 1.1884674155712127
  batch 600 loss: 1.1923966908454895
  batch 650 loss: 1.169669120311737
  batch 700 loss: 1.167940809726715
  batch 750 loss: 1.227040182352066
  batch 800 loss: 1.1592335975170136
  batch 850 loss: 1.19944033741951
  batch 900 loss: 1.1466932845115663
LOSS train 1.14669 valid 1.22702, valid PER 37.44%
EPOCH 4:
  batch 50 loss: 1.101687045097351
  batch 100 loss: 1.1251201808452607
  batch 150 loss: 1.104420303106308
  batch 200 loss: 1.1367252051830292
  batch 250 loss: 1.191102476119995
  batch 300 loss: 1.1707496845722198
  batch 350 loss: 1.100094598531723
  batch 400 loss: 1.1265247297286987
  batch 450 loss: 1.095931990146637
  batch 500 loss: 1.0806439161300658
  batch 550 loss: 1.1328305578231812
  batch 600 loss: 1.1581463587284089
  batch 650 loss: 1.1302516853809357
  batch 700 loss: 1.0980811250209808
  batch 750 loss: 1.0971051490306853
  batch 800 loss: 1.042296175956726
  batch 850 loss: 1.0895506834983826
  batch 900 loss: 1.143098908662796
LOSS train 1.14310 valid 1.14967, valid PER 35.50%
EPOCH 5:
  batch 50 loss: 1.0487922739982605
  batch 100 loss: 1.0401552486419678
  batch 150 loss: 1.1251234602928162
  batch 200 loss: 1.0203943538665772
  batch 250 loss: 1.0364891636371611
  batch 300 loss: 1.0666860103607179
  batch 350 loss: 1.074965670108795
  batch 400 loss: 1.1059796166419984
  batch 450 loss: 1.1347622668743134
  batch 500 loss: 1.1813537883758545
  batch 550 loss: 1.089370894432068
  batch 600 loss: 1.1622195875644683
  batch 650 loss: 1.1398025524616242
  batch 700 loss: 1.1322703647613526
  batch 750 loss: 1.0673269164562225
  batch 800 loss: 1.0740070164203643
  batch 850 loss: 1.0905154967308044
  batch 900 loss: 1.1074854862689971
LOSS train 1.10749 valid 1.14167, valid PER 34.85%
EPOCH 6:
  batch 50 loss: 1.068830008506775
  batch 100 loss: 1.0173005366325378
  batch 150 loss: 0.9942945194244385
  batch 200 loss: 1.012324630022049
  batch 250 loss: 1.0591719806194306
  batch 300 loss: 1.0333758068084717
  batch 350 loss: 1.0271788799762727
  batch 400 loss: 1.0199840021133424
  batch 450 loss: 1.037219854593277
  batch 500 loss: 1.0223941898345947
  batch 550 loss: 1.0389485669136047
  batch 600 loss: 1.0012778222560883
  batch 650 loss: 1.0316370129585266
  batch 700 loss: 1.026667413711548
  batch 750 loss: 1.005069444179535
  batch 800 loss: 0.9904325723648071
  batch 850 loss: 1.0294929134845734
  batch 900 loss: 1.0511082983016968
LOSS train 1.05111 valid 1.06732, valid PER 33.96%
EPOCH 7:
  batch 50 loss: 0.9726812767982483
  batch 100 loss: 0.9715583956241608
  batch 150 loss: 0.9636207962036133
  batch 200 loss: 0.9605623877048493
  batch 250 loss: 0.9875934457778931
  batch 300 loss: 0.9805932545661926
  batch 350 loss: 0.9922723352909089
  batch 400 loss: 0.9870087373256683
  batch 450 loss: 0.9928548645973205
  batch 500 loss: 0.9886212694644928
  batch 550 loss: 0.9807634747028351
  batch 600 loss: 1.0136175239086151
  batch 650 loss: 0.9770838189125061
  batch 700 loss: 1.0029632115364076
  batch 750 loss: 0.9904135262966156
  batch 800 loss: 0.9797564852237701
  batch 850 loss: 1.035085244178772
  batch 900 loss: 1.0879885601997374
LOSS train 1.08799 valid 1.11841, valid PER 34.66%
EPOCH 8:
  batch 50 loss: 1.0433248317241668
  batch 100 loss: 1.0333652555942536
  batch 150 loss: 1.0544988846778869
  batch 200 loss: 1.0280947232246398
  batch 250 loss: 1.0788713026046752
  batch 300 loss: 0.9680752587318421
  batch 350 loss: 1.0354405426979065
  batch 400 loss: 0.9776839995384217
  batch 450 loss: 1.0113104450702668
  batch 500 loss: 1.0506902396678925
  batch 550 loss: 0.9840559756755829
  batch 600 loss: 1.007961163520813
  batch 650 loss: 1.0311752462387085
  batch 700 loss: 0.9925094938278198
  batch 750 loss: 0.9892139196395874
  batch 800 loss: 0.9939148044586181
  batch 850 loss: 0.9911865246295929
  batch 900 loss: 1.0059371042251586
LOSS train 1.00594 valid 1.09855, valid PER 33.72%
EPOCH 9:
  batch 50 loss: 0.9340853559970855
  batch 100 loss: 1.0169691026210785
  batch 150 loss: 0.9619080817699432
  batch 200 loss: 0.919465742111206
  batch 250 loss: 0.9548839724063873
  batch 300 loss: 0.9766453540325165
  batch 350 loss: 0.9856063330173492
  batch 400 loss: 0.9536065387725831
  batch 450 loss: 1.0240769350528718
  batch 500 loss: 0.9890188097953796
  batch 550 loss: 1.0196770024299622
  batch 600 loss: 1.0023345947265625
  batch 650 loss: 0.9831172287464142
  batch 700 loss: 0.9688366305828094
  batch 750 loss: 1.00328777551651
  batch 800 loss: 0.9811299920082093
  batch 850 loss: 0.9778081440925598
  batch 900 loss: 0.9406333494186402
LOSS train 0.94063 valid 1.08590, valid PER 33.45%
EPOCH 10:
  batch 50 loss: 0.9387689852714538
  batch 100 loss: 0.910222133398056
  batch 150 loss: 0.9413589370250702
  batch 200 loss: 0.9655920040607452
  batch 250 loss: 0.9979414904117584
  batch 300 loss: 0.9145715713500977
  batch 350 loss: 0.9628628098964691
  batch 400 loss: 0.9247458982467651
  batch 450 loss: 0.9046267819404602
  batch 500 loss: 0.9576353061199189
  batch 550 loss: 0.9834535467624664
  batch 600 loss: 0.9419023013114929
  batch 650 loss: 0.9610433375835419
  batch 700 loss: 0.9595159423351288
  batch 750 loss: 0.9496392190456391
  batch 800 loss: 0.9606886053085327
  batch 850 loss: 1.0248474335670472
  batch 900 loss: 1.00657510638237
LOSS train 1.00658 valid 1.10259, valid PER 33.86%
EPOCH 11:
  batch 50 loss: 0.9530239641666413
  batch 100 loss: 0.8968593633174896
  batch 150 loss: 0.9132428050041199
  batch 200 loss: 0.9454809951782227
  batch 250 loss: 0.9448019981384277
  batch 300 loss: 0.915525803565979
  batch 350 loss: 0.9830269026756286
  batch 400 loss: 0.9742451131343841
  batch 450 loss: 1.0432186210155487
  batch 500 loss: 1.0010736215114593
  batch 550 loss: 0.9805089569091797
  batch 600 loss: 0.9734032046794892
  batch 650 loss: 1.0066517889499664
  batch 700 loss: 0.9271621417999267
  batch 750 loss: 0.9396574962139129
  batch 800 loss: 0.9775833487510681
  batch 850 loss: 1.0052626085281373
  batch 900 loss: 0.9784609115123749
LOSS train 0.97846 valid 1.08695, valid PER 32.56%
EPOCH 12:
  batch 50 loss: 0.9080633676052093
  batch 100 loss: 0.8781358599662781
  batch 150 loss: 0.8600838565826416
  batch 200 loss: 0.9089558672904968
  batch 250 loss: 0.9211401081085205
  batch 300 loss: 0.886148978471756
  batch 350 loss: 0.9015751123428345
  batch 400 loss: 0.931097285747528
  batch 450 loss: 0.9409699654579162
  batch 500 loss: 0.9592047727108002
  batch 550 loss: 0.8778102815151214
  batch 600 loss: 0.9049742412567139
  batch 650 loss: 0.9574271821975708
  batch 700 loss: 0.9390662682056427
  batch 750 loss: 0.9310526168346405
  batch 800 loss: 0.9117977631092071
  batch 850 loss: 0.9650596082210541
  batch 900 loss: 0.9495201802253723
LOSS train 0.94952 valid 1.06350, valid PER 32.68%
EPOCH 13:
  batch 50 loss: 0.8775377655029297
  batch 100 loss: 0.9180859124660492
  batch 150 loss: 0.8745917892456054
  batch 200 loss: 0.9068942070007324
  batch 250 loss: 0.8920761060714721
  batch 300 loss: 0.8882193720340729
  batch 350 loss: 0.9062472927570343
  batch 400 loss: 0.9578749108314514
  batch 450 loss: 0.9700296938419342
  batch 500 loss: 0.9075678873062134
  batch 550 loss: 0.9282401955127716
  batch 600 loss: 0.9156970572471619
  batch 650 loss: 0.9566748297214508
  batch 700 loss: 0.9487904822826385
  batch 750 loss: 0.8990732336044311
  batch 800 loss: 0.9263484072685242
  batch 850 loss: 0.9343646347522736
  batch 900 loss: 0.9181879436969758
LOSS train 0.91819 valid 1.07543, valid PER 31.64%
EPOCH 14:
  batch 50 loss: 0.8559349358081818
  batch 100 loss: 0.8904728150367737
  batch 150 loss: 0.8891424429416657
  batch 200 loss: 0.9097815454006195
  batch 250 loss: 0.9044992482662201
  batch 300 loss: 0.916800719499588
  batch 350 loss: 0.8612200951576233
  batch 400 loss: 0.8665208649635315
  batch 450 loss: 0.8854059529304504
  batch 500 loss: 0.8904172492027282
  batch 550 loss: 0.9025965118408203
  batch 600 loss: 0.8473492324352264
  batch 650 loss: 0.9201881766319275
  batch 700 loss: 0.928510160446167
  batch 750 loss: 0.899878009557724
  batch 800 loss: 0.8848246943950653
  batch 850 loss: 0.976065673828125
  batch 900 loss: 0.94954265832901
LOSS train 0.94954 valid 1.07237, valid PER 32.90%
EPOCH 15:
  batch 50 loss: 0.8613955783843994
  batch 100 loss: 0.8523180615901947
  batch 150 loss: 0.8451652896404266
  batch 200 loss: 0.9122102642059327
  batch 250 loss: 0.8702560615539551
  batch 300 loss: 0.8511592447757721
  batch 350 loss: 0.8823648273944855
  batch 400 loss: 0.8565449178218841
  batch 450 loss: 0.8754072844982147
  batch 500 loss: 0.8320570611953735
  batch 550 loss: 0.8655966007709504
  batch 600 loss: 0.8734574854373932
  batch 650 loss: 0.889964623451233
  batch 700 loss: 0.882292697429657
  batch 750 loss: 0.8906473433971405
  batch 800 loss: 0.8709894859790802
  batch 850 loss: 0.8518124771118164
  batch 900 loss: 0.8716154372692109
LOSS train 0.87162 valid 1.04937, valid PER 31.91%
EPOCH 16:
  batch 50 loss: 0.8736657917499542
  batch 100 loss: 0.8345841801166535
  batch 150 loss: 0.8318272566795349
  batch 200 loss: 0.8737278091907501
  batch 250 loss: 0.8933738434314727
  batch 300 loss: 0.8730295586585999
  batch 350 loss: 0.8869600439071655
  batch 400 loss: 0.9000965023040771
  batch 450 loss: 0.8920623552799225
  batch 500 loss: 0.8531781578063965
  batch 550 loss: 0.8753551197052002
  batch 600 loss: 0.9391124761104583
  batch 650 loss: 0.9294156408309937
  batch 700 loss: 0.8663035094738006
  batch 750 loss: 0.8960136222839356
  batch 800 loss: 0.8795937097072601
  batch 850 loss: 0.8829883873462677
  batch 900 loss: 0.8975287222862244
LOSS train 0.89753 valid 1.05994, valid PER 32.33%
EPOCH 17:
  batch 50 loss: 0.8424832391738891
  batch 100 loss: 0.831217827796936
  batch 150 loss: 0.8383430814743043
  batch 200 loss: 0.8232471311092376
  batch 250 loss: 0.8284038925170898
  batch 300 loss: 0.8734362077713013
  batch 350 loss: 0.8165593266487121
  batch 400 loss: 0.8789618980884552
  batch 450 loss: 0.8880255341529846
  batch 500 loss: 0.8571174132823944
  batch 550 loss: 0.9007175099849701
  batch 600 loss: 0.8902096033096314
  batch 650 loss: 0.9125294137001038
  batch 700 loss: 0.8902367353439331
  batch 750 loss: 0.8521936047077179
  batch 800 loss: 0.8724186778068542
  batch 850 loss: 0.9002640104293823
  batch 900 loss: 0.8601021480560302
LOSS train 0.86010 valid 1.07227, valid PER 32.13%
EPOCH 18:
  batch 50 loss: 0.8641488981246949
  batch 100 loss: 0.9200350797176361
  batch 150 loss: 1.0137537908554077
  batch 200 loss: 0.9455079507827758
  batch 250 loss: 0.9412980115413666
  batch 300 loss: 0.8655208051204681
  batch 350 loss: 0.9095386934280395
  batch 400 loss: 0.8770644891262055
  batch 450 loss: 0.9161270701885224
  batch 500 loss: 0.9039286947250367
  batch 550 loss: 0.9038793349266052
  batch 600 loss: 0.8419271695613861
  batch 650 loss: 0.8530208027362823
  batch 700 loss: 0.8924760484695434
  batch 750 loss: 0.8698184263706207
  batch 800 loss: 0.8832797563076019
  batch 850 loss: 0.8914468097686767
  batch 900 loss: 0.8805432260036469
LOSS train 0.88054 valid 1.04913, valid PER 31.42%
EPOCH 19:
  batch 50 loss: 0.7947324538230895
  batch 100 loss: 0.7732698738574981
  batch 150 loss: 0.8158877694606781
  batch 200 loss: 0.8323620855808258
  batch 250 loss: 0.8600944256782532
  batch 300 loss: 0.8276048159599304
  batch 350 loss: 0.838868967294693
  batch 400 loss: 0.8490780127048493
  batch 450 loss: 0.8580276072025299
  batch 500 loss: 0.8630818450450897
  batch 550 loss: 0.8207233893871307
  batch 600 loss: 0.8482282125949859
  batch 650 loss: 0.9081896615028381
  batch 700 loss: 0.8331221413612365
  batch 750 loss: 0.8322425544261932
  batch 800 loss: 0.8767666065692902
  batch 850 loss: 0.8780999946594238
  batch 900 loss: 0.8778721630573273
LOSS train 0.87787 valid 1.04983, valid PER 31.26%
EPOCH 20:
  batch 50 loss: 0.7974243366718292
  batch 100 loss: 0.821316694021225
  batch 150 loss: 0.8197807651758194
  batch 200 loss: 0.8332197976112365
  batch 250 loss: 0.8243617612123489
  batch 300 loss: 0.8584753990173339
  batch 350 loss: 0.7932719576358795
  batch 400 loss: 0.8392334413528443
  batch 450 loss: 0.8287457406520844
  batch 500 loss: 0.8225771653652191
  batch 550 loss: 0.8728911054134368
  batch 600 loss: 0.8094948935508728
  batch 650 loss: 0.8341787183284759
  batch 700 loss: 0.8661506044864654
  batch 750 loss: 0.8106958091259002
  batch 800 loss: 0.847580440044403
  batch 850 loss: 0.8684744036197662
  batch 900 loss: 0.8705512654781341
LOSS train 0.87055 valid 1.05586, valid PER 31.16%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_222050/model_18
Loading model from checkpoints/20230117_222050/model_18
SUB: 18.96%, DEL: 11.15%, INS: 2.63%, COR: 69.89%, PER: 32.74%
