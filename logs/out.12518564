Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 8.722096123695373
  batch 100 loss: 3.2250699377059937
  batch 150 loss: 3.121347146034241
  batch 200 loss: 3.031336431503296
  batch 250 loss: 2.9881688833236693
  batch 300 loss: 2.885242667198181
  batch 350 loss: 2.7301185941696167
  batch 400 loss: 2.6675768852233888
  batch 450 loss: 2.6111146783828736
  batch 500 loss: 2.5045743942260743
  batch 550 loss: 2.4163716173172
  batch 600 loss: 2.358266282081604
  batch 650 loss: 2.28090708732605
  batch 700 loss: 2.199993453025818
  batch 750 loss: 2.117383439540863
  batch 800 loss: 2.087884726524353
  batch 850 loss: 2.0291136384010313
  batch 900 loss: 1.9851835322380067
LOSS train 1.98518 valid 1.94627, valid PER 72.76%
EPOCH 2:
  batch 50 loss: 1.9253127217292785
  batch 100 loss: 1.871050968170166
  batch 150 loss: 1.830171196460724
  batch 200 loss: 1.8305516719818116
  batch 250 loss: 1.8097805905342101
  batch 300 loss: 1.7903728771209717
  batch 350 loss: 1.7023674178123473
  batch 400 loss: 1.7075848531723024
  batch 450 loss: 1.6591619753837585
  batch 500 loss: 1.6764887499809265
  batch 550 loss: 1.6702960562705993
  batch 600 loss: 1.6224410820007324
  batch 650 loss: 1.6411601972579957
  batch 700 loss: 1.6170129370689392
  batch 750 loss: 1.5907327103614808
  batch 800 loss: 1.5410727167129517
  batch 850 loss: 1.5327867698669433
  batch 900 loss: 1.568153202533722
LOSS train 1.56815 valid 1.55818, valid PER 56.64%
EPOCH 3:
  batch 50 loss: 1.5175447511672973
  batch 100 loss: 1.4723982453346252
  batch 150 loss: 1.4624889206886291
  batch 200 loss: 1.462133631706238
  batch 250 loss: 1.430663721561432
  batch 300 loss: 1.4275870609283448
  batch 350 loss: 1.4619147872924805
  batch 400 loss: 1.423032865524292
  batch 450 loss: 1.4165146327018738
  batch 500 loss: 1.4147376799583435
  batch 550 loss: 1.389835810661316
  batch 600 loss: 1.3634114575386047
  batch 650 loss: 1.3425131416320801
  batch 700 loss: 1.3501382517814635
  batch 750 loss: 1.4039691519737243
  batch 800 loss: 1.328690357208252
  batch 850 loss: 1.36679368019104
  batch 900 loss: 1.3131853556632995
LOSS train 1.31319 valid 1.36296, valid PER 44.19%
EPOCH 4:
  batch 50 loss: 1.2837566971778869
  batch 100 loss: 1.294854085445404
  batch 150 loss: 1.2633997297286987
  batch 200 loss: 1.3156926536560059
  batch 250 loss: 1.3108858895301818
  batch 300 loss: 1.2832847046852112
  batch 350 loss: 1.2276622939109803
  batch 400 loss: 1.2867056322097778
  batch 450 loss: 1.2583371388912201
  batch 500 loss: 1.2454391503334046
  batch 550 loss: 1.2786616063117981
  batch 600 loss: 1.2719279313087464
  batch 650 loss: 1.2508076643943786
  batch 700 loss: 1.224532997608185
  batch 750 loss: 1.2172250247001648
  batch 800 loss: 1.183508893251419
  batch 850 loss: 1.2132333815097809
  batch 900 loss: 1.2453012192249298
LOSS train 1.24530 valid 1.23220, valid PER 39.34%
EPOCH 5:
  batch 50 loss: 1.1801937854290008
  batch 100 loss: 1.1667748951911927
  batch 150 loss: 1.2143199527263642
  batch 200 loss: 1.1440122306346894
  batch 250 loss: 1.1671082782745361
  batch 300 loss: 1.1777318930625915
  batch 350 loss: 1.1942659318447113
  batch 400 loss: 1.1922697591781617
  batch 450 loss: 1.177061940431595
  batch 500 loss: 1.182402845621109
  batch 550 loss: 1.1348181760311127
  batch 600 loss: 1.192913725376129
  batch 650 loss: 1.1460517811775208
  batch 700 loss: 1.1936625695228578
  batch 750 loss: 1.1400856125354766
  batch 800 loss: 1.1560719454288482
  batch 850 loss: 1.1748173344135284
  batch 900 loss: 1.1590298616886139
LOSS train 1.15903 valid 1.18827, valid PER 37.36%
EPOCH 6:
  batch 50 loss: 1.1455020475387574
  batch 100 loss: 1.0948832380771636
  batch 150 loss: 1.0930153024196625
  batch 200 loss: 1.1050992012023926
  batch 250 loss: 1.1398877131938934
  batch 300 loss: 1.113137148618698
  batch 350 loss: 1.101671541929245
  batch 400 loss: 1.0866342854499818
  batch 450 loss: 1.1285539591312408
  batch 500 loss: 1.1079211711883545
  batch 550 loss: 1.1020962250232698
  batch 600 loss: 1.087550402879715
  batch 650 loss: 1.1230268931388856
  batch 700 loss: 1.1193000280857086
  batch 750 loss: 1.0726914811134338
  batch 800 loss: 1.0753505325317383
  batch 850 loss: 1.0879815757274627
  batch 900 loss: 1.0940802597999573
LOSS train 1.09408 valid 1.13123, valid PER 36.52%
EPOCH 7:
  batch 50 loss: 1.0696506571769715
  batch 100 loss: 1.0764225018024445
  batch 150 loss: 1.0747642362117766
  batch 200 loss: 1.0470333743095397
  batch 250 loss: 1.0496022844314574
  batch 300 loss: 1.0407600843906402
  batch 350 loss: 1.0363337624073028
  batch 400 loss: 1.0603827691078187
  batch 450 loss: 1.059094090461731
  batch 500 loss: 1.068503302335739
  batch 550 loss: 1.0476060175895692
  batch 600 loss: 1.061928322315216
  batch 650 loss: 1.027152557373047
  batch 700 loss: 1.0696680128574372
  batch 750 loss: 1.037153536081314
  batch 800 loss: 1.0415383565425873
  batch 850 loss: 1.0541626620292663
  batch 900 loss: 1.0709971022605895
LOSS train 1.07100 valid 1.12437, valid PER 35.92%
EPOCH 8:
  batch 50 loss: 1.0392264997959137
  batch 100 loss: 1.0068831741809845
  batch 150 loss: 1.003624620437622
  batch 200 loss: 0.9955818831920624
  batch 250 loss: 1.0094407558441163
  batch 300 loss: 0.9454299902915955
  batch 350 loss: 1.0306141042709351
  batch 400 loss: 0.9863528525829315
  batch 450 loss: 1.0180235481262208
  batch 500 loss: 1.0356739497184753
  batch 550 loss: 0.9978127932548523
  batch 600 loss: 1.041150656938553
  batch 650 loss: 1.0461645591259003
  batch 700 loss: 0.9818615257740021
  batch 750 loss: 1.044428724050522
  batch 800 loss: 1.025871170759201
  batch 850 loss: 1.0015939152240754
  batch 900 loss: 0.9863213837146759
LOSS train 0.98632 valid 1.06223, valid PER 33.02%
EPOCH 9:
  batch 50 loss: 0.9228329002857208
  batch 100 loss: 0.9743765234947205
  batch 150 loss: 0.9756799077987671
  batch 200 loss: 0.9454216110706329
  batch 250 loss: 0.9699133026599884
  batch 300 loss: 0.9626454579830169
  batch 350 loss: 1.02074759721756
  batch 400 loss: 0.9677551555633545
  batch 450 loss: 0.9882022190093994
  batch 500 loss: 0.945710027217865
  batch 550 loss: 0.9855630779266358
  batch 600 loss: 0.9912361645698547
  batch 650 loss: 0.9805176186561585
  batch 700 loss: 0.9614857077598572
  batch 750 loss: 0.9586964857578277
  batch 800 loss: 0.963615540266037
  batch 850 loss: 0.9930224168300629
  batch 900 loss: 0.9512676405906677
LOSS train 0.95127 valid 1.03149, valid PER 32.44%
EPOCH 10:
  batch 50 loss: 0.8977807414531708
  batch 100 loss: 0.9260118281841279
  batch 150 loss: 0.956709337234497
  batch 200 loss: 0.9636867237091065
  batch 250 loss: 0.954167263507843
  batch 300 loss: 0.907325119972229
  batch 350 loss: 0.9539071249961854
  batch 400 loss: 0.9137390327453613
  batch 450 loss: 0.9045752441883087
  batch 500 loss: 0.9643439531326294
  batch 550 loss: 0.9676145935058593
  batch 600 loss: 0.9331808352470398
  batch 650 loss: 0.9213152253627777
  batch 700 loss: 0.9620037174224854
  batch 750 loss: 0.941429545879364
  batch 800 loss: 0.9300671279430389
  batch 850 loss: 0.9407141530513763
  batch 900 loss: 0.9434029638767243
LOSS train 0.94340 valid 1.03628, valid PER 32.56%
EPOCH 11:
  batch 50 loss: 0.8899911451339722
  batch 100 loss: 0.882253407239914
  batch 150 loss: 0.8730801320075989
  batch 200 loss: 0.9280561494827271
  batch 250 loss: 0.9103093826770783
  batch 300 loss: 0.88329549908638
  batch 350 loss: 0.895392496585846
  batch 400 loss: 0.9097160434722901
  batch 450 loss: 0.9168240702152253
  batch 500 loss: 0.8860564148426056
  batch 550 loss: 0.8900455129146576
  batch 600 loss: 0.8634752011299134
  batch 650 loss: 0.9501245236396789
  batch 700 loss: 0.9050764167308807
  batch 750 loss: 0.90719491481781
  batch 800 loss: 0.9170671093463898
  batch 850 loss: 0.9455622529983521
  batch 900 loss: 0.9250575470924377
LOSS train 0.92506 valid 1.02997, valid PER 31.29%
EPOCH 12:
  batch 50 loss: 0.8930279743671418
  batch 100 loss: 0.8581445860862732
  batch 150 loss: 0.8415078723430633
  batch 200 loss: 0.8641388416290283
  batch 250 loss: 0.8788545060157776
  batch 300 loss: 0.8503450131416321
  batch 350 loss: 0.8667680823802948
  batch 400 loss: 0.8759946393966674
  batch 450 loss: 0.8803146946430206
  batch 500 loss: 0.8880894982814789
  batch 550 loss: 0.8121731662750244
  batch 600 loss: 0.8627372264862061
  batch 650 loss: 0.9001054179668426
  batch 700 loss: 0.8928145349025727
  batch 750 loss: 0.851548478603363
  batch 800 loss: 0.8586646831035614
  batch 850 loss: 0.9013575136661529
  batch 900 loss: 0.8923844969272614
LOSS train 0.89238 valid 0.99282, valid PER 31.90%
EPOCH 13:
  batch 50 loss: 0.8082245206832885
  batch 100 loss: 0.8410194456577301
  batch 150 loss: 0.8282352477312088
  batch 200 loss: 0.8448339569568634
  batch 250 loss: 0.8297249495983123
  batch 300 loss: 0.8492862164974213
  batch 350 loss: 0.8151793563365937
  batch 400 loss: 0.8569578599929809
  batch 450 loss: 0.8805186641216278
  batch 500 loss: 0.8270364725589752
  batch 550 loss: 0.8690458393096924
  batch 600 loss: 0.8494459676742554
  batch 650 loss: 0.8567524933815003
  batch 700 loss: 0.8633773303031922
  batch 750 loss: 0.8181977820396423
  batch 800 loss: 0.8422712206840515
  batch 850 loss: 0.8914205253124237
  batch 900 loss: 0.8801536548137665
LOSS train 0.88015 valid 0.97896, valid PER 31.16%
EPOCH 14:
  batch 50 loss: 0.8088458144664764
  batch 100 loss: 0.8274904835224152
  batch 150 loss: 0.7992191553115845
  batch 200 loss: 0.8221795797348023
  batch 250 loss: 0.8134314572811127
  batch 300 loss: 0.8392091023921967
  batch 350 loss: 0.7798183465003967
  batch 400 loss: 0.8114037549495697
  batch 450 loss: 0.8226396489143372
  batch 500 loss: 0.8343600571155548
  batch 550 loss: 0.8560937035083771
  batch 600 loss: 0.7978127968311309
  batch 650 loss: 0.8624626743793488
  batch 700 loss: 0.8444420993328094
  batch 750 loss: 0.8233375644683838
  batch 800 loss: 0.8032304453849792
  batch 850 loss: 0.8614053380489349
  batch 900 loss: 0.8420460188388824
LOSS train 0.84205 valid 0.97397, valid PER 30.59%
EPOCH 15:
  batch 50 loss: 0.7936161041259766
  batch 100 loss: 0.7864558696746826
  batch 150 loss: 0.7829818308353425
  batch 200 loss: 0.8200806593894958
  batch 250 loss: 0.8327954745292664
  batch 300 loss: 0.7928247344493866
  batch 350 loss: 0.8034944093227386
  batch 400 loss: 0.7954120695590973
  batch 450 loss: 0.7933842182159424
  batch 500 loss: 0.7622268128395081
  batch 550 loss: 0.8104025673866272
  batch 600 loss: 0.8269354641437531
  batch 650 loss: 0.8348188424110412
  batch 700 loss: 0.8277650821208954
  batch 750 loss: 0.8355018186569214
  batch 800 loss: 0.7992934727668762
  batch 850 loss: 0.7781164431571961
  batch 900 loss: 0.8040527558326721
LOSS train 0.80405 valid 0.96435, valid PER 29.53%
EPOCH 16:
  batch 50 loss: 0.7868226552009583
  batch 100 loss: 0.7430485904216766
  batch 150 loss: 0.7771676290035248
  batch 200 loss: 0.7884137606620789
  batch 250 loss: 0.7826610994338989
  batch 300 loss: 0.7790684914588928
  batch 350 loss: 0.796613564491272
  batch 400 loss: 0.8083756744861603
  batch 450 loss: 0.7850791972875595
  batch 500 loss: 0.752258665561676
  batch 550 loss: 0.7879158461093902
  batch 600 loss: 0.8134979927539825
  batch 650 loss: 0.8038131630420685
  batch 700 loss: 0.7548256731033325
  batch 750 loss: 0.7846696758270264
  batch 800 loss: 0.7891338765621185
  batch 850 loss: 0.7841083526611328
  batch 900 loss: 0.7805910778045654
LOSS train 0.78059 valid 0.94914, valid PER 29.98%
EPOCH 17:
  batch 50 loss: 0.7394004958868027
  batch 100 loss: 0.7571716278791427
  batch 150 loss: 0.756114416718483
  batch 200 loss: 0.7345171111822129
  batch 250 loss: 0.7551200819015503
  batch 300 loss: 0.7534185361862182
  batch 350 loss: 0.7173159438371658
  batch 400 loss: 0.8061093854904174
  batch 450 loss: 0.7904651701450348
  batch 500 loss: 0.7477416682243347
  batch 550 loss: 0.7701253753900528
  batch 600 loss: 0.8064047414064407
  batch 650 loss: 0.7615719985961914
  batch 700 loss: 0.7493143928050995
  batch 750 loss: 0.7414716601371765
  batch 800 loss: 0.7535794520378113
  batch 850 loss: 0.7651198607683182
  batch 900 loss: 0.7375878810882568
LOSS train 0.73759 valid 0.96154, valid PER 29.51%
EPOCH 18:
  batch 50 loss: 0.7141343033313752
  batch 100 loss: 0.7198708629608155
  batch 150 loss: 0.7472901809215545
  batch 200 loss: 0.7457250118255615
  batch 250 loss: 0.7538414859771728
  batch 300 loss: 0.7195164132118225
  batch 350 loss: 0.7601002180576324
  batch 400 loss: 0.7398379009962082
  batch 450 loss: 0.7775700950622558
  batch 500 loss: 0.7719013619422913
  batch 550 loss: 0.7750127702951431
  batch 600 loss: 0.7251288020610809
  batch 650 loss: 0.7457756978273392
  batch 700 loss: 0.7493124365806579
  batch 750 loss: 0.7360759204626084
  batch 800 loss: 0.7330848568677902
  batch 850 loss: 0.7331741237640381
  batch 900 loss: 0.7684453046321869
LOSS train 0.76845 valid 0.95304, valid PER 29.08%
EPOCH 19:
  batch 50 loss: 0.688147703409195
  batch 100 loss: 0.6875428825616836
  batch 150 loss: 0.7090223932266235
  batch 200 loss: 0.7203979521989823
  batch 250 loss: 0.7650127804279327
  batch 300 loss: 0.7513606095314026
  batch 350 loss: 0.7063971984386445
  batch 400 loss: 0.7076038634777069
  batch 450 loss: 0.763397604227066
  batch 500 loss: 0.7752956104278564
  batch 550 loss: 0.7446884763240814
  batch 600 loss: 0.7378775906562806
  batch 650 loss: 0.7892222827672959
  batch 700 loss: 0.7173372143507004
  batch 750 loss: 0.7047263443470001
  batch 800 loss: 0.750193612575531
  batch 850 loss: 0.7409732842445373
  batch 900 loss: 0.7096196055412293
LOSS train 0.70962 valid 0.93540, valid PER 28.52%
EPOCH 20:
  batch 50 loss: 0.6805490547418594
  batch 100 loss: 0.6899169063568116
  batch 150 loss: 0.6704717588424682
  batch 200 loss: 0.6773318278789521
  batch 250 loss: 0.6805932140350341
  batch 300 loss: 0.7226085340976716
  batch 350 loss: 0.6777487063407898
  batch 400 loss: 0.7053347104787826
  batch 450 loss: 0.691780806183815
  batch 500 loss: 0.6825045573711396
  batch 550 loss: 0.7494042158126831
  batch 600 loss: 0.6945981013774872
  batch 650 loss: 0.7235425943136216
  batch 700 loss: 0.7188294458389283
  batch 750 loss: 0.7025806385278702
  batch 800 loss: 0.7324959433078766
  batch 850 loss: 0.747330983877182
  batch 900 loss: 0.7441346251964569
LOSS train 0.74413 valid 0.95169, valid PER 29.06%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_223444/model_19
Loading model from checkpoints/20230117_223444/model_19
SUB: 16.34%, DEL: 11.36%, INS: 2.60%, COR: 72.31%, PER: 30.29%
