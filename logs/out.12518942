Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 9.497078490257262
  batch 100 loss: 3.298131823539734
  batch 150 loss: 3.188040099143982
  batch 200 loss: 2.981644878387451
  batch 250 loss: 2.728359832763672
  batch 300 loss: 2.516980309486389
  batch 350 loss: 2.3372445487976075
  batch 400 loss: 2.2571979093551637
  batch 450 loss: 2.153707830905914
  batch 500 loss: 2.057410788536072
  batch 550 loss: 1.9781787490844727
  batch 600 loss: 1.9271198749542235
  batch 650 loss: 1.8542068123817443
  batch 700 loss: 1.856767086982727
  batch 750 loss: 1.79212943315506
  batch 800 loss: 1.7697077584266663
  batch 850 loss: 1.7297301077842713
  batch 900 loss: 1.7078325152397156
LOSS train 1.70783 valid 1.68546, valid PER 65.40%
EPOCH 2:
  batch 50 loss: 1.6534257340431213
  batch 100 loss: 1.60752375125885
  batch 150 loss: 1.5715187120437621
  batch 200 loss: 1.5963890337944031
  batch 250 loss: 1.584587812423706
  batch 300 loss: 1.5695465779304505
  batch 350 loss: 1.478942975997925
  batch 400 loss: 1.5108382630348205
  batch 450 loss: 1.457237846851349
  batch 500 loss: 1.4856817507743836
  batch 550 loss: 1.4923675775527954
  batch 600 loss: 1.4425880479812623
  batch 650 loss: 1.4783336067199706
  batch 700 loss: 1.4323631811141968
  batch 750 loss: 1.4340938138961792
  batch 800 loss: 1.3520752215385436
  batch 850 loss: 1.3819942784309387
  batch 900 loss: 1.4118519234657287
LOSS train 1.41185 valid 1.40250, valid PER 49.31%
EPOCH 3:
  batch 50 loss: 1.3670067501068115
  batch 100 loss: 1.3317549085617066
  batch 150 loss: 1.336746060848236
  batch 200 loss: 1.3087410497665406
  batch 250 loss: 1.3133842849731445
  batch 300 loss: 1.3022446298599244
  batch 350 loss: 1.3344057965278626
  batch 400 loss: 1.3112043929100037
  batch 450 loss: 1.2842701745033265
  batch 500 loss: 1.2681932377815246
  batch 550 loss: 1.262381772994995
  batch 600 loss: 1.2413060092926025
  batch 650 loss: 1.239341951608658
  batch 700 loss: 1.2459677827358246
  batch 750 loss: 1.2948897576332092
  batch 800 loss: 1.2410237324237823
  batch 850 loss: 1.2701265573501588
  batch 900 loss: 1.2011692595481873
LOSS train 1.20117 valid 1.26850, valid PER 40.47%
EPOCH 4:
  batch 50 loss: 1.187914445400238
  batch 100 loss: 1.2106404423713684
  batch 150 loss: 1.1655298554897309
  batch 200 loss: 1.2149006080627442
  batch 250 loss: 1.214779258966446
  batch 300 loss: 1.198688359260559
  batch 350 loss: 1.1394894063472747
  batch 400 loss: 1.1810403299331664
  batch 450 loss: 1.1491761159896852
  batch 500 loss: 1.134442321062088
  batch 550 loss: 1.1665588438510894
  batch 600 loss: 1.193308333158493
  batch 650 loss: 1.1734842920303346
  batch 700 loss: 1.1314323985576629
  batch 750 loss: 1.1190982210636138
  batch 800 loss: 1.097033269405365
  batch 850 loss: 1.136960084438324
  batch 900 loss: 1.1739010190963746
LOSS train 1.17390 valid 1.16380, valid PER 36.10%
EPOCH 5:
  batch 50 loss: 1.0977128875255584
  batch 100 loss: 1.077244609594345
  batch 150 loss: 1.1481205534934997
  batch 200 loss: 1.0614461302757263
  batch 250 loss: 1.0882254922389984
  batch 300 loss: 1.093973889350891
  batch 350 loss: 1.096746883392334
  batch 400 loss: 1.0872933328151704
  batch 450 loss: 1.076279343366623
  batch 500 loss: 1.1026681327819825
  batch 550 loss: 1.0427085626125336
  batch 600 loss: 1.1026103162765504
  batch 650 loss: 1.0738845646381379
  batch 700 loss: 1.1029779863357545
  batch 750 loss: 1.0370636677742004
  batch 800 loss: 1.084252163171768
  batch 850 loss: 1.072452825307846
  batch 900 loss: 1.0811413037776947
LOSS train 1.08114 valid 1.12390, valid PER 35.20%
EPOCH 6:
  batch 50 loss: 1.074270212650299
  batch 100 loss: 1.016639152765274
  batch 150 loss: 1.0002923905849457
  batch 200 loss: 1.0061425530910493
  batch 250 loss: 1.0489636731147767
  batch 300 loss: 1.0264217567443847
  batch 350 loss: 1.0350443494319916
  batch 400 loss: 1.0141365540027618
  batch 450 loss: 1.0244013226032258
  batch 500 loss: 0.9989536583423615
  batch 550 loss: 1.0316717910766602
  batch 600 loss: 1.0065734231472014
  batch 650 loss: 1.0261546933650971
  batch 700 loss: 1.029716384410858
  batch 750 loss: 1.0232143461704255
  batch 800 loss: 1.013926875591278
  batch 850 loss: 1.0021314072608947
  batch 900 loss: 1.020542446374893
LOSS train 1.02054 valid 1.08026, valid PER 33.89%
EPOCH 7:
  batch 50 loss: 0.9816109848022461
  batch 100 loss: 1.0028335213661195
  batch 150 loss: 0.9555825114250183
  batch 200 loss: 0.9615411508083344
  batch 250 loss: 0.9569052243232727
  batch 300 loss: 0.9577655327320099
  batch 350 loss: 0.9795583391189575
  batch 400 loss: 0.968830120563507
  batch 450 loss: 0.9760046184062958
  batch 500 loss: 0.9596017265319824
  batch 550 loss: 0.9678643333911896
  batch 600 loss: 0.9881894135475159
  batch 650 loss: 0.9351670491695404
  batch 700 loss: 0.9806313240528106
  batch 750 loss: 0.9467481303215027
  batch 800 loss: 0.9693532490730286
  batch 850 loss: 0.9788050854206085
  batch 900 loss: 0.987272995710373
LOSS train 0.98727 valid 1.05243, valid PER 32.32%
EPOCH 8:
  batch 50 loss: 0.9265101075172424
  batch 100 loss: 0.927202332019806
  batch 150 loss: 0.9148481833934784
  batch 200 loss: 0.9038113760948181
  batch 250 loss: 0.9406489706039429
  batch 300 loss: 0.8823324406147003
  batch 350 loss: 0.9637315380573273
  batch 400 loss: 0.918518226146698
  batch 450 loss: 0.9355756855010986
  batch 500 loss: 0.9732667434215546
  batch 550 loss: 0.90446608543396
  batch 600 loss: 0.9523885524272919
  batch 650 loss: 0.9667261326313019
  batch 700 loss: 0.9162701761722565
  batch 750 loss: 0.9246998345851898
  batch 800 loss: 0.9427679789066314
  batch 850 loss: 0.9148551762104035
  batch 900 loss: 0.9368977189064026
LOSS train 0.93690 valid 1.01816, valid PER 32.29%
EPOCH 9:
  batch 50 loss: 0.8634776258468628
  batch 100 loss: 0.8972590744495392
  batch 150 loss: 0.8941021287441253
  batch 200 loss: 0.856292439699173
  batch 250 loss: 0.8904019570350648
  batch 300 loss: 0.9030732834339141
  batch 350 loss: 0.9299477827548981
  batch 400 loss: 0.890376638174057
  batch 450 loss: 0.8929304909706116
  batch 500 loss: 0.8646049916744232
  batch 550 loss: 0.907176923751831
  batch 600 loss: 0.9092946112155914
  batch 650 loss: 0.8717391872406006
  batch 700 loss: 0.8740837621688843
  batch 750 loss: 0.8900168550014496
  batch 800 loss: 0.9147030603885651
  batch 850 loss: 0.9143049669265747
  batch 900 loss: 0.8766916608810424
LOSS train 0.87669 valid 0.99053, valid PER 30.52%
EPOCH 10:
  batch 50 loss: 0.8315060305595398
  batch 100 loss: 0.8431121289730072
  batch 150 loss: 0.8650748705863953
  batch 200 loss: 0.8825143039226532
  batch 250 loss: 0.8616691195964813
  batch 300 loss: 0.8426968777179717
  batch 350 loss: 0.8638050603866577
  batch 400 loss: 0.8287348628044129
  batch 450 loss: 0.8436586594581604
  batch 500 loss: 0.8734025597572327
  batch 550 loss: 0.8939563429355621
  batch 600 loss: 0.8482316613197327
  batch 650 loss: 0.8392679440975189
  batch 700 loss: 0.8769679200649262
  batch 750 loss: 0.8454589247703552
  batch 800 loss: 0.8777209591865539
  batch 850 loss: 0.8597177648544312
  batch 900 loss: 0.8743419814109802
LOSS train 0.87434 valid 0.97760, valid PER 30.18%
EPOCH 11:
  batch 50 loss: 0.8026541376113892
  batch 100 loss: 0.7749044930934906
  batch 150 loss: 0.804079076051712
  batch 200 loss: 0.8454532045125961
  batch 250 loss: 0.8593750888109207
  batch 300 loss: 0.8247353518009186
  batch 350 loss: 0.8444610285758972
  batch 400 loss: 0.846387529373169
  batch 450 loss: 0.8338428580760956
  batch 500 loss: 0.8078328406810761
  batch 550 loss: 0.8249763000011444
  batch 600 loss: 0.8213540184497833
  batch 650 loss: 0.8728808617591858
  batch 700 loss: 0.8039862167835236
  batch 750 loss: 0.8179624366760254
  batch 800 loss: 0.8428024625778199
  batch 850 loss: 0.8621986711025238
  batch 900 loss: 0.8611146986484528
LOSS train 0.86111 valid 0.98484, valid PER 30.07%
EPOCH 12:
  batch 50 loss: 0.7952112138271332
  batch 100 loss: 0.7796495586633683
  batch 150 loss: 0.7628006529808045
  batch 200 loss: 0.7956414663791657
  batch 250 loss: 0.8001837253570556
  batch 300 loss: 0.7886121022701263
  batch 350 loss: 0.7787289488315582
  batch 400 loss: 0.8127535796165466
  batch 450 loss: 0.8154379016160965
  batch 500 loss: 0.8330993211269379
  batch 550 loss: 0.7572129774093628
  batch 600 loss: 0.8113939428329467
  batch 650 loss: 0.8432211887836456
  batch 700 loss: 0.8060076534748077
  batch 750 loss: 0.8026383972167969
  batch 800 loss: 0.7853573262691498
  batch 850 loss: 0.8487286448478699
  batch 900 loss: 0.8430302321910859
LOSS train 0.84303 valid 0.97127, valid PER 29.73%
EPOCH 13:
  batch 50 loss: 0.747638714313507
  batch 100 loss: 0.7795700705051423
  batch 150 loss: 0.7566811060905456
  batch 200 loss: 0.7883786547183991
  batch 250 loss: 0.769987513422966
  batch 300 loss: 0.7537211596965789
  batch 350 loss: 0.7696178531646729
  batch 400 loss: 0.7750400078296661
  batch 450 loss: 0.7939973974227905
  batch 500 loss: 0.748041615486145
  batch 550 loss: 0.7890650963783264
  batch 600 loss: 0.7682860946655273
  batch 650 loss: 0.7955341374874115
  batch 700 loss: 0.7946623635292053
  batch 750 loss: 0.7706252586841583
  batch 800 loss: 0.7811776959896087
  batch 850 loss: 0.8167660021781922
  batch 900 loss: 0.8123173892498017
LOSS train 0.81232 valid 0.94923, valid PER 29.34%
EPOCH 14:
  batch 50 loss: 0.7307603120803833
  batch 100 loss: 0.7520448392629624
  batch 150 loss: 0.7446478629112243
  batch 200 loss: 0.732295982837677
  batch 250 loss: 0.7504413646459579
  batch 300 loss: 0.786965879201889
  batch 350 loss: 0.7423198109865189
  batch 400 loss: 0.7442424321174621
  batch 450 loss: 0.7432929253578187
  batch 500 loss: 0.7670414823293686
  batch 550 loss: 0.7808520841598511
  batch 600 loss: 0.7310061353445053
  batch 650 loss: 0.7775785636901855
  batch 700 loss: 0.7934114754199981
  batch 750 loss: 0.7447736048698426
  batch 800 loss: 0.7310225468873978
  batch 850 loss: 0.7886866283416748
  batch 900 loss: 0.7663517266511917
LOSS train 0.76635 valid 0.96132, valid PER 29.62%
EPOCH 15:
  batch 50 loss: 0.7253035295009613
  batch 100 loss: 0.7231644797325134
  batch 150 loss: 0.7331606191396713
  batch 200 loss: 0.7274163496494294
  batch 250 loss: 0.7377637803554535
  batch 300 loss: 0.7171969699859619
  batch 350 loss: 0.7249551129341125
  batch 400 loss: 0.7303161025047302
  batch 450 loss: 0.7286990970373154
  batch 500 loss: 0.7167737829685211
  batch 550 loss: 0.7315656816959382
  batch 600 loss: 0.7374857771396637
  batch 650 loss: 0.755897274017334
  batch 700 loss: 0.7527024030685425
  batch 750 loss: 0.7702295994758606
  batch 800 loss: 0.7242145562171936
  batch 850 loss: 0.7150248146057129
  batch 900 loss: 0.7542596369981766
LOSS train 0.75426 valid 0.96019, valid PER 29.24%
EPOCH 16:
  batch 50 loss: 0.7356554704904557
  batch 100 loss: 0.6661667698621749
  batch 150 loss: 0.6889947032928467
  batch 200 loss: 0.699565578699112
  batch 250 loss: 0.6992720365524292
  batch 300 loss: 0.70422169983387
  batch 350 loss: 0.7234498310089111
  batch 400 loss: 0.7264000487327575
  batch 450 loss: 0.7292253828048706
  batch 500 loss: 0.6880773019790649
  batch 550 loss: 0.736160899400711
  batch 600 loss: 0.7038765996694565
  batch 650 loss: 0.746942378282547
  batch 700 loss: 0.7100710725784302
  batch 750 loss: 0.7165225231647492
  batch 800 loss: 0.7453527212142944
  batch 850 loss: 0.7212655133008957
  batch 900 loss: 0.7289235508441925
LOSS train 0.72892 valid 0.95301, valid PER 29.06%
EPOCH 17:
  batch 50 loss: 0.6954097330570221
  batch 100 loss: 0.6690380334854126
  batch 150 loss: 0.6752574801445007
  batch 200 loss: 0.6755480086803436
  batch 250 loss: 0.7025074905157089
  batch 300 loss: 0.7016886889934539
  batch 350 loss: 0.6760826104879379
  batch 400 loss: 0.7241571772098542
  batch 450 loss: 0.7122405040264129
  batch 500 loss: 0.6835418754816055
  batch 550 loss: 0.7029202234745026
  batch 600 loss: 0.7350408607721328
  batch 650 loss: 0.6981502258777619
  batch 700 loss: 0.6988709777593612
  batch 750 loss: 0.6759190273284912
  batch 800 loss: 0.6779522186517716
  batch 850 loss: 0.7019552648067474
  batch 900 loss: 0.6891070145368576
LOSS train 0.68911 valid 0.95595, valid PER 28.18%
EPOCH 18:
  batch 50 loss: 0.6536765193939209
  batch 100 loss: 0.667112535238266
  batch 150 loss: 0.6894557654857636
  batch 200 loss: 0.6852184623479843
  batch 250 loss: 0.6689208900928497
  batch 300 loss: 0.6528192871809005
  batch 350 loss: 0.6784084540605545
  batch 400 loss: 0.6608110266923904
  batch 450 loss: 0.692711089849472
  batch 500 loss: 0.6677286267280579
  batch 550 loss: 0.6827421391010284
  batch 600 loss: 0.6627101463079452
  batch 650 loss: 0.6619755810499192
  batch 700 loss: 0.6893685722351074
  batch 750 loss: 0.6608269894123078
  batch 800 loss: 0.6812723577022552
  batch 850 loss: 0.6719368749856949
  batch 900 loss: 0.7082709681987762
LOSS train 0.70827 valid 0.95252, valid PER 28.04%
EPOCH 19:
  batch 50 loss: 0.6056982207298279
  batch 100 loss: 0.6109763586521149
  batch 150 loss: 0.6320204907655715
  batch 200 loss: 0.6525413727760315
  batch 250 loss: 0.6595457577705384
  batch 300 loss: 0.6524417787790299
  batch 350 loss: 0.6572156375646592
  batch 400 loss: 0.6493750995397568
  batch 450 loss: 0.6791797077655792
  batch 500 loss: 0.6461300849914551
  batch 550 loss: 0.6348445719480514
  batch 600 loss: 0.6619309413433075
  batch 650 loss: 0.6876835858821869
  batch 700 loss: 0.6329400008916855
  batch 750 loss: 0.6553184539079666
  batch 800 loss: 0.6835369038581848
  batch 850 loss: 0.6763220280408859
  batch 900 loss: 0.6854911780357361
LOSS train 0.68549 valid 0.96749, valid PER 28.28%
EPOCH 20:
  batch 50 loss: 0.6139505100250244
  batch 100 loss: 0.631182867884636
  batch 150 loss: 0.6223255431652069
  batch 200 loss: 0.6324125534296036
  batch 250 loss: 0.6233385521173477
  batch 300 loss: 0.6504948782920837
  batch 350 loss: 0.6178252649307251
  batch 400 loss: 0.6376866745948792
  batch 450 loss: 0.6310633951425553
  batch 500 loss: 0.6191207456588745
  batch 550 loss: 0.6799218565225601
  batch 600 loss: 0.626611779332161
  batch 650 loss: 0.6911253523826599
  batch 700 loss: 0.6585462802648544
  batch 750 loss: 0.6337780743837357
  batch 800 loss: 0.6645563924312592
  batch 850 loss: 0.6653320729732514
  batch 900 loss: 0.664090918302536
LOSS train 0.66409 valid 0.96816, valid PER 28.60%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_224836/model_13
Loading model from checkpoints/20230117_224836/model_13
SUB: 15.95%, DEL: 13.71%, INS: 1.79%, COR: 70.35%, PER: 31.45%
