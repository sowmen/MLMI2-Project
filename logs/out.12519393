Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.5)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 8.602445678710938
  batch 100 loss: 3.178349771499634
  batch 150 loss: 3.0343790197372438
  batch 200 loss: 2.9176892328262327
  batch 250 loss: 2.8168184900283815
  batch 300 loss: 2.6277492570877077
  batch 350 loss: 2.4582271814346313
  batch 400 loss: 2.3763361740112305
  batch 450 loss: 2.3089233303070067
  batch 500 loss: 2.172418656349182
  batch 550 loss: 2.0465543913841246
  batch 600 loss: 1.9884740328788757
  batch 650 loss: 1.9130554628372192
  batch 700 loss: 1.9077206158638
  batch 750 loss: 1.8361897993087768
  batch 800 loss: 1.8012045216560364
  batch 850 loss: 1.759748420715332
  batch 900 loss: 1.7387660837173462
LOSS train 1.73877 valid 1.70732, valid PER 62.59%
EPOCH 2:
  batch 50 loss: 1.6767660856246949
  batch 100 loss: 1.6345106482505798
  batch 150 loss: 1.5867434000968934
  batch 200 loss: 1.6199330735206603
  batch 250 loss: 1.5854287433624268
  batch 300 loss: 1.55445556640625
  batch 350 loss: 1.4847494769096374
  batch 400 loss: 1.5005970811843872
  batch 450 loss: 1.4523062777519227
  batch 500 loss: 1.4855077838897706
  batch 550 loss: 1.4837540698051452
  batch 600 loss: 1.412649290561676
  batch 650 loss: 1.4448063278198242
  batch 700 loss: 1.419852921962738
  batch 750 loss: 1.3768079161643982
  batch 800 loss: 1.3512427401542664
  batch 850 loss: 1.3493764972686768
  batch 900 loss: 1.377234194278717
LOSS train 1.37723 valid 1.35424, valid PER 44.39%
EPOCH 3:
  batch 50 loss: 1.322657504081726
  batch 100 loss: 1.2996435809135436
  batch 150 loss: 1.2983455109596251
  batch 200 loss: 1.2878659391403198
  batch 250 loss: 1.2752497315406799
  batch 300 loss: 1.2618459379673004
  batch 350 loss: 1.3115609288215637
  batch 400 loss: 1.2670044136047363
  batch 450 loss: 1.250849974155426
  batch 500 loss: 1.2422551584243775
  batch 550 loss: 1.253910391330719
  batch 600 loss: 1.2184269452095031
  batch 650 loss: 1.2036656141281128
  batch 700 loss: 1.2326210939884186
  batch 750 loss: 1.2771506416797638
  batch 800 loss: 1.202201874256134
  batch 850 loss: 1.236314754486084
  batch 900 loss: 1.1843846035003662
LOSS train 1.18438 valid 1.28203, valid PER 40.63%
EPOCH 4:
  batch 50 loss: 1.1633063793182372
  batch 100 loss: 1.1955704951286317
  batch 150 loss: 1.1366616702079773
  batch 200 loss: 1.1671175050735474
  batch 250 loss: 1.1820441913604736
  batch 300 loss: 1.1779842019081115
  batch 350 loss: 1.142696611881256
  batch 400 loss: 1.1735679888725281
  batch 450 loss: 1.1453577148914338
  batch 500 loss: 1.1281011915206909
  batch 550 loss: 1.1636301112174987
  batch 600 loss: 1.1758206737041474
  batch 650 loss: 1.1527849185466765
  batch 700 loss: 1.1229611623287201
  batch 750 loss: 1.1204237926006317
  batch 800 loss: 1.100614458322525
  batch 850 loss: 1.13877969622612
  batch 900 loss: 1.1417549192905425
LOSS train 1.14175 valid 1.15611, valid PER 36.72%
EPOCH 5:
  batch 50 loss: 1.084410514831543
  batch 100 loss: 1.067907383441925
  batch 150 loss: 1.1277902770042418
  batch 200 loss: 1.0498221385478974
  batch 250 loss: 1.06819300532341
  batch 300 loss: 1.0877846813201903
  batch 350 loss: 1.0941085028648376
  batch 400 loss: 1.080626403093338
  batch 450 loss: 1.0753575873374939
  batch 500 loss: 1.088431987762451
  batch 550 loss: 1.0464789748191834
  batch 600 loss: 1.1102293348312378
  batch 650 loss: 1.0752048707008361
  batch 700 loss: 1.1019710743427276
  batch 750 loss: 1.0351767599582673
  batch 800 loss: 1.0618149173259734
  batch 850 loss: 1.0618698918819427
  batch 900 loss: 1.0621642827987672
LOSS train 1.06216 valid 1.09867, valid PER 35.27%
EPOCH 6:
  batch 50 loss: 1.0434418797492981
  batch 100 loss: 1.0088463258743285
  batch 150 loss: 0.987508225440979
  batch 200 loss: 1.027450783252716
  batch 250 loss: 1.0534805858135223
  batch 300 loss: 1.0234018635749818
  batch 350 loss: 1.0340727913379668
  batch 400 loss: 1.0174170517921448
  batch 450 loss: 1.050750458240509
  batch 500 loss: 1.029817681312561
  batch 550 loss: 1.043120231628418
  batch 600 loss: 1.014113210439682
  batch 650 loss: 1.029070076942444
  batch 700 loss: 1.0105982887744904
  batch 750 loss: 1.0015142822265626
  batch 800 loss: 0.9942908954620361
  batch 850 loss: 0.9934895813465119
  batch 900 loss: 1.0118382477760315
LOSS train 1.01184 valid 1.07400, valid PER 34.52%
EPOCH 7:
  batch 50 loss: 0.9808617818355561
  batch 100 loss: 0.9926259207725525
  batch 150 loss: 0.979617463350296
  batch 200 loss: 0.9603070259094239
  batch 250 loss: 0.9594241833686828
  batch 300 loss: 0.9580552077293396
  batch 350 loss: 0.9545699465274811
  batch 400 loss: 0.994377384185791
  batch 450 loss: 0.9844829070568085
  batch 500 loss: 0.9535820686817169
  batch 550 loss: 0.9655360984802246
  batch 600 loss: 0.9597096753120422
  batch 650 loss: 0.9678997647762299
  batch 700 loss: 0.9756932735443116
  batch 750 loss: 0.9552096080780029
  batch 800 loss: 0.957667475938797
  batch 850 loss: 0.9631114542484284
  batch 900 loss: 0.9979529845714569
LOSS train 0.99795 valid 1.04711, valid PER 33.54%
EPOCH 8:
  batch 50 loss: 0.9236551237106323
  batch 100 loss: 0.9350706601142883
  batch 150 loss: 0.9286525869369506
  batch 200 loss: 0.8966783857345582
  batch 250 loss: 0.9283927452564239
  batch 300 loss: 0.8763437342643737
  batch 350 loss: 0.9554787468910217
  batch 400 loss: 0.9098613405227661
  batch 450 loss: 0.9449452435970307
  batch 500 loss: 0.9703836917877198
  batch 550 loss: 0.9227551817893982
  batch 600 loss: 0.9745934402942658
  batch 650 loss: 0.9834923577308655
  batch 700 loss: 0.9102907776832581
  batch 750 loss: 0.9285507988929749
  batch 800 loss: 0.9283480846881866
  batch 850 loss: 0.9207076728343964
  batch 900 loss: 0.9258946323394776
LOSS train 0.92589 valid 1.02032, valid PER 32.46%
EPOCH 9:
  batch 50 loss: 0.8583237028121948
  batch 100 loss: 0.8822342121601104
  batch 150 loss: 0.9390441930294037
  batch 200 loss: 0.8810324108600617
  batch 250 loss: 0.9129737889766694
  batch 300 loss: 0.8976245224475861
  batch 350 loss: 0.9200999748706817
  batch 400 loss: 0.8932845902442932
  batch 450 loss: 0.9069755184650421
  batch 500 loss: 0.8758900463581085
  batch 550 loss: 0.9197191965579986
  batch 600 loss: 0.9362992537021637
  batch 650 loss: 0.8950961112976075
  batch 700 loss: 0.889761700630188
  batch 750 loss: 0.8843125426769256
  batch 800 loss: 0.9220948624610901
  batch 850 loss: 0.9349076795578003
  batch 900 loss: 0.9059469819068908
LOSS train 0.90595 valid 1.03701, valid PER 32.07%
EPOCH 10:
  batch 50 loss: 0.8577097296714783
  batch 100 loss: 0.8673456013202667
  batch 150 loss: 0.8748715162277222
  batch 200 loss: 0.8944335460662842
  batch 250 loss: 0.8920792472362519
  batch 300 loss: 0.8545704972743988
  batch 350 loss: 0.8950559055805206
  batch 400 loss: 0.8524030447006226
  batch 450 loss: 0.8468541991710663
  batch 500 loss: 0.8768053889274597
  batch 550 loss: 0.8795483577251434
  batch 600 loss: 0.8530836796760559
  batch 650 loss: 0.8548511564731598
  batch 700 loss: 0.8825763916969299
  batch 750 loss: 0.859962432384491
  batch 800 loss: 0.8758817362785339
  batch 850 loss: 0.8737695229053497
  batch 900 loss: 0.884733556509018
LOSS train 0.88473 valid 1.00975, valid PER 31.53%
EPOCH 11:
  batch 50 loss: 0.8329855346679688
  batch 100 loss: 0.8018918371200562
  batch 150 loss: 0.8080384230613709
  batch 200 loss: 0.8609973001480102
  batch 250 loss: 0.8407874464988708
  batch 300 loss: 0.8201229727268219
  batch 350 loss: 0.8352205240726471
  batch 400 loss: 0.8587396836280823
  batch 450 loss: 0.8488054263591767
  batch 500 loss: 0.8315400290489197
  batch 550 loss: 0.8389433205127717
  batch 600 loss: 0.8277806580066681
  batch 650 loss: 0.8940432059764862
  batch 700 loss: 0.8149431133270264
  batch 750 loss: 0.8258340716361999
  batch 800 loss: 0.8754597759246826
  batch 850 loss: 0.8693013036251068
  batch 900 loss: 0.8734322535991669
LOSS train 0.87343 valid 0.98980, valid PER 30.34%
EPOCH 12:
  batch 50 loss: 0.8367387807369232
  batch 100 loss: 0.7985197460651398
  batch 150 loss: 0.7966612899303436
  batch 200 loss: 0.8177254420518875
  batch 250 loss: 0.8121557986736297
  batch 300 loss: 0.8186411619186401
  batch 350 loss: 0.8145926570892335
  batch 400 loss: 0.8277892851829529
  batch 450 loss: 0.8382374954223633
  batch 500 loss: 0.8513471567630768
  batch 550 loss: 0.7808380031585693
  batch 600 loss: 0.8069902813434601
  batch 650 loss: 0.838506201505661
  batch 700 loss: 0.8438923144340515
  batch 750 loss: 0.8023961162567139
  batch 800 loss: 0.8057287836074829
  batch 850 loss: 0.8481815230846405
  batch 900 loss: 0.851164630651474
LOSS train 0.85116 valid 0.97698, valid PER 31.06%
EPOCH 13:
  batch 50 loss: 0.7835552203655243
  batch 100 loss: 0.8088820171356201
  batch 150 loss: 0.7736116707324981
  batch 200 loss: 0.79576824426651
  batch 250 loss: 0.7926730412244797
  batch 300 loss: 0.7732394325733185
  batch 350 loss: 0.8001330745220184
  batch 400 loss: 0.8235380017757415
  batch 450 loss: 0.7987467217445373
  batch 500 loss: 0.7707748651504517
  batch 550 loss: 0.7865785253047943
  batch 600 loss: 0.791842337846756
  batch 650 loss: 0.8003618520498276
  batch 700 loss: 0.8132634353637695
  batch 750 loss: 0.7655790936946869
  batch 800 loss: 0.785561900138855
  batch 850 loss: 0.8265951490402221
  batch 900 loss: 0.8261770558357239
LOSS train 0.82618 valid 0.95720, valid PER 29.89%
EPOCH 14:
  batch 50 loss: 0.7593796879053116
  batch 100 loss: 0.7698090076446533
  batch 150 loss: 0.7566348898410797
  batch 200 loss: 0.7716675329208375
  batch 250 loss: 0.7544526684284211
  batch 300 loss: 0.81126345038414
  batch 350 loss: 0.7594546997547149
  batch 400 loss: 0.7735212373733521
  batch 450 loss: 0.7765865993499755
  batch 500 loss: 0.7896522486209869
  batch 550 loss: 0.7935001730918885
  batch 600 loss: 0.7503026676177978
  batch 650 loss: 0.7900514853000641
  batch 700 loss: 0.8020539557933808
  batch 750 loss: 0.753954246044159
  batch 800 loss: 0.7464231312274933
  batch 850 loss: 0.791816691160202
  batch 900 loss: 0.7810556936264038
LOSS train 0.78106 valid 0.95845, valid PER 30.42%
EPOCH 15:
  batch 50 loss: 0.7340220248699189
  batch 100 loss: 0.7211558932065963
  batch 150 loss: 0.7437541735172272
  batch 200 loss: 0.7738649368286132
  batch 250 loss: 0.760865045785904
  batch 300 loss: 0.7286298900842667
  batch 350 loss: 0.7448016953468323
  batch 400 loss: 0.7299075317382813
  batch 450 loss: 0.7450828182697297
  batch 500 loss: 0.7227994918823242
  batch 550 loss: 0.764074319601059
  batch 600 loss: 0.7767514383792877
  batch 650 loss: 0.7759322166442871
  batch 700 loss: 0.7697324013710022
  batch 750 loss: 0.7799880981445313
  batch 800 loss: 0.7536793351173401
  batch 850 loss: 0.7219798564910889
  batch 900 loss: 0.7541980063915252
LOSS train 0.75420 valid 0.94343, valid PER 29.55%
EPOCH 16:
  batch 50 loss: 0.7152605772018432
  batch 100 loss: 0.7137727689743042
  batch 150 loss: 0.7242588204145431
  batch 200 loss: 0.7162965536117554
  batch 250 loss: 0.7471027421951294
  batch 300 loss: 0.7402102220058441
  batch 350 loss: 0.7668057721853256
  batch 400 loss: 0.7406954246759415
  batch 450 loss: 0.755807603597641
  batch 500 loss: 0.7073105037212372
  batch 550 loss: 0.730546373128891
  batch 600 loss: 0.7239201307296753
  batch 650 loss: 0.7563151669502258
  batch 700 loss: 0.7309720814228058
  batch 750 loss: 0.7387555485963821
  batch 800 loss: 0.7422196567058563
  batch 850 loss: 0.7345026481151581
  batch 900 loss: 0.7387283158302307
LOSS train 0.73873 valid 0.94537, valid PER 29.69%
EPOCH 17:
  batch 50 loss: 0.6970030295848847
  batch 100 loss: 0.6981685799360275
  batch 150 loss: 0.6831297743320465
  batch 200 loss: 0.6838974368572235
  batch 250 loss: 0.7326502764225006
  batch 300 loss: 0.7257941699028015
  batch 350 loss: 0.6883252418041229
  batch 400 loss: 0.7435313874483108
  batch 450 loss: 0.7348107886314392
  batch 500 loss: 0.6990078914165497
  batch 550 loss: 0.7066201436519622
  batch 600 loss: 0.7617092120647431
  batch 650 loss: 0.7071604347229004
  batch 700 loss: 0.7109199059009552
  batch 750 loss: 0.6850241178274155
  batch 800 loss: 0.6966982746124267
  batch 850 loss: 0.7283852362632751
  batch 900 loss: 0.7067428737878799
LOSS train 0.70674 valid 0.95700, valid PER 29.05%
EPOCH 18:
  batch 50 loss: 0.6782495588064194
  batch 100 loss: 0.7000718659162521
  batch 150 loss: 0.7035906422138214
  batch 200 loss: 0.6871033805608749
  batch 250 loss: 0.6944457530975342
  batch 300 loss: 0.6733405858278274
  batch 350 loss: 0.693618796467781
  batch 400 loss: 0.6675724065303803
  batch 450 loss: 0.7246717023849487
  batch 500 loss: 0.6978955882787704
  batch 550 loss: 0.7061351186037064
  batch 600 loss: 0.687758714556694
  batch 650 loss: 0.6849088776111603
  batch 700 loss: 0.7184263211488724
  batch 750 loss: 0.6985554522275925
  batch 800 loss: 0.7046071124076844
  batch 850 loss: 0.6987049490213394
  batch 900 loss: 0.7268493139743805
LOSS train 0.72685 valid 0.94808, valid PER 29.11%
EPOCH 19:
  batch 50 loss: 0.6486576664447784
  batch 100 loss: 0.637389994263649
  batch 150 loss: 0.6691199213266372
  batch 200 loss: 0.6708077085018158
  batch 250 loss: 0.6948219448328018
  batch 300 loss: 0.7122324979305268
  batch 350 loss: 0.6763847321271896
  batch 400 loss: 0.668211926817894
  batch 450 loss: 0.6810988283157349
  batch 500 loss: 0.6918770730495453
  batch 550 loss: 0.6714100539684296
  batch 600 loss: 0.6801966214179993
  batch 650 loss: 0.7264981812238693
  batch 700 loss: 0.670920177102089
  batch 750 loss: 0.6542668712139129
  batch 800 loss: 0.6959862625598907
  batch 850 loss: 0.7058715373277664
  batch 900 loss: 0.6966793102025985
LOSS train 0.69668 valid 0.94556, valid PER 28.53%
EPOCH 20:
  batch 50 loss: 0.6345308434963226
  batch 100 loss: 0.6515552687644959
  batch 150 loss: 0.6331425076723098
  batch 200 loss: 0.628668549656868
  batch 250 loss: 0.6503277754783631
  batch 300 loss: 0.6614133220911026
  batch 350 loss: 0.6398225265741349
  batch 400 loss: 0.6720046591758728
  batch 450 loss: 0.6681152379512787
  batch 500 loss: 0.6354858285188675
  batch 550 loss: 0.7158947342634201
  batch 600 loss: 0.6437827545404434
  batch 650 loss: 0.6697379511594772
  batch 700 loss: 0.6781813853979111
  batch 750 loss: 0.666160084605217
  batch 800 loss: 0.6719516670703888
  batch 850 loss: 0.6913553059101105
  batch 900 loss: 0.6898137152194976
LOSS train 0.68981 valid 0.95252, valid PER 29.09%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_225731/model_15
Loading model from checkpoints/20230117_225731/model_15
SUB: 17.28%, DEL: 10.98%, INS: 2.66%, COR: 71.74%, PER: 30.92%
