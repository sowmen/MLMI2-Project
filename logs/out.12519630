Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=2.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 9.176946978569031
  batch 100 loss: 3.118271298408508
  batch 150 loss: 3.0007866191864014
  batch 200 loss: 2.8815792322158815
  batch 250 loss: 2.815644869804382
  batch 300 loss: 2.6495562744140626
  batch 350 loss: 2.487391653060913
  batch 400 loss: 2.3992882108688356
  batch 450 loss: 2.3267239999771117
  batch 500 loss: 2.2302272176742552
  batch 550 loss: 2.0813251090049745
  batch 600 loss: 2.02975478887558
  batch 650 loss: 1.9247896552085877
  batch 700 loss: 1.904567470550537
  batch 750 loss: 1.8299488830566406
  batch 800 loss: 1.8050373482704163
  batch 850 loss: 1.7536419320106507
  batch 900 loss: 1.728422372341156
LOSS train 1.72842 valid 1.71214, valid PER 64.70%
EPOCH 2:
  batch 50 loss: 1.6833691239356994
  batch 100 loss: 1.6428767871856689
  batch 150 loss: 1.594504156112671
  batch 200 loss: 1.6151195359230042
  batch 250 loss: 1.6037316584587098
  batch 300 loss: 1.5817762875556947
  batch 350 loss: 1.4923340892791748
  batch 400 loss: 1.5116348671913147
  batch 450 loss: 1.4511227083206177
  batch 500 loss: 1.4770858931541442
  batch 550 loss: 1.4864250540733337
  batch 600 loss: 1.4304310512542724
  batch 650 loss: 1.4357641458511352
  batch 700 loss: 1.4200429892539979
  batch 750 loss: 1.3925743103027344
  batch 800 loss: 1.341296215057373
  batch 850 loss: 1.341650733947754
  batch 900 loss: 1.3748818826675415
LOSS train 1.37488 valid 1.36413, valid PER 45.62%
EPOCH 3:
  batch 50 loss: 1.334070818424225
  batch 100 loss: 1.2987690925598145
  batch 150 loss: 1.2953709697723388
  batch 200 loss: 1.2768401527404785
  batch 250 loss: 1.2660395359992982
  batch 300 loss: 1.246442003250122
  batch 350 loss: 1.2982673645019531
  batch 400 loss: 1.257962783575058
  batch 450 loss: 1.2553633975982665
  batch 500 loss: 1.2378022015094756
  batch 550 loss: 1.251883889436722
  batch 600 loss: 1.223111355304718
  batch 650 loss: 1.204166921377182
  batch 700 loss: 1.2246340191364289
  batch 750 loss: 1.2686807477474213
  batch 800 loss: 1.1840089321136475
  batch 850 loss: 1.2176070618629455
  batch 900 loss: 1.168491415977478
LOSS train 1.16849 valid 1.23142, valid PER 39.16%
EPOCH 4:
  batch 50 loss: 1.1562550806999206
  batch 100 loss: 1.1649779105186462
  batch 150 loss: 1.1390144228935242
  batch 200 loss: 1.1664215731620788
  batch 250 loss: 1.1708852338790894
  batch 300 loss: 1.183972066640854
  batch 350 loss: 1.1139207768440247
  batch 400 loss: 1.154284816980362
  batch 450 loss: 1.1353073751926421
  batch 500 loss: 1.1248490393161774
  batch 550 loss: 1.1452379941940307
  batch 600 loss: 1.152883199453354
  batch 650 loss: 1.1237689816951753
  batch 700 loss: 1.1216372048854828
  batch 750 loss: 1.1090451967716217
  batch 800 loss: 1.0754522049427033
  batch 850 loss: 1.1293312215805054
  batch 900 loss: 1.1499582743644714
LOSS train 1.14996 valid 1.14656, valid PER 37.10%
EPOCH 5:
  batch 50 loss: 1.064666051864624
  batch 100 loss: 1.061328682899475
  batch 150 loss: 1.1195538055896759
  batch 200 loss: 1.0389283967018128
  batch 250 loss: 1.0627026629447938
  batch 300 loss: 1.0799598455429078
  batch 350 loss: 1.0820863461494445
  batch 400 loss: 1.0730812406539918
  batch 450 loss: 1.0686163425445556
  batch 500 loss: 1.0766880786418915
  batch 550 loss: 1.0236973404884337
  batch 600 loss: 1.1043194234371185
  batch 650 loss: 1.0552696323394775
  batch 700 loss: 1.0869379556179046
  batch 750 loss: 1.0339213860034944
  batch 800 loss: 1.0601932036876678
  batch 850 loss: 1.0588204491138458
  batch 900 loss: 1.0756359338760375
LOSS train 1.07564 valid 1.11042, valid PER 35.38%
EPOCH 6:
  batch 50 loss: 1.050409404039383
  batch 100 loss: 0.998639349937439
  batch 150 loss: 0.9901945221424103
  batch 200 loss: 1.01892316699028
  batch 250 loss: 1.0440417587757111
  batch 300 loss: 1.0267557847499846
  batch 350 loss: 1.0220627093315124
  batch 400 loss: 0.9947731935977936
  batch 450 loss: 1.0435854613780975
  batch 500 loss: 1.0280942583084107
  batch 550 loss: 1.0408466053009033
  batch 600 loss: 1.0072746992111206
  batch 650 loss: 1.0375696444511413
  batch 700 loss: 1.019016568660736
  batch 750 loss: 0.9990881824493408
  batch 800 loss: 1.0022700572013854
  batch 850 loss: 0.9841235852241517
  batch 900 loss: 1.0093222963809967
LOSS train 1.00932 valid 1.06608, valid PER 34.11%
EPOCH 7:
  batch 50 loss: 0.9908852183818817
  batch 100 loss: 1.0057265055179596
  batch 150 loss: 0.9758793544769288
  batch 200 loss: 0.9560144805908203
  batch 250 loss: 0.964122760295868
  batch 300 loss: 0.9583629107475281
  batch 350 loss: 0.9604782795906067
  batch 400 loss: 0.984043642282486
  batch 450 loss: 0.9808589589595794
  batch 500 loss: 0.9589405918121338
  batch 550 loss: 0.9606662082672119
  batch 600 loss: 0.9709067928791046
  batch 650 loss: 0.9500098741054535
  batch 700 loss: 0.9742867994308472
  batch 750 loss: 0.9444762444496155
  batch 800 loss: 0.9575407195091248
  batch 850 loss: 0.9738393199443817
  batch 900 loss: 1.0130174136161805
LOSS train 1.01302 valid 1.03730, valid PER 33.18%
EPOCH 8:
  batch 50 loss: 0.9509788608551025
  batch 100 loss: 0.9344610929489136
  batch 150 loss: 0.9265039360523224
  batch 200 loss: 0.9173626124858856
  batch 250 loss: 0.9487063646316528
  batch 300 loss: 0.8637247240543365
  batch 350 loss: 0.9501338875293732
  batch 400 loss: 0.9053597748279572
  batch 450 loss: 0.945935914516449
  batch 500 loss: 0.9734317588806153
  batch 550 loss: 0.9152324044704437
  batch 600 loss: 0.9542987871170044
  batch 650 loss: 0.982848869562149
  batch 700 loss: 0.9220744073390961
  batch 750 loss: 0.9337548696994782
  batch 800 loss: 0.949720777273178
  batch 850 loss: 0.9071470081806183
  batch 900 loss: 0.9145546591281891
LOSS train 0.91455 valid 1.00859, valid PER 31.98%
EPOCH 9:
  batch 50 loss: 0.8672659170627594
  batch 100 loss: 0.9120102417469025
  batch 150 loss: 0.927677264213562
  batch 200 loss: 0.8694851350784302
  batch 250 loss: 0.9054859972000122
  batch 300 loss: 0.9239652717113495
  batch 350 loss: 0.924608188867569
  batch 400 loss: 0.9120843350887299
  batch 450 loss: 0.8933825528621674
  batch 500 loss: 0.8733143043518067
  batch 550 loss: 0.9189674663543701
  batch 600 loss: 0.9383153879642486
  batch 650 loss: 0.8827132153511047
  batch 700 loss: 0.8812345933914184
  batch 750 loss: 0.8955029761791229
  batch 800 loss: 0.908071619272232
  batch 850 loss: 0.9095651471614837
  batch 900 loss: 0.877431173324585
LOSS train 0.87743 valid 0.99717, valid PER 31.65%
EPOCH 10:
  batch 50 loss: 0.8335242629051208
  batch 100 loss: 0.8621591603755951
  batch 150 loss: 0.887201521396637
  batch 200 loss: 0.8994325947761536
  batch 250 loss: 0.8829496908187866
  batch 300 loss: 0.8514098846912384
  batch 350 loss: 0.8734375655651092
  batch 400 loss: 0.8291083312034607
  batch 450 loss: 0.8196826004981994
  batch 500 loss: 0.8935898125171662
  batch 550 loss: 0.8951765251159668
  batch 600 loss: 0.8909279942512512
  batch 650 loss: 0.8749086749553681
  batch 700 loss: 0.9070018792152404
  batch 750 loss: 0.8709168088436127
  batch 800 loss: 0.8706800079345703
  batch 850 loss: 0.9092550802230835
  batch 900 loss: 0.8827035510540009
LOSS train 0.88270 valid 0.98850, valid PER 30.98%
EPOCH 11:
  batch 50 loss: 0.8212567567825317
  batch 100 loss: 0.8078529131412506
  batch 150 loss: 0.8260554432868957
  batch 200 loss: 0.8849574947357177
  batch 250 loss: 0.856491152048111
  batch 300 loss: 0.8164592981338501
  batch 350 loss: 0.8426605367660522
  batch 400 loss: 0.8682488119602203
  batch 450 loss: 0.8528811109066009
  batch 500 loss: 0.8263687705993652
  batch 550 loss: 0.8457114517688751
  batch 600 loss: 0.8199419093132019
  batch 650 loss: 0.9029157376289367
  batch 700 loss: 0.8177079558372498
  batch 750 loss: 0.8461660581827164
  batch 800 loss: 0.857233716249466
  batch 850 loss: 0.8708008277416229
  batch 900 loss: 0.8750059461593628
LOSS train 0.87501 valid 0.98097, valid PER 30.40%
EPOCH 12:
  batch 50 loss: 0.8271522939205169
  batch 100 loss: 0.7961697959899903
  batch 150 loss: 0.7912824308872223
  batch 200 loss: 0.8210035359859467
  batch 250 loss: 0.8223386836051941
  batch 300 loss: 0.8004408645629882
  batch 350 loss: 0.8058827447891236
  batch 400 loss: 0.8401824688911438
  batch 450 loss: 0.8278592765331269
  batch 500 loss: 0.8562592041492462
  batch 550 loss: 0.766093773841858
  batch 600 loss: 0.8067108917236329
  batch 650 loss: 0.8602211546897888
  batch 700 loss: 0.8357127368450165
  batch 750 loss: 0.8107413351535797
  batch 800 loss: 0.8029555541276931
  batch 850 loss: 0.84561039686203
  batch 900 loss: 0.8466397333145141
LOSS train 0.84664 valid 0.96148, valid PER 30.26%
EPOCH 13:
  batch 50 loss: 0.7836589336395263
  batch 100 loss: 0.8090556824207306
  batch 150 loss: 0.7684407782554626
  batch 200 loss: 0.8008919095993042
  batch 250 loss: 0.7782693558931351
  batch 300 loss: 0.795786469578743
  batch 350 loss: 0.7978179490566254
  batch 400 loss: 0.8115919423103333
  batch 450 loss: 0.8051799702644348
  batch 500 loss: 0.7765747284889222
  batch 550 loss: 0.806708745956421
  batch 600 loss: 0.7895752191543579
  batch 650 loss: 0.7905484646558761
  batch 700 loss: 0.8295040893554687
  batch 750 loss: 0.788376680612564
  batch 800 loss: 0.7894396770000458
  batch 850 loss: 0.8338816809654236
  batch 900 loss: 0.8293634343147278
LOSS train 0.82936 valid 0.94894, valid PER 30.01%
EPOCH 14:
  batch 50 loss: 0.7597269690036774
  batch 100 loss: 0.7647798800468445
  batch 150 loss: 0.7573372101783753
  batch 200 loss: 0.7708446687459946
  batch 250 loss: 0.7851153182983398
  batch 300 loss: 0.8035152685642243
  batch 350 loss: 0.7579802215099335
  batch 400 loss: 0.7540681970119476
  batch 450 loss: 0.7778791105747223
  batch 500 loss: 0.7888988852500916
  batch 550 loss: 0.7947386908531189
  batch 600 loss: 0.7493670392036438
  batch 650 loss: 0.7963887488842011
  batch 700 loss: 0.8036544620990753
  batch 750 loss: 0.7498512589931488
  batch 800 loss: 0.7551958012580872
  batch 850 loss: 0.7813147664070129
  batch 900 loss: 0.7783337724208832
LOSS train 0.77833 valid 0.95300, valid PER 30.18%
EPOCH 15:
  batch 50 loss: 0.7480686271190643
  batch 100 loss: 0.7389320427179337
  batch 150 loss: 0.7387015664577484
  batch 200 loss: 0.7602017259597779
  batch 250 loss: 0.7672168624401092
  batch 300 loss: 0.751791764497757
  batch 350 loss: 0.7527818727493286
  batch 400 loss: 0.7341809809207916
  batch 450 loss: 0.7523426413536072
  batch 500 loss: 0.7375246918201447
  batch 550 loss: 0.7446945214271545
  batch 600 loss: 0.7773709714412689
  batch 650 loss: 0.7825191694498063
  batch 700 loss: 0.7772427463531494
  batch 750 loss: 0.7752274632453918
  batch 800 loss: 0.7769804102182388
  batch 850 loss: 0.7437408459186554
  batch 900 loss: 0.7619685447216034
LOSS train 0.76197 valid 0.95670, valid PER 29.70%
EPOCH 16:
  batch 50 loss: 0.7502537631988525
  batch 100 loss: 0.7084602570533752
  batch 150 loss: 0.7291860860586167
  batch 200 loss: 0.7215175700187683
  batch 250 loss: 0.7396207487583161
  batch 300 loss: 0.729114801287651
  batch 350 loss: 0.7497641086578369
  batch 400 loss: 0.7563599562644958
  batch 450 loss: 0.7480599182844162
  batch 500 loss: 0.7326156693696976
  batch 550 loss: 0.7359583801031113
  batch 600 loss: 0.7370667231082916
  batch 650 loss: 0.7486986768245697
  batch 700 loss: 0.7230990648269653
  batch 750 loss: 0.746566323041916
  batch 800 loss: 0.7656755363941192
  batch 850 loss: 0.7353010207414628
  batch 900 loss: 0.7343214666843414
LOSS train 0.73432 valid 0.95482, valid PER 29.68%
EPOCH 17:
  batch 50 loss: 0.7008135968446731
  batch 100 loss: 0.7140290611982345
  batch 150 loss: 0.6866001069545746
  batch 200 loss: 0.7015922880172729
  batch 250 loss: 0.7245756340026855
  batch 300 loss: 0.7263117307424545
  batch 350 loss: 0.7057004153728486
  batch 400 loss: 0.7492926251888276
  batch 450 loss: 0.7309366583824157
  batch 500 loss: 0.7002895653247834
  batch 550 loss: 0.7261240720748902
  batch 600 loss: 0.7740767621994018
  batch 650 loss: 0.7262626051902771
  batch 700 loss: 0.7172221350669861
  batch 750 loss: 0.6934715813398361
  batch 800 loss: 0.7049683618545532
  batch 850 loss: 0.719418259859085
  batch 900 loss: 0.6815142804384231
LOSS train 0.68151 valid 0.93762, valid PER 28.48%
EPOCH 18:
  batch 50 loss: 0.693760656118393
  batch 100 loss: 0.6911251777410508
  batch 150 loss: 0.7114274233579636
  batch 200 loss: 0.6958097642660142
  batch 250 loss: 0.7140210020542145
  batch 300 loss: 0.6674613678455352
  batch 350 loss: 0.7020875072479248
  batch 400 loss: 0.6827992713451385
  batch 450 loss: 0.7305694735050201
  batch 500 loss: 0.7029876631498336
  batch 550 loss: 0.704524599313736
  batch 600 loss: 0.6902446228265763
  batch 650 loss: 0.6930532085895539
  batch 700 loss: 0.7311185276508332
  batch 750 loss: 0.6882345533370972
  batch 800 loss: 0.6960604226589203
  batch 850 loss: 0.6977291578054428
  batch 900 loss: 0.7123469614982605
LOSS train 0.71235 valid 0.93204, valid PER 28.85%
EPOCH 19:
  batch 50 loss: 0.6426644325256348
  batch 100 loss: 0.6391956323385238
  batch 150 loss: 0.6727803546190262
  batch 200 loss: 0.6758621710538865
  batch 250 loss: 0.6926509815454484
  batch 300 loss: 0.6826727372407914
  batch 350 loss: 0.6785416132211686
  batch 400 loss: 0.6792286562919617
  batch 450 loss: 0.6915603107213975
  batch 500 loss: 0.7014959502220154
  batch 550 loss: 0.6695769888162613
  batch 600 loss: 0.6857267642021179
  batch 650 loss: 0.7424423521757126
  batch 700 loss: 0.674369370341301
  batch 750 loss: 0.6790130656957626
  batch 800 loss: 0.7066394847631454
  batch 850 loss: 0.7019486796855926
  batch 900 loss: 0.680570832490921
LOSS train 0.68057 valid 0.95032, valid PER 28.90%
EPOCH 20:
  batch 50 loss: 0.6563912153244018
  batch 100 loss: 0.6645055520534515
  batch 150 loss: 0.6421846240758896
  batch 200 loss: 0.6617607665061951
  batch 250 loss: 0.6603401333093644
  batch 300 loss: 0.6766817730665207
  batch 350 loss: 0.6470958530902863
  batch 400 loss: 0.6680654615163804
  batch 450 loss: 0.6687632131576539
  batch 500 loss: 0.6358559715747834
  batch 550 loss: 0.715356251001358
  batch 600 loss: 0.6465768766403198
  batch 650 loss: 0.6845940345525742
  batch 700 loss: 0.6801273339986801
  batch 750 loss: 0.6821577471494674
  batch 800 loss: 0.7089533913135528
  batch 850 loss: 0.6944566714763641
  batch 900 loss: 0.6926873677968979
LOSS train 0.69269 valid 0.94573, valid PER 28.86%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230117_230636/model_18
Loading model from checkpoints/20230117_230636/model_18
SUB: 16.36%, DEL: 11.76%, INS: 2.20%, COR: 71.88%, PER: 30.32%
