Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.17323851108551
  batch 100 loss: 3.300068802833557
  batch 150 loss: 3.313449912071228
  batch 200 loss: 2.992075033187866
  batch 250 loss: 2.784232020378113
  batch 300 loss: 2.570692629814148
  batch 350 loss: 2.406174311637878
  batch 400 loss: 2.355821623802185
  batch 450 loss: 2.2687226271629335
  batch 500 loss: 2.149152522087097
  batch 550 loss: 2.1000707173347473
  batch 600 loss: 2.025191278457642
  batch 650 loss: 1.985160586833954
  batch 700 loss: 1.9478721499443055
  batch 750 loss: 1.8648662495613098
  batch 800 loss: 1.8416196775436402
  batch 850 loss: 1.8109892725944519
  batch 900 loss: 1.783674955368042
LOSS train 1.78367 valid 1.73064, valid PER 62.82%
EPOCH 2:
  batch 50 loss: 1.7407830023765565
  batch 100 loss: 1.661671931743622
  batch 150 loss: 1.6487787580490112
  batch 200 loss: 1.6542121434211732
  batch 250 loss: 1.6555891680717467
  batch 300 loss: 1.6086882066726684
  batch 350 loss: 1.522656786441803
  batch 400 loss: 1.5526249384880066
  batch 450 loss: 1.5097588658332826
  batch 500 loss: 1.5350076675415039
  batch 550 loss: 1.543543837070465
  batch 600 loss: 1.4621954560279846
  batch 650 loss: 1.504303102493286
  batch 700 loss: 1.4756344151496887
  batch 750 loss: 1.4596836948394776
  batch 800 loss: 1.3973610997200012
  batch 850 loss: 1.4071364426612853
  batch 900 loss: 1.4252214312553406
LOSS train 1.42522 valid 1.41363, valid PER 45.92%
EPOCH 3:
  batch 50 loss: 1.3854589104652404
  batch 100 loss: 1.3491503596305847
  batch 150 loss: 1.3503028011322022
  batch 200 loss: 1.3305397653579711
  batch 250 loss: 1.3109602558612823
  batch 300 loss: 1.317015631198883
  batch 350 loss: 1.3512367606163025
  batch 400 loss: 1.3453636837005616
  batch 450 loss: 1.3103906321525574
  batch 500 loss: 1.2862870335578918
  batch 550 loss: 1.2961882185935973
  batch 600 loss: 1.2670045697689056
  batch 650 loss: 1.248380604982376
  batch 700 loss: 1.2720602107048036
  batch 750 loss: 1.3283090257644654
  batch 800 loss: 1.2465728080272676
  batch 850 loss: 1.2714836812019348
  batch 900 loss: 1.1869783806800842
LOSS train 1.18698 valid 1.29330, valid PER 39.20%
EPOCH 4:
  batch 50 loss: 1.197364708185196
  batch 100 loss: 1.1951985514163972
  batch 150 loss: 1.171800651550293
  batch 200 loss: 1.1891127550601959
  batch 250 loss: 1.2073720216751098
  batch 300 loss: 1.1927026033401489
  batch 350 loss: 1.1328214061260224
  batch 400 loss: 1.159468014240265
  batch 450 loss: 1.1638461720943452
  batch 500 loss: 1.1384888195991516
  batch 550 loss: 1.1699886214733124
  batch 600 loss: 1.187253794670105
  batch 650 loss: 1.165141363143921
  batch 700 loss: 1.1261133444309235
  batch 750 loss: 1.1178566229343414
  batch 800 loss: 1.0773563957214356
  batch 850 loss: 1.1184728491306304
  batch 900 loss: 1.155759004354477
LOSS train 1.15576 valid 1.15109, valid PER 34.96%
EPOCH 5:
  batch 50 loss: 1.0703097200393676
  batch 100 loss: 1.0661010563373565
  batch 150 loss: 1.1247776544094086
  batch 200 loss: 1.0308337020874023
  batch 250 loss: 1.0573180305957794
  batch 300 loss: 1.076218228340149
  batch 350 loss: 1.0582072114944459
  batch 400 loss: 1.0655697286128998
  batch 450 loss: 1.0547344136238097
  batch 500 loss: 1.0718551313877105
  batch 550 loss: 1.0161563229560853
  batch 600 loss: 1.1050068628787995
  batch 650 loss: 1.0489879369735717
  batch 700 loss: 1.088861005306244
  batch 750 loss: 1.0097277212142943
  batch 800 loss: 1.0636702954769135
  batch 850 loss: 1.033702621459961
  batch 900 loss: 1.0419978153705598
LOSS train 1.04200 valid 1.09088, valid PER 33.50%
EPOCH 6:
  batch 50 loss: 1.0255656039714813
  batch 100 loss: 0.9837413311004639
  batch 150 loss: 0.9771641111373901
  batch 200 loss: 0.9921870625019074
  batch 250 loss: 1.0221717965602874
  batch 300 loss: 0.9981908857822418
  batch 350 loss: 0.9975460946559906
  batch 400 loss: 0.9851029276847839
  batch 450 loss: 1.017826954126358
  batch 500 loss: 0.988751403093338
  batch 550 loss: 1.014457414150238
  batch 600 loss: 0.969319771528244
  batch 650 loss: 0.9965270662307739
  batch 700 loss: 0.9994794523715973
  batch 750 loss: 0.9699772012233734
  batch 800 loss: 0.978469831943512
  batch 850 loss: 0.9768418741226196
  batch 900 loss: 0.9884181463718414
LOSS train 0.98842 valid 1.05572, valid PER 32.74%
EPOCH 7:
  batch 50 loss: 0.9475616323947906
  batch 100 loss: 0.951870150566101
  batch 150 loss: 0.9396467959880829
  batch 200 loss: 0.9302640676498413
  batch 250 loss: 0.925845627784729
  batch 300 loss: 0.9135727417469025
  batch 350 loss: 0.9500410866737365
  batch 400 loss: 0.9394630336761475
  batch 450 loss: 0.9430594265460968
  batch 500 loss: 0.9293247878551483
  batch 550 loss: 0.9267717278003693
  batch 600 loss: 0.9424188923835755
  batch 650 loss: 0.9245785188674926
  batch 700 loss: 0.9489574682712555
  batch 750 loss: 0.9231018054485322
  batch 800 loss: 0.9247632348537445
  batch 850 loss: 0.9529952692985535
  batch 900 loss: 0.96770463347435
LOSS train 0.96770 valid 1.03385, valid PER 31.50%
EPOCH 8:
  batch 50 loss: 0.8766566455364228
  batch 100 loss: 0.8875016069412232
  batch 150 loss: 0.879416583776474
  batch 200 loss: 0.8607802832126618
  batch 250 loss: 0.885759037733078
  batch 300 loss: 0.8450526356697082
  batch 350 loss: 0.9183674311637878
  batch 400 loss: 0.8807875859737396
  batch 450 loss: 0.9031291699409485
  batch 500 loss: 0.9263331317901611
  batch 550 loss: 0.8540698254108429
  batch 600 loss: 0.8933644104003906
  batch 650 loss: 0.9317376089096069
  batch 700 loss: 0.8734583818912506
  batch 750 loss: 0.9020264279842377
  batch 800 loss: 0.9031833446025849
  batch 850 loss: 0.8926782310009003
  batch 900 loss: 0.9021486055850982
LOSS train 0.90215 valid 1.01851, valid PER 30.71%
EPOCH 9:
  batch 50 loss: 0.8015916752815246
  batch 100 loss: 0.8342119705677032
  batch 150 loss: 0.8638510549068451
  batch 200 loss: 0.8134699141979218
  batch 250 loss: 0.8499677193164825
  batch 300 loss: 0.8685861885547638
  batch 350 loss: 0.880765790939331
  batch 400 loss: 0.8577805995941162
  batch 450 loss: 0.9011575877666473
  batch 500 loss: 0.876685950756073
  batch 550 loss: 0.9030099129676818
  batch 600 loss: 0.9014791333675385
  batch 650 loss: 0.865514235496521
  batch 700 loss: 0.8323416543006897
  batch 750 loss: 0.8567922341823578
  batch 800 loss: 0.881693251132965
  batch 850 loss: 0.9025828468799592
  batch 900 loss: 0.843056725859642
LOSS train 0.84306 valid 0.99645, valid PER 30.70%
EPOCH 10:
  batch 50 loss: 0.7629359138011932
  batch 100 loss: 0.8024949610233307
  batch 150 loss: 0.8374363505840301
  batch 200 loss: 0.8303093576431274
  batch 250 loss: 0.8365619432926178
  batch 300 loss: 0.8010315799713135
  batch 350 loss: 0.8359021008014679
  batch 400 loss: 0.7855580139160157
  batch 450 loss: 0.7990536439418793
  batch 500 loss: 0.8390463399887085
  batch 550 loss: 0.8499882388114929
  batch 600 loss: 0.818295704126358
  batch 650 loss: 0.8010257804393768
  batch 700 loss: 0.8510479927062988
  batch 750 loss: 0.8127417898178101
  batch 800 loss: 0.8233624172210693
  batch 850 loss: 0.841806948184967
  batch 900 loss: 0.835150979757309
LOSS train 0.83515 valid 1.00672, valid PER 31.42%
EPOCH 11:
  batch 50 loss: 0.7168676948547363
  batch 100 loss: 0.6843761217594146
  batch 150 loss: 0.6928075826168061
  batch 200 loss: 0.7335974222421646
  batch 250 loss: 0.7181295883655548
  batch 300 loss: 0.7001867717504502
  batch 350 loss: 0.7203204846382141
  batch 400 loss: 0.7295345777273178
  batch 450 loss: 0.7219832015037536
  batch 500 loss: 0.698757819533348
  batch 550 loss: 0.7100709223747254
  batch 600 loss: 0.7073659038543701
  batch 650 loss: 0.7587227594852447
  batch 700 loss: 0.6841850197315216
  batch 750 loss: 0.6980844390392303
  batch 800 loss: 0.748251736164093
  batch 850 loss: 0.7437186980247498
  batch 900 loss: 0.7377072894573211
LOSS train 0.73771 valid 0.93965, valid PER 28.14%
EPOCH 12:
  batch 50 loss: 0.6594864737987518
  batch 100 loss: 0.6687322860956192
  batch 150 loss: 0.6452244639396667
  batch 200 loss: 0.6787703734636307
  batch 250 loss: 0.6961588501930237
  batch 300 loss: 0.681643933057785
  batch 350 loss: 0.652275470495224
  batch 400 loss: 0.7076026546955109
  batch 450 loss: 0.6967587780952453
  batch 500 loss: 0.7041690063476562
  batch 550 loss: 0.652148008942604
  batch 600 loss: 0.676392239332199
  batch 650 loss: 0.713230197429657
  batch 700 loss: 0.7011722201108932
  batch 750 loss: 0.6843214303255081
  batch 800 loss: 0.6890701621770858
  batch 850 loss: 0.7431208366155624
  batch 900 loss: 0.7275432819128036
LOSS train 0.72754 valid 0.93956, valid PER 28.90%
EPOCH 13:
  batch 50 loss: 0.6289549815654755
  batch 100 loss: 0.6352724295854568
  batch 150 loss: 0.6173251473903656
  batch 200 loss: 0.6357537931203843
  batch 250 loss: 0.6182263433933258
  batch 300 loss: 0.6081432783603669
  batch 350 loss: 0.6124068146944046
  batch 400 loss: 0.648317638039589
  batch 450 loss: 0.6334016406536103
  batch 500 loss: 0.6211970216035843
  batch 550 loss: 0.6512031656503677
  batch 600 loss: 0.619124846458435
  batch 650 loss: 0.6422981077432632
  batch 700 loss: 0.6656066054105758
  batch 750 loss: 0.5966775470972061
  batch 800 loss: 0.6272720891237259
  batch 850 loss: 0.6503599298000335
  batch 900 loss: 0.6588930928707123
LOSS train 0.65889 valid 0.93132, valid PER 27.74%
EPOCH 14:
  batch 50 loss: 0.6001055538654327
  batch 100 loss: 0.60819670855999
  batch 150 loss: 0.6110062873363495
  batch 200 loss: 0.6043973731994629
  batch 250 loss: 0.6232556742429733
  batch 300 loss: 0.6383145582675934
  batch 350 loss: 0.5837527698278427
  batch 400 loss: 0.6118797314167023
  batch 450 loss: 0.6108645194768906
  batch 500 loss: 0.6276426106691361
  batch 550 loss: 0.637618339061737
  batch 600 loss: 0.5825402116775513
  batch 650 loss: 0.6227507400512695
  batch 700 loss: 0.653743959069252
  batch 750 loss: 0.5908298116922378
  batch 800 loss: 0.5918623745441437
  batch 850 loss: 0.6407316356897355
  batch 900 loss: 0.6208126562833786
LOSS train 0.62081 valid 0.94205, valid PER 27.79%
EPOCH 15:
  batch 50 loss: 0.5841602343320846
  batch 100 loss: 0.5780273169279099
  batch 150 loss: 0.5875707358121872
  batch 200 loss: 0.600168201327324
  batch 250 loss: 0.6071172004938126
  batch 300 loss: 0.5589949607849121
  batch 350 loss: 0.5813088285923004
  batch 400 loss: 0.5857177203893662
  batch 450 loss: 0.5805835503339768
  batch 500 loss: 0.5650607788562775
  batch 550 loss: 0.5884539163112641
  batch 600 loss: 0.6010262691974639
  batch 650 loss: 0.5995275300741195
  batch 700 loss: 0.5961253201961517
  batch 750 loss: 0.5907065761089325
  batch 800 loss: 0.5745042812824249
  batch 850 loss: 0.55559346139431
  batch 900 loss: 0.5893265134096146
LOSS train 0.58933 valid 0.93230, valid PER 27.83%
EPOCH 16:
  batch 50 loss: 0.5853008556365967
  batch 100 loss: 0.5505394399166107
  batch 150 loss: 0.5620141184329986
  batch 200 loss: 0.5657326936721802
  batch 250 loss: 0.5863262957334519
  batch 300 loss: 0.5604602444171906
  batch 350 loss: 0.5705018591880798
  batch 400 loss: 0.5754996019601822
  batch 450 loss: 0.584178449511528
  batch 500 loss: 0.5496706187725067
  batch 550 loss: 0.553908469080925
  batch 600 loss: 0.5621253204345703
  batch 650 loss: 0.571970522403717
  batch 700 loss: 0.5415467816591263
  batch 750 loss: 0.5677608913183212
  batch 800 loss: 0.5730375170707702
  batch 850 loss: 0.5644088554382324
  batch 900 loss: 0.5613900917768478
LOSS train 0.56139 valid 0.93382, valid PER 27.66%
EPOCH 17:
  batch 50 loss: 0.5594242459535599
  batch 100 loss: 0.5763866019248962
  batch 150 loss: 0.5602897107601166
  batch 200 loss: 0.559960852265358
  batch 250 loss: 0.5730471819639206
  batch 300 loss: 0.5717624819278717
  batch 350 loss: 0.5329087483882904
  batch 400 loss: 0.5768326383829117
  batch 450 loss: 0.5635749924182892
  batch 500 loss: 0.5402388596534728
  batch 550 loss: 0.5507375305891037
  batch 600 loss: 0.5739296799898148
  batch 650 loss: 0.547664880156517
  batch 700 loss: 0.5472226047515869
  batch 750 loss: 0.5232291412353516
  batch 800 loss: 0.5543562787771225
  batch 850 loss: 0.5600502735376358
  batch 900 loss: 0.5427497726678848
LOSS train 0.54275 valid 0.93387, valid PER 27.60%
EPOCH 18:
  batch 50 loss: 0.5554353272914887
  batch 100 loss: 0.5614756160974502
  batch 150 loss: 0.5706991177797317
  batch 200 loss: 0.5559057891368866
  batch 250 loss: 0.5614295566082
  batch 300 loss: 0.5393946629762649
  batch 350 loss: 0.5454553073644638
  batch 400 loss: 0.5498828250169754
  batch 450 loss: 0.5663775807619095
  batch 500 loss: 0.5436142957210541
  batch 550 loss: 0.5628118580579757
  batch 600 loss: 0.5402727019786835
  batch 650 loss: 0.5248111891746521
  batch 700 loss: 0.557839537858963
  batch 750 loss: 0.5416485697031022
  batch 800 loss: 0.5511386716365814
  batch 850 loss: 0.5298629546165466
  batch 900 loss: 0.572667286992073
LOSS train 0.57267 valid 0.93368, valid PER 27.61%
EPOCH 19:
  batch 50 loss: 0.5373994970321655
  batch 100 loss: 0.5384977698326111
  batch 150 loss: 0.5492961895465851
  batch 200 loss: 0.5548887008428574
  batch 250 loss: 0.5612623232603073
  batch 300 loss: 0.5647126394510269
  batch 350 loss: 0.5447071182727814
  batch 400 loss: 0.5466902220249176
  batch 450 loss: 0.5441085296869278
  batch 500 loss: 0.5515372556447983
  batch 550 loss: 0.5384582018852234
  batch 600 loss: 0.542445389032364
  batch 650 loss: 0.5667638015747071
  batch 700 loss: 0.5395156413316726
  batch 750 loss: 0.5330326676368713
  batch 800 loss: 0.5501686918735504
  batch 850 loss: 0.5631084769964219
  batch 900 loss: 0.5491662287712097
LOSS train 0.54917 valid 0.93420, valid PER 27.64%
EPOCH 20:
  batch 50 loss: 0.5508288717269898
  batch 100 loss: 0.5385376912355423
  batch 150 loss: 0.5436964565515519
  batch 200 loss: 0.5542200970649719
  batch 250 loss: 0.5333405649662017
  batch 300 loss: 0.5596487796306611
  batch 350 loss: 0.5188114380836487
  batch 400 loss: 0.5491780662536621
  batch 450 loss: 0.5561859798431397
  batch 500 loss: 0.5369428867101669
  batch 550 loss: 0.5753508859872818
  batch 600 loss: 0.5318068170547485
  batch 650 loss: 0.5510332638025284
  batch 700 loss: 0.5515364503860474
  batch 750 loss: 0.5320080602169037
  batch 800 loss: 0.5658631402254105
  batch 850 loss: 0.556282851099968
  batch 900 loss: 0.545643727183342
LOSS train 0.54564 valid 0.93445, valid PER 27.60%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230118_090903/model_13
Loading model from checkpoints/20230118_090903/model_13
SUB: 16.27%, DEL: 10.65%, INS: 3.09%, COR: 73.08%, PER: 30.00%
