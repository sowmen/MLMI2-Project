Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.187516407966614
  batch 100 loss: 3.306412353515625
  batch 150 loss: 3.117492437362671
  batch 200 loss: 2.876417775154114
  batch 250 loss: 2.6841029739379882
  batch 300 loss: 2.5093112897872927
  batch 350 loss: 2.396456928253174
  batch 400 loss: 2.3474019384384155
  batch 450 loss: 2.255918974876404
  batch 500 loss: 2.1532729721069335
  batch 550 loss: 2.0797196936607363
  batch 600 loss: 2.0121093821525573
  batch 650 loss: 1.9203789520263672
  batch 700 loss: 1.9102597784996034
  batch 750 loss: 1.8508786463737488
  batch 800 loss: 1.8361537623405457
  batch 850 loss: 1.7841736626625062
  batch 900 loss: 1.7678625679016113
LOSS train 1.76786 valid 1.71082, valid PER 61.27%
EPOCH 2:
  batch 50 loss: 1.6938141989707947
  batch 100 loss: 1.6437109303474426
  batch 150 loss: 1.6194500184059144
  batch 200 loss: 1.6401193284988402
  batch 250 loss: 1.6377789855003357
  batch 300 loss: 1.6063452768325805
  batch 350 loss: 1.519353883266449
  batch 400 loss: 1.531778745651245
  batch 450 loss: 1.4811513018608093
  batch 500 loss: 1.512613344192505
  batch 550 loss: 1.4996080541610717
  batch 600 loss: 1.4741931056976318
  batch 650 loss: 1.5006879043579102
  batch 700 loss: 1.4512288427352906
  batch 750 loss: 1.4451134586334229
  batch 800 loss: 1.392958514690399
  batch 850 loss: 1.3785939455032348
  batch 900 loss: 1.4174773812294006
LOSS train 1.41748 valid 1.42729, valid PER 43.87%
EPOCH 3:
  batch 50 loss: 1.3796829891204834
  batch 100 loss: 1.3358905625343322
  batch 150 loss: 1.327732789516449
  batch 200 loss: 1.3018743348121644
  batch 250 loss: 1.3076507449150085
  batch 300 loss: 1.3002216744422912
  batch 350 loss: 1.345636053085327
  batch 400 loss: 1.2868744444847107
  batch 450 loss: 1.2832900667190552
  batch 500 loss: 1.2575051665306092
  batch 550 loss: 1.2644438779354095
  batch 600 loss: 1.236671814918518
  batch 650 loss: 1.213049362897873
  batch 700 loss: 1.219605153799057
  batch 750 loss: 1.2779826962947844
  batch 800 loss: 1.2049195683002472
  batch 850 loss: 1.2334561443328858
  batch 900 loss: 1.16578049659729
LOSS train 1.16578 valid 1.23117, valid PER 36.94%
EPOCH 4:
  batch 50 loss: 1.1688953745365143
  batch 100 loss: 1.18224786400795
  batch 150 loss: 1.1327202916145325
  batch 200 loss: 1.172035539150238
  batch 250 loss: 1.160895940065384
  batch 300 loss: 1.1889868402481079
  batch 350 loss: 1.1034117150306701
  batch 400 loss: 1.151106048822403
  batch 450 loss: 1.140031442642212
  batch 500 loss: 1.1182618570327758
  batch 550 loss: 1.1482052099704743
  batch 600 loss: 1.1510762059688568
  batch 650 loss: 1.1668225288391114
  batch 700 loss: 1.115122480392456
  batch 750 loss: 1.1054356563091279
  batch 800 loss: 1.0793076348304749
  batch 850 loss: 1.1055430150032044
  batch 900 loss: 1.1395839858055115
LOSS train 1.13958 valid 1.16375, valid PER 35.32%
EPOCH 5:
  batch 50 loss: 1.065578418970108
  batch 100 loss: 1.0691458141803742
  batch 150 loss: 1.1132584857940673
  batch 200 loss: 1.054459981918335
  batch 250 loss: 1.0742816710472107
  batch 300 loss: 1.0533159685134887
  batch 350 loss: 1.067347160577774
  batch 400 loss: 1.086687307357788
  batch 450 loss: 1.0546186530590058
  batch 500 loss: 1.07829984664917
  batch 550 loss: 1.009655660390854
  batch 600 loss: 1.0990096724033356
  batch 650 loss: 1.0753103065490723
  batch 700 loss: 1.0797262299060821
  batch 750 loss: 1.0084873640537262
  batch 800 loss: 1.0546007037162781
  batch 850 loss: 1.0630292093753815
  batch 900 loss: 1.053292329311371
LOSS train 1.05329 valid 1.10829, valid PER 34.73%
EPOCH 6:
  batch 50 loss: 1.0530688893795013
  batch 100 loss: 1.0000937390327453
  batch 150 loss: 0.9790971338748932
  batch 200 loss: 1.0047983062267303
  batch 250 loss: 1.0295636773109436
  batch 300 loss: 0.9903350341320037
  batch 350 loss: 1.0003789937496186
  batch 400 loss: 0.9933114767074585
  batch 450 loss: 1.0392375636100768
  batch 500 loss: 1.0029372906684875
  batch 550 loss: 1.02344566822052
  batch 600 loss: 0.9756094670295715
  batch 650 loss: 1.0014106523990631
  batch 700 loss: 0.9986260569095612
  batch 750 loss: 0.9900087058544159
  batch 800 loss: 0.9790451741218567
  batch 850 loss: 0.978740336894989
  batch 900 loss: 0.9931849777698517
LOSS train 0.99318 valid 1.05832, valid PER 33.39%
EPOCH 7:
  batch 50 loss: 0.9753079164028168
  batch 100 loss: 0.9849632251262664
  batch 150 loss: 0.9654672276973725
  batch 200 loss: 0.9468378067016602
  batch 250 loss: 0.9511846089363098
  batch 300 loss: 0.9393119657039642
  batch 350 loss: 0.9639333856105804
  batch 400 loss: 0.9456362128257751
  batch 450 loss: 0.9642997193336487
  batch 500 loss: 0.9425020253658295
  batch 550 loss: 0.9497520339488983
  batch 600 loss: 0.9410925579071044
  batch 650 loss: 0.9368038523197174
  batch 700 loss: 0.9633292376995086
  batch 750 loss: 0.9250002980232239
  batch 800 loss: 0.9303954076766968
  batch 850 loss: 0.9659436154365539
  batch 900 loss: 0.9945059382915497
LOSS train 0.99451 valid 1.02424, valid PER 32.78%
EPOCH 8:
  batch 50 loss: 0.9144403517246247
  batch 100 loss: 0.9249576997756958
  batch 150 loss: 0.9147957730293274
  batch 200 loss: 0.900347626209259
  batch 250 loss: 0.915848685503006
  batch 300 loss: 0.858211133480072
  batch 350 loss: 0.9399801552295685
  batch 400 loss: 0.9017593455314636
  batch 450 loss: 0.9177631080150604
  batch 500 loss: 0.943505403995514
  batch 550 loss: 0.8971868920326233
  batch 600 loss: 0.9232720494270324
  batch 650 loss: 0.945617390871048
  batch 700 loss: 0.9041468548774719
  batch 750 loss: 0.9048900353908539
  batch 800 loss: 0.9258168983459473
  batch 850 loss: 0.9233773910999298
  batch 900 loss: 0.9131637179851532
LOSS train 0.91316 valid 1.00351, valid PER 30.96%
EPOCH 9:
  batch 50 loss: 0.8487194013595581
  batch 100 loss: 0.8910851967334747
  batch 150 loss: 0.8885778975486756
  batch 200 loss: 0.8445767050981522
  batch 250 loss: 0.8866177880764008
  batch 300 loss: 0.8816540789604187
  batch 350 loss: 0.9034712874889373
  batch 400 loss: 0.895567946434021
  batch 450 loss: 0.8617377686500549
  batch 500 loss: 0.852575627565384
  batch 550 loss: 0.8968220639228821
  batch 600 loss: 0.9159766447544098
  batch 650 loss: 0.8772591173648834
  batch 700 loss: 0.8813090085983276
  batch 750 loss: 0.8640312016010284
  batch 800 loss: 0.9038296103477478
  batch 850 loss: 0.9117776727676392
  batch 900 loss: 0.8631040227413177
LOSS train 0.86310 valid 0.97714, valid PER 30.52%
EPOCH 10:
  batch 50 loss: 0.817635303735733
  batch 100 loss: 0.8391772818565368
  batch 150 loss: 0.8587758374214173
  batch 200 loss: 0.8559885156154633
  batch 250 loss: 0.8635029149055481
  batch 300 loss: 0.8134425628185272
  batch 350 loss: 0.8537920677661895
  batch 400 loss: 0.8091262066364289
  batch 450 loss: 0.8374962162971497
  batch 500 loss: 0.856334558725357
  batch 550 loss: 0.8624227046966553
  batch 600 loss: 0.8661817777156829
  batch 650 loss: 0.8479856050014496
  batch 700 loss: 0.8661006164550781
  batch 750 loss: 0.8431025660037994
  batch 800 loss: 0.8735280430316925
  batch 850 loss: 0.8635888755321502
  batch 900 loss: 0.8663395392894745
LOSS train 0.86634 valid 0.96995, valid PER 30.01%
EPOCH 11:
  batch 50 loss: 0.8015381848812103
  batch 100 loss: 0.776507112979889
  batch 150 loss: 0.7932047402858734
  batch 200 loss: 0.8426232266426087
  batch 250 loss: 0.8414834249019623
  batch 300 loss: 0.7996965324878693
  batch 350 loss: 0.8356694507598877
  batch 400 loss: 0.8463276898860932
  batch 450 loss: 0.8443066120147705
  batch 500 loss: 0.8164230847358703
  batch 550 loss: 0.812281438112259
  batch 600 loss: 0.8201092004776
  batch 650 loss: 0.8672075295448303
  batch 700 loss: 0.8181745171546936
  batch 750 loss: 0.8126861333847046
  batch 800 loss: 0.8467543816566467
  batch 850 loss: 0.8549810791015625
  batch 900 loss: 0.8540820014476777
LOSS train 0.85408 valid 0.95839, valid PER 30.06%
EPOCH 12:
  batch 50 loss: 0.8047817516326904
  batch 100 loss: 0.7770602262020111
  batch 150 loss: 0.760280932188034
  batch 200 loss: 0.7923705494403839
  batch 250 loss: 0.7919016313552857
  batch 300 loss: 0.7894843924045563
  batch 350 loss: 0.7947047770023346
  batch 400 loss: 0.8031581705808639
  batch 450 loss: 0.8101782095432282
  batch 500 loss: 0.8190157377719879
  batch 550 loss: 0.7545822030305862
  batch 600 loss: 0.7857786417007446
  batch 650 loss: 0.8317457258701324
  batch 700 loss: 0.8210152673721314
  batch 750 loss: 0.7958607995510101
  batch 800 loss: 0.8065005946159363
  batch 850 loss: 0.8342231106758118
  batch 900 loss: 0.8547871398925782
LOSS train 0.85479 valid 0.94519, valid PER 30.07%
EPOCH 13:
  batch 50 loss: 0.7614759671688079
  batch 100 loss: 0.7803579568862915
  batch 150 loss: 0.7523747432231903
  batch 200 loss: 0.771854521036148
  batch 250 loss: 0.7710098427534103
  batch 300 loss: 0.7531839537620545
  batch 350 loss: 0.7806071817874909
  batch 400 loss: 0.7904530036449432
  batch 450 loss: 0.7871165239810943
  batch 500 loss: 0.7641009557247161
  batch 550 loss: 0.791166958808899
  batch 600 loss: 0.7604696190357209
  batch 650 loss: 0.7977235507965088
  batch 700 loss: 0.7985132718086243
  batch 750 loss: 0.7488846242427826
  batch 800 loss: 0.7882717025279998
  batch 850 loss: 0.8009125435352326
  batch 900 loss: 0.81307741522789
LOSS train 0.81308 valid 0.96585, valid PER 29.79%
EPOCH 14:
  batch 50 loss: 0.7186828327178955
  batch 100 loss: 0.6961979722976684
  batch 150 loss: 0.6974340516328812
  batch 200 loss: 0.6835573101043702
  batch 250 loss: 0.6947973155975342
  batch 300 loss: 0.7238856089115143
  batch 350 loss: 0.6582130467891694
  batch 400 loss: 0.6804774522781372
  batch 450 loss: 0.684262957572937
  batch 500 loss: 0.7024550962448121
  batch 550 loss: 0.713903477191925
  batch 600 loss: 0.6695630621910095
  batch 650 loss: 0.6971448040008545
  batch 700 loss: 0.7200218665599823
  batch 750 loss: 0.6712906211614609
  batch 800 loss: 0.6495820939540863
  batch 850 loss: 0.6931312024593353
  batch 900 loss: 0.6995481216907501
LOSS train 0.69955 valid 0.92158, valid PER 28.20%
EPOCH 15:
  batch 50 loss: 0.6481065511703491
  batch 100 loss: 0.6478609275817871
  batch 150 loss: 0.6698600691556931
  batch 200 loss: 0.699652379155159
  batch 250 loss: 0.7061767023801804
  batch 300 loss: 0.6765364694595337
  batch 350 loss: 0.6755690574645996
  batch 400 loss: 0.673144268989563
  batch 450 loss: 0.6809185576438904
  batch 500 loss: 0.6528320920467376
  batch 550 loss: 0.6790111190080643
  batch 600 loss: 0.6995947366952896
  batch 650 loss: 0.6904274535179138
  batch 700 loss: 0.70066430747509
  batch 750 loss: 0.676974760890007
  batch 800 loss: 0.6584036183357239
  batch 850 loss: 0.6579860615730285
  batch 900 loss: 0.6666692405939102
LOSS train 0.66667 valid 0.91062, valid PER 27.43%
EPOCH 16:
  batch 50 loss: 0.6641195893287659
  batch 100 loss: 0.6283260655403137
  batch 150 loss: 0.630454266667366
  batch 200 loss: 0.6355910950899124
  batch 250 loss: 0.6642098718881607
  batch 300 loss: 0.6414566904306411
  batch 350 loss: 0.6548152422904968
  batch 400 loss: 0.6675962418317795
  batch 450 loss: 0.7034763503074646
  batch 500 loss: 0.635383352637291
  batch 550 loss: 0.6433194667100907
  batch 600 loss: 0.6499680799245834
  batch 650 loss: 0.6739852458238602
  batch 700 loss: 0.644258177280426
  batch 750 loss: 0.6696300727128982
  batch 800 loss: 0.6611838006973266
  batch 850 loss: 0.6409239286184311
  batch 900 loss: 0.6500676620006561
LOSS train 0.65007 valid 0.90722, valid PER 27.59%
EPOCH 17:
  batch 50 loss: 0.6392790615558624
  batch 100 loss: 0.6367626059055328
  batch 150 loss: 0.6204901915788651
  batch 200 loss: 0.6394003999233245
  batch 250 loss: 0.65358034491539
  batch 300 loss: 0.6270575517416
  batch 350 loss: 0.6285808336734772
  batch 400 loss: 0.6675376331806183
  batch 450 loss: 0.6415879303216934
  batch 500 loss: 0.6295086532831192
  batch 550 loss: 0.6435293841362
  batch 600 loss: 0.6752312332391739
  batch 650 loss: 0.6417392575740815
  batch 700 loss: 0.6213793456554413
  batch 750 loss: 0.6145491433143616
  batch 800 loss: 0.6186421942710877
  batch 850 loss: 0.6411110305786133
  batch 900 loss: 0.6198718899488449
LOSS train 0.61987 valid 0.91617, valid PER 27.26%
EPOCH 18:
  batch 50 loss: 0.5995227974653244
  batch 100 loss: 0.6098787766695023
  batch 150 loss: 0.6106890261173248
  batch 200 loss: 0.597999764084816
  batch 250 loss: 0.6069132572412491
  batch 300 loss: 0.5733022171258927
  batch 350 loss: 0.579376454949379
  batch 400 loss: 0.5766181153059006
  batch 450 loss: 0.603546758890152
  batch 500 loss: 0.585821967124939
  batch 550 loss: 0.6009269672632217
  batch 600 loss: 0.5717941135168075
  batch 650 loss: 0.5657722371816635
  batch 700 loss: 0.6045648330450057
  batch 750 loss: 0.5750545293092728
  batch 800 loss: 0.5765062046051025
  batch 850 loss: 0.5869696754217147
  batch 900 loss: 0.6077386635541916
LOSS train 0.60774 valid 0.90354, valid PER 27.16%
EPOCH 19:
  batch 50 loss: 0.5582757091522217
  batch 100 loss: 0.571988336443901
  batch 150 loss: 0.5808616542816162
  batch 200 loss: 0.5735980957746506
  batch 250 loss: 0.5986878430843353
  batch 300 loss: 0.5774458009004593
  batch 350 loss: 0.5696134060621262
  batch 400 loss: 0.576365619301796
  batch 450 loss: 0.6007649385929108
  batch 500 loss: 0.5777059423923493
  batch 550 loss: 0.5616497004032135
  batch 600 loss: 0.5678416049480438
  batch 650 loss: 0.6196120178699493
  batch 700 loss: 0.5621214747428894
  batch 750 loss: 0.569349907040596
  batch 800 loss: 0.5853403508663177
  batch 850 loss: 0.5920420527458191
  batch 900 loss: 0.5751878482103348
LOSS train 0.57519 valid 0.90496, valid PER 27.20%
EPOCH 20:
  batch 50 loss: 0.5562203848361968
  batch 100 loss: 0.536116035580635
  batch 150 loss: 0.5414607167243958
  batch 200 loss: 0.5393702167272568
  batch 250 loss: 0.538977821469307
  batch 300 loss: 0.556245733499527
  batch 350 loss: 0.5288307982683181
  batch 400 loss: 0.5498655295372009
  batch 450 loss: 0.5609850817918778
  batch 500 loss: 0.5256337833404541
  batch 550 loss: 0.5964221322536468
  batch 600 loss: 0.5253146213293075
  batch 650 loss: 0.5537522780895233
  batch 700 loss: 0.5506484538316727
  batch 750 loss: 0.5231560015678406
  batch 800 loss: 0.5708955484628677
  batch 850 loss: 0.5647649550437928
  batch 900 loss: 0.5598343282938003
LOSS train 0.55983 valid 0.90204, valid PER 26.82%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230118_091529/model_20
Loading model from checkpoints/20230118_091529/model_20
SUB: 16.25%, DEL: 9.45%, INS: 2.60%, COR: 74.29%, PER: 28.31%
