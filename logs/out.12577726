Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0, schedule='false')
cuda:0
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 4.277292304039001
  batch 100 loss: 3.3087278032302856
  batch 150 loss: 3.2447263050079345
  batch 200 loss: 3.119654989242554
  batch 250 loss: 2.933020534515381
  batch 300 loss: 2.7505629730224608
  batch 350 loss: 2.64783034324646
  batch 400 loss: 2.534235119819641
  batch 450 loss: 2.445901346206665
  batch 500 loss: 2.3097827649116516
  batch 550 loss: 2.25809592962265
  batch 600 loss: 2.1669187426567076
  batch 650 loss: 2.087771990299225
  batch 700 loss: 2.0736201119422915
  batch 750 loss: 1.9738467645645141
  batch 800 loss: 1.939933452606201
  batch 850 loss: 1.8916596055030823
  batch 900 loss: 1.8455157256126404
LOSS train 1.84552 valid 1.76231, valid PER 69.87%
EPOCH 2:
  batch 50 loss: 1.7665969467163085
  batch 100 loss: 1.6766507959365844
  batch 150 loss: 1.6612166738510132
  batch 200 loss: 1.6396455216407775
  batch 250 loss: 1.6294169759750365
  batch 300 loss: 1.5512922930717468
  batch 350 loss: 1.4808964943885803
  batch 400 loss: 1.4771238660812378
  batch 450 loss: 1.3987458777427673
  batch 500 loss: 1.4196457195281982
  batch 550 loss: 1.4273248624801635
  batch 600 loss: 1.3517935943603516
  batch 650 loss: 1.3864608383178711
  batch 700 loss: 1.3256719970703126
  batch 750 loss: 1.3135540544986726
  batch 800 loss: 1.2619991755485536
  batch 850 loss: 1.2619253313541412
  batch 900 loss: 1.2686616146564484
LOSS train 1.26866 valid 1.26233, valid PER 39.23%
EPOCH 3:
  batch 50 loss: 1.2048048841953278
  batch 100 loss: 1.2036321592330932
  batch 150 loss: 1.189551453590393
  batch 200 loss: 1.1773189175128937
  batch 250 loss: 1.1761315166950226
  batch 300 loss: 1.1481573283672333
  batch 350 loss: 1.191755495071411
  batch 400 loss: 1.1757004904747008
  batch 450 loss: 1.151804028749466
  batch 500 loss: 1.128077198266983
  batch 550 loss: 1.1423646533489227
  batch 600 loss: 1.106385509967804
  batch 650 loss: 1.0820112657546996
  batch 700 loss: 1.1012400364875794
  batch 750 loss: 1.1404365074634553
  batch 800 loss: 1.0840308797359466
  batch 850 loss: 1.1058026897907256
  batch 900 loss: 1.040460661649704
LOSS train 1.04046 valid 1.11257, valid PER 33.94%
EPOCH 4:
  batch 50 loss: 1.0180879986286164
  batch 100 loss: 1.0499645113945006
  batch 150 loss: 0.9914957249164581
  batch 200 loss: 1.0268813478946686
  batch 250 loss: 1.0657151246070862
  batch 300 loss: 1.0518417394161224
  batch 350 loss: 0.9663195049762726
  batch 400 loss: 1.011539123058319
  batch 450 loss: 0.9917784416675568
  batch 500 loss: 0.9898028600215912
  batch 550 loss: 1.0434539270401002
  batch 600 loss: 1.0242900264263153
  batch 650 loss: 1.000251944065094
  batch 700 loss: 0.9796768414974213
  batch 750 loss: 0.9761305642127991
  batch 800 loss: 0.9378398358821869
  batch 850 loss: 0.9480566906929017
  batch 900 loss: 1.0213474905490876
LOSS train 1.02135 valid 0.98458, valid PER 30.78%
EPOCH 5:
  batch 50 loss: 0.9189237487316132
  batch 100 loss: 0.911872512102127
  batch 150 loss: 0.9681216442584991
  batch 200 loss: 0.8915653371810913
  batch 250 loss: 0.9002135848999023
  batch 300 loss: 0.9206553947925568
  batch 350 loss: 0.8880313348770141
  batch 400 loss: 0.9234068322181702
  batch 450 loss: 0.9187319946289062
  batch 500 loss: 0.9176454091072083
  batch 550 loss: 0.8693231439590454
  batch 600 loss: 0.9413480269908905
  batch 650 loss: 0.9110673391819
  batch 700 loss: 0.9468059062957763
  batch 750 loss: 0.8732913088798523
  batch 800 loss: 0.9074848926067353
  batch 850 loss: 0.9018717741966248
  batch 900 loss: 0.886872992515564
LOSS train 0.88687 valid 0.93755, valid PER 29.07%
EPOCH 6:
  batch 50 loss: 0.8919925200939178
  batch 100 loss: 0.8370662701129913
  batch 150 loss: 0.8208760857582093
  batch 200 loss: 0.8432327711582184
  batch 250 loss: 0.8811672937870025
  batch 300 loss: 0.8639645063877106
  batch 350 loss: 0.8694963610172272
  batch 400 loss: 0.842996906042099
  batch 450 loss: 0.8511569046974182
  batch 500 loss: 0.8377671778202057
  batch 550 loss: 0.8683772122859955
  batch 600 loss: 0.8423807442188262
  batch 650 loss: 0.849166362285614
  batch 700 loss: 0.8467481470108033
  batch 750 loss: 0.8198906767368317
  batch 800 loss: 0.8457382261753082
  batch 850 loss: 0.8046604323387146
  batch 900 loss: 0.8235938000679016
LOSS train 0.82359 valid 0.92272, valid PER 28.11%
EPOCH 7:
  batch 50 loss: 0.8120692038536071
  batch 100 loss: 0.8347034406661987
  batch 150 loss: 0.7907533156871795
  batch 200 loss: 0.7784981346130371
  batch 250 loss: 0.7792861229181289
  batch 300 loss: 0.785658962726593
  batch 350 loss: 0.7983854508399963
  batch 400 loss: 0.7904209983348847
  batch 450 loss: 0.7864146775007248
  batch 500 loss: 0.8142418110370636
  batch 550 loss: 0.7894987964630127
  batch 600 loss: 0.7989465582370758
  batch 650 loss: 0.8058517599105834
  batch 700 loss: 0.8149861431121826
  batch 750 loss: 0.7756924450397491
  batch 800 loss: 0.786958532333374
  batch 850 loss: 0.797660995721817
  batch 900 loss: 0.8313033783435821
LOSS train 0.83130 valid 0.86801, valid PER 27.28%
EPOCH 8:
  batch 50 loss: 0.7596162390708924
  batch 100 loss: 0.7753888487815856
  batch 150 loss: 0.7585932368040085
  batch 200 loss: 0.7355574810504913
  batch 250 loss: 0.7656809651851654
  batch 300 loss: 0.726762667298317
  batch 350 loss: 0.778535088300705
  batch 400 loss: 0.7304550516605377
  batch 450 loss: 0.7418462479114533
  batch 500 loss: 0.7983441483974457
  batch 550 loss: 0.7330668431520462
  batch 600 loss: 0.7496106421947479
  batch 650 loss: 0.7677479994297027
  batch 700 loss: 0.7389242362976074
  batch 750 loss: 0.7599434638023377
  batch 800 loss: 0.7798351502418518
  batch 850 loss: 0.7557248878479004
  batch 900 loss: 0.7793541085720063
LOSS train 0.77935 valid 0.86401, valid PER 26.50%
EPOCH 9:
  batch 50 loss: 0.6748120641708374
  batch 100 loss: 0.7260966062545776
  batch 150 loss: 0.7091423612833023
  batch 200 loss: 0.6922375470399856
  batch 250 loss: 0.7419341915845871
  batch 300 loss: 0.7148505926132203
  batch 350 loss: 0.7438553184270859
  batch 400 loss: 0.714587323665619
  batch 450 loss: 0.726941192150116
  batch 500 loss: 0.7064846408367157
  batch 550 loss: 0.721748731136322
  batch 600 loss: 0.7328204560279846
  batch 650 loss: 0.7099316251277924
  batch 700 loss: 0.6876764345169067
  batch 750 loss: 0.699129883646965
  batch 800 loss: 0.7717731058597564
  batch 850 loss: 0.7705330193042755
  batch 900 loss: 0.6985771679878234
LOSS train 0.69858 valid 0.84177, valid PER 25.84%
EPOCH 10:
  batch 50 loss: 0.636770310997963
  batch 100 loss: 0.6414460718631745
  batch 150 loss: 0.6726609855890274
  batch 200 loss: 0.6781510293483735
  batch 250 loss: 0.6969534951448441
  batch 300 loss: 0.6881712728738785
  batch 350 loss: 0.7243893694877624
  batch 400 loss: 0.6538903856277466
  batch 450 loss: 0.6451049774885178
  batch 500 loss: 0.6869956904649734
  batch 550 loss: 0.7025961285829544
  batch 600 loss: 0.6616631501913071
  batch 650 loss: 0.6697689563035965
  batch 700 loss: 0.6750602823495865
  batch 750 loss: 0.6624240916967392
  batch 800 loss: 0.6899808430671692
  batch 850 loss: 0.7273103529214859
  batch 900 loss: 0.732424955368042
LOSS train 0.73242 valid 0.84969, valid PER 26.69%
EPOCH 11:
  batch 50 loss: 0.6214458656311035
  batch 100 loss: 0.6087771284580231
  batch 150 loss: 0.626261745095253
  batch 200 loss: 0.6653214091062546
  batch 250 loss: 0.6662092459201813
  batch 300 loss: 0.6266059631109238
  batch 350 loss: 0.635416579246521
  batch 400 loss: 0.6424711340665817
  batch 450 loss: 0.6596505159139633
  batch 500 loss: 0.6210408610105514
  batch 550 loss: 0.6327441799640655
  batch 600 loss: 0.6126469528675079
  batch 650 loss: 0.6754821252822876
  batch 700 loss: 0.6132224225997924
  batch 750 loss: 0.6272754782438278
  batch 800 loss: 0.6902036386728286
  batch 850 loss: 0.6703240567445755
  batch 900 loss: 0.6624679183959961
LOSS train 0.66247 valid 0.83560, valid PER 25.30%
EPOCH 12:
  batch 50 loss: 0.5803687995672226
  batch 100 loss: 0.592842031121254
  batch 150 loss: 0.5576673442125321
  batch 200 loss: 0.5869524562358857
  batch 250 loss: 0.6215534561872482
  batch 300 loss: 0.5799225705862046
  batch 350 loss: 0.5848913967609406
  batch 400 loss: 0.6079558610916138
  batch 450 loss: 0.6156493693590164
  batch 500 loss: 0.6280016398429871
  batch 550 loss: 0.5900698238611222
  batch 600 loss: 0.6229134607315063
  batch 650 loss: 0.6417688250541687
  batch 700 loss: 0.6166595155000687
  batch 750 loss: 0.6028609240055084
  batch 800 loss: 0.6013192307949066
  batch 850 loss: 0.650135458111763
  batch 900 loss: 0.654701172709465
LOSS train 0.65470 valid 0.82401, valid PER 24.80%
EPOCH 13:
  batch 50 loss: 0.5488935530185699
  batch 100 loss: 0.5458631628751754
  batch 150 loss: 0.5386619502305985
  batch 200 loss: 0.5681084901094436
  batch 250 loss: 0.596301708817482
  batch 300 loss: 0.5562980091571808
  batch 350 loss: 0.546893218755722
  batch 400 loss: 0.5781154131889343
  batch 450 loss: 0.5929944258928299
  batch 500 loss: 0.5485233116149902
  batch 550 loss: 0.6071341925859451
  batch 600 loss: 0.5828747993707657
  batch 650 loss: 0.6154300528764725
  batch 700 loss: 0.5930588507652282
  batch 750 loss: 0.5883568859100342
  batch 800 loss: 0.5971553725004196
  batch 850 loss: 0.6361139845848084
  batch 900 loss: 0.614045278429985
LOSS train 0.61405 valid 0.82470, valid PER 25.02%
EPOCH 14:
  batch 50 loss: 0.5249041992425919
  batch 100 loss: 0.5445944619178772
  batch 150 loss: 0.5359518104791641
  batch 200 loss: 0.5408218139410019
  batch 250 loss: 0.5330804514884949
  batch 300 loss: 0.5717743498086929
  batch 350 loss: 0.542155956029892
  batch 400 loss: 0.527083312869072
  batch 450 loss: 0.5392600268125534
  batch 500 loss: 0.5455112904310226
  batch 550 loss: 0.5685014051198959
  batch 600 loss: 0.5450438344478608
  batch 650 loss: 0.5703496396541595
  batch 700 loss: 0.6070990198850632
  batch 750 loss: 0.6071220809221267
  batch 800 loss: 0.5661392033100128
  batch 850 loss: 0.5951425492763519
  batch 900 loss: 0.5932901859283447
LOSS train 0.59329 valid 0.83384, valid PER 25.13%
EPOCH 15:
  batch 50 loss: 0.5045696449279785
  batch 100 loss: 0.526575139760971
  batch 150 loss: 0.5100775444507599
  batch 200 loss: 0.5451826047897339
  batch 250 loss: 0.5206805205345154
  batch 300 loss: 0.5219263964891434
  batch 350 loss: 0.5340256512165069
  batch 400 loss: 0.5400801265239715
  batch 450 loss: 0.534305044412613
  batch 500 loss: 0.5024196302890778
  batch 550 loss: 0.5226635718345642
  batch 600 loss: 0.5548945397138596
  batch 650 loss: 0.5380782455205917
  batch 700 loss: 0.5400876235961914
  batch 750 loss: 0.540392450094223
  batch 800 loss: 0.5129925072193146
  batch 850 loss: 0.494779024720192
  batch 900 loss: 0.5083611416816711
LOSS train 0.50836 valid 0.84161, valid PER 24.66%
EPOCH 16:
  batch 50 loss: 0.4818379837274551
  batch 100 loss: 0.4563745015859604
  batch 150 loss: 0.45212369501590727
  batch 200 loss: 0.4548828625679016
  batch 250 loss: 0.5388558298349381
  batch 300 loss: 0.5032480120658874
  batch 350 loss: 0.526439278125763
  batch 400 loss: 0.5266905796527862
  batch 450 loss: 0.5217800211906433
  batch 500 loss: 0.47666577517986297
  batch 550 loss: 0.4858524376153946
  batch 600 loss: 0.48916598200798034
  batch 650 loss: 0.5212437844276429
  batch 700 loss: 0.4914820444583893
  batch 750 loss: 0.5166730362176896
  batch 800 loss: 0.5259264487028122
  batch 850 loss: 0.5138488066196442
  batch 900 loss: 0.5191426366567612
LOSS train 0.51914 valid 0.82233, valid PER 24.08%
EPOCH 17:
  batch 50 loss: 0.4479772937297821
  batch 100 loss: 0.4706982350349426
  batch 150 loss: 0.5094080859422684
  batch 200 loss: 0.5011765456199646
  batch 250 loss: 0.5002908855676651
  batch 300 loss: 0.5155855989456177
  batch 350 loss: 0.48597343802452087
  batch 400 loss: 0.5265276819467545
  batch 450 loss: 0.5135380679368973
  batch 500 loss: 0.4847230488061905
  batch 550 loss: 0.4972905868291855
  batch 600 loss: 0.5061913323402405
  batch 650 loss: 0.48876374363899233
  batch 700 loss: 0.48393750071525576
  batch 750 loss: 0.4859631532430649
  batch 800 loss: 0.46457033276557924
  batch 850 loss: 0.4906446206569672
  batch 900 loss: 0.47138590812683107
LOSS train 0.47139 valid 0.85883, valid PER 24.88%
EPOCH 18:
  batch 50 loss: 0.46155142843723296
  batch 100 loss: 0.4665672701597214
  batch 150 loss: 0.46370455145835876
  batch 200 loss: 0.4513380300998688
  batch 250 loss: 0.45849643111228944
  batch 300 loss: 0.4297885274887085
  batch 350 loss: 0.4503512707352638
  batch 400 loss: 0.43974501013755796
  batch 450 loss: 0.46268844962120054
  batch 500 loss: 0.44789311349391936
  batch 550 loss: 0.4491181737184525
  batch 600 loss: 0.43481206089258195
  batch 650 loss: 0.44530710130929946
  batch 700 loss: 0.45233717799186707
  batch 750 loss: 0.4222383323311806
  batch 800 loss: 0.4472044813632965
  batch 850 loss: 0.4619597584009171
  batch 900 loss: 0.47400895059108733
LOSS train 0.47401 valid 0.81178, valid PER 23.79%
EPOCH 19:
  batch 50 loss: 0.39788554251194
  batch 100 loss: 0.40609511524438857
  batch 150 loss: 0.43357893645763396
  batch 200 loss: 0.41403054356575014
  batch 250 loss: 0.4090326023101807
  batch 300 loss: 0.4117074653506279
  batch 350 loss: 0.4008115029335022
  batch 400 loss: 0.4197963339090347
  batch 450 loss: 0.4280131721496582
  batch 500 loss: 0.43080661237239837
  batch 550 loss: 0.4007992693781853
  batch 600 loss: 0.40292216807603837
  batch 650 loss: 0.4502202415466309
  batch 700 loss: 0.4147602194547653
  batch 750 loss: 0.4207391893863678
  batch 800 loss: 0.43660881400108337
  batch 850 loss: 0.4256379371881485
  batch 900 loss: 0.46571637570858004
LOSS train 0.46572 valid 0.83389, valid PER 23.49%
EPOCH 20:
  batch 50 loss: 0.3768159517645836
  batch 100 loss: 0.3686308366060257
  batch 150 loss: 0.3687438914179802
  batch 200 loss: 0.3688816049695015
  batch 250 loss: 0.3476878887414932
  batch 300 loss: 0.3831059205532074
  batch 350 loss: 0.3662849959731102
  batch 400 loss: 0.3801345780491829
  batch 450 loss: 0.386948903799057
  batch 500 loss: 0.3845778125524521
  batch 550 loss: 0.4372412499785423
  batch 600 loss: 0.39817104339599607
  batch 650 loss: 0.4350839120149612
  batch 700 loss: 0.43698198556900025
  batch 750 loss: 0.4071402233839035
  batch 800 loss: 0.44358251214027405
  batch 850 loss: 0.4196096533536911
  batch 900 loss: 0.4256531172990799
LOSS train 0.42565 valid 0.85640, valid PER 24.34%
Training finished in 12.0 minutes.
Model saved to checkpoints/20230118_093914/model_18
Loading model from checkpoints/20230118_093914/model_18
SUB: 15.57%, DEL: 8.31%, INS: 2.15%, COR: 76.12%, PER: 26.04%
