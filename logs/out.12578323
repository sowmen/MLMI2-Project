Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0, schedule='false')
cuda:0
Total number of model parameters is 194728
EPOCH 1:
  batch 50 loss: 4.144411106109619
  batch 100 loss: 3.2537273931503297
  batch 150 loss: 3.18897855758667
  batch 200 loss: 3.149561700820923
  batch 250 loss: 2.9740690660476683
  batch 300 loss: 2.7111607837677
  batch 350 loss: 2.685057339668274
  batch 400 loss: 2.4954031467437745
  batch 450 loss: 2.4316212940216064
  batch 500 loss: 2.345406723022461
  batch 550 loss: 2.255011646747589
  batch 600 loss: 2.199485456943512
  batch 650 loss: 2.113053479194641
  batch 700 loss: 2.1156766486167906
  batch 750 loss: 2.0205764055252073
  batch 800 loss: 2.0381913328170778
  batch 850 loss: 1.9205640411376954
  batch 900 loss: 1.9009688019752502
LOSS train 1.90097 valid 1.78936, valid PER 59.08%
EPOCH 2:
  batch 50 loss: 1.8225655961036682
  batch 100 loss: 1.789435396194458
  batch 150 loss: 1.7040548181533814
  batch 200 loss: 1.7386393308639527
  batch 250 loss: 1.725393798351288
  batch 300 loss: 1.6528342008590697
  batch 350 loss: 1.5820103478431702
  batch 400 loss: 1.6026224517822265
  batch 450 loss: 1.5195060396194457
  batch 500 loss: 1.5344502210617066
  batch 550 loss: 1.5181905078887938
  batch 600 loss: 1.4687613868713378
  batch 650 loss: 1.5406720185279845
  batch 700 loss: 1.4584304738044738
  batch 750 loss: 1.4677410864830016
  batch 800 loss: 1.408094515800476
  batch 850 loss: 1.4263765871524812
  batch 900 loss: 1.4318278455734252
LOSS train 1.43183 valid 1.36676, valid PER 43.41%
EPOCH 3:
  batch 50 loss: 1.373869433403015
  batch 100 loss: 1.3411704862117768
  batch 150 loss: 1.3353535509109498
  batch 200 loss: 1.3075043225288392
  batch 250 loss: 1.304688901901245
  batch 300 loss: 1.3235210704803466
  batch 350 loss: 1.3895243215560913
  batch 400 loss: 1.3117081260681152
  batch 450 loss: 1.3214895665645598
  batch 500 loss: 1.254517023563385
  batch 550 loss: 1.2936973130702973
  batch 600 loss: 1.2332990610599517
  batch 650 loss: 1.2204732668399811
  batch 700 loss: 1.226089870929718
  batch 750 loss: 1.3111705255508423
  batch 800 loss: 1.2243543183803558
  batch 850 loss: 1.2507670795917512
  batch 900 loss: 1.1777845668792724
LOSS train 1.17778 valid 1.25673, valid PER 37.65%
EPOCH 4:
  batch 50 loss: 1.1682702243328094
  batch 100 loss: 1.1702110433578492
  batch 150 loss: 1.1295274460315705
  batch 200 loss: 1.1740608787536622
  batch 250 loss: 1.1903566670417787
  batch 300 loss: 1.19825990319252
  batch 350 loss: 1.0890205299854279
  batch 400 loss: 1.1717319107055664
  batch 450 loss: 1.1376878261566161
  batch 500 loss: 1.1310338652133942
  batch 550 loss: 1.160537179708481
  batch 600 loss: 1.190947983264923
  batch 650 loss: 1.1569004702568053
  batch 700 loss: 1.1002805125713349
  batch 750 loss: 1.1017670297622681
  batch 800 loss: 1.053011234998703
  batch 850 loss: 1.109303287267685
  batch 900 loss: 1.1452182269096374
LOSS train 1.14522 valid 1.16074, valid PER 37.17%
EPOCH 5:
  batch 50 loss: 1.0543736970424653
  batch 100 loss: 1.0625430750846863
  batch 150 loss: 1.1011713933944702
  batch 200 loss: 1.0379931390285493
  batch 250 loss: 1.055951772928238
  batch 300 loss: 1.0692585921287536
  batch 350 loss: 1.0476379418373107
  batch 400 loss: 1.0574455893039703
  batch 450 loss: 1.030335581302643
  batch 500 loss: 1.0741535687446595
  batch 550 loss: 1.0161574649810792
  batch 600 loss: 1.0828663468360902
  batch 650 loss: 1.0376877641677857
  batch 700 loss: 1.0729591822624207
  batch 750 loss: 1.0155694222450256
  batch 800 loss: 1.023831968307495
  batch 850 loss: 1.036349514722824
  batch 900 loss: 1.0293119263648987
LOSS train 1.02931 valid 1.04970, valid PER 31.76%
EPOCH 6:
  batch 50 loss: 1.0408842802047729
  batch 100 loss: 0.9906516301631928
  batch 150 loss: 0.9450156927108765
  batch 200 loss: 0.9963383758068085
  batch 250 loss: 1.0126999270915986
  batch 300 loss: 0.9833266520500183
  batch 350 loss: 0.9853945577144623
  batch 400 loss: 0.9648811507225037
  batch 450 loss: 1.007423630952835
  batch 500 loss: 0.9666378545761108
  batch 550 loss: 1.012988806962967
  batch 600 loss: 0.9569293606281281
  batch 650 loss: 0.9821531641483306
  batch 700 loss: 0.9874496257305145
  batch 750 loss: 0.9588800251483918
  batch 800 loss: 0.982689003944397
  batch 850 loss: 0.9572612822055817
  batch 900 loss: 0.9691977608203888
LOSS train 0.96920 valid 1.04024, valid PER 31.36%
EPOCH 7:
  batch 50 loss: 0.9605581831932067
  batch 100 loss: 0.9649969506263733
  batch 150 loss: 0.9432905459403992
  batch 200 loss: 0.9226914513111114
  batch 250 loss: 0.9430095469951629
  batch 300 loss: 0.9178672623634339
  batch 350 loss: 0.9233464288711548
  batch 400 loss: 0.8948741149902344
  batch 450 loss: 0.9338268959522247
  batch 500 loss: 0.9422127735614777
  batch 550 loss: 0.9302610409259796
  batch 600 loss: 0.9500674569606781
  batch 650 loss: 0.9732085382938385
  batch 700 loss: 0.9715934109687805
  batch 750 loss: 0.9452988255023956
  batch 800 loss: 0.9282330918312073
  batch 850 loss: 0.96098965883255
  batch 900 loss: 0.9963697290420532
LOSS train 0.99637 valid 0.98970, valid PER 31.21%
EPOCH 8:
  batch 50 loss: 0.889195339679718
  batch 100 loss: 0.8787751078605652
  batch 150 loss: 0.9125020837783814
  batch 200 loss: 0.8880850946903229
  batch 250 loss: 0.9029170417785645
  batch 300 loss: 0.8657850420475006
  batch 350 loss: 0.9346937048435211
  batch 400 loss: 0.8726608538627625
  batch 450 loss: 0.916432363986969
  batch 500 loss: 0.9425184452533721
  batch 550 loss: 0.8631499540805817
  batch 600 loss: 0.9011339139938355
  batch 650 loss: 0.9607743072509766
  batch 700 loss: 0.8851670920848846
  batch 750 loss: 0.9095774352550506
  batch 800 loss: 0.9134491515159607
  batch 850 loss: 0.8957136070728302
  batch 900 loss: 0.9055254685878754
LOSS train 0.90553 valid 0.97324, valid PER 29.95%
EPOCH 9:
  batch 50 loss: 0.8193542170524597
  batch 100 loss: 0.8518506956100463
  batch 150 loss: 0.8928345954418182
  batch 200 loss: 0.8866213393211365
  batch 250 loss: 0.9388368165493012
  batch 300 loss: 0.8933782827854156
  batch 350 loss: 0.9168893218040466
  batch 400 loss: 0.8739439594745636
  batch 450 loss: 0.8891173827648163
  batch 500 loss: 0.8191804754734039
  batch 550 loss: 0.8666821539402008
  batch 600 loss: 0.9021744668483734
  batch 650 loss: 0.875335156917572
  batch 700 loss: 0.8451944363117218
  batch 750 loss: 0.8651946830749512
  batch 800 loss: 0.8702112674713135
  batch 850 loss: 0.9174536573886871
  batch 900 loss: 0.8518690752983094
LOSS train 0.85187 valid 1.07577, valid PER 31.94%
EPOCH 10:
  batch 50 loss: 0.8853599143028259
  batch 100 loss: 0.8501269423961639
  batch 150 loss: 0.8444814538955688
  batch 200 loss: 0.8651612734794617
  batch 250 loss: 0.8600127303600311
  batch 300 loss: 0.8166656517982482
  batch 350 loss: 0.8492826187610626
  batch 400 loss: 0.7935486048460006
  batch 450 loss: 0.8143178939819335
  batch 500 loss: 0.8744417583942413
  batch 550 loss: 0.9027451515197754
  batch 600 loss: 0.8627274322509766
  batch 650 loss: 0.8519473922252655
  batch 700 loss: 0.8538787674903869
  batch 750 loss: 0.8231646144390106
  batch 800 loss: 0.8666648209095001
  batch 850 loss: 0.8787429702281951
  batch 900 loss: 0.8674910187721252
LOSS train 0.86749 valid 0.98896, valid PER 31.76%
EPOCH 11:
  batch 50 loss: 0.7798325401544571
  batch 100 loss: 0.7634123873710632
  batch 150 loss: 0.7810492688417434
  batch 200 loss: 0.839589833021164
  batch 250 loss: 0.8325768423080444
  batch 300 loss: 0.8236822414398194
  batch 350 loss: 0.8013783377408982
  batch 400 loss: 0.8531243860721588
  batch 450 loss: 0.8336312401294709
  batch 500 loss: 0.8048012721538543
  batch 550 loss: 0.8262472689151764
  batch 600 loss: 0.8323946154117584
  batch 650 loss: 0.9791660368442535
  batch 700 loss: 0.8516098260879517
  batch 750 loss: 0.8237458312511444
  batch 800 loss: 0.8660493540763855
  batch 850 loss: 0.9329296720027923
  batch 900 loss: 0.8852567398548126
LOSS train 0.88526 valid 0.98446, valid PER 30.53%
EPOCH 12:
  batch 50 loss: 0.8031600153446198
  batch 100 loss: 0.7584121012687683
  batch 150 loss: 0.7970656454563141
  batch 200 loss: 0.8036636352539063
  batch 250 loss: 0.8337082970142364
  batch 300 loss: 0.8639530539512634
  batch 350 loss: 0.8107968354225159
  batch 400 loss: 0.8362312352657318
  batch 450 loss: 0.851330554485321
  batch 500 loss: 0.837302485704422
  batch 550 loss: 0.7942012250423431
  batch 600 loss: 0.8161225318908691
  batch 650 loss: 0.8874510800838471
  batch 700 loss: 0.8595994055271149
  batch 750 loss: 0.874274914264679
  batch 800 loss: 0.8658892041444779
  batch 850 loss: 0.9079059171676636
  batch 900 loss: 0.8855009472370148
LOSS train 0.88550 valid 0.96463, valid PER 29.95%
EPOCH 13:
  batch 50 loss: 0.7815822231769561
  batch 100 loss: 0.7719024753570557
  batch 150 loss: 0.7694479846954345
  batch 200 loss: 0.8181566691398621
  batch 250 loss: 0.7907696795463562
  batch 300 loss: 0.8713766038417816
  batch 350 loss: 0.8156442105770111
  batch 400 loss: 0.8888613259792328
  batch 450 loss: 0.8839654493331909
  batch 500 loss: 0.8224525701999664
  batch 550 loss: 0.811664879322052
  batch 600 loss: 0.8068052148818969
  batch 650 loss: 0.829996513724327
  batch 700 loss: 0.8237529695034027
  batch 750 loss: 0.7859684598445892
  batch 800 loss: 0.8102001273632049
  batch 850 loss: 0.8464577054977417
  batch 900 loss: 0.8325467419624328
LOSS train 0.83255 valid 1.01130, valid PER 30.60%
EPOCH 14:
  batch 50 loss: 0.7940738666057586
  batch 100 loss: 0.8856346672773361
  batch 150 loss: 0.8550059282779694
  batch 200 loss: 0.8510996246337891
  batch 250 loss: 0.8093350768089295
  batch 300 loss: 0.8445723223686218
  batch 350 loss: 0.8517111909389495
  batch 400 loss: 0.8219072878360748
  batch 450 loss: 0.800021140575409
  batch 500 loss: 0.8342818856239319
  batch 550 loss: 0.8287029731273651
  batch 600 loss: 0.8134745925664901
  batch 650 loss: 0.8309839582443237
  batch 700 loss: 0.8515035021305084
  batch 750 loss: 0.8181884109973907
  batch 800 loss: 0.7962351131439209
  batch 850 loss: 0.8521144700050354
  batch 900 loss: 0.8538240385055542
LOSS train 0.85382 valid 0.97552, valid PER 30.43%
EPOCH 15:
  batch 50 loss: 0.7856558442115784
  batch 100 loss: 0.7711848574876785
  batch 150 loss: 0.795995472073555
  batch 200 loss: 0.8213007736206055
  batch 250 loss: 0.7959755241870881
  batch 300 loss: 0.7840452027320862
  batch 350 loss: 0.7871926546096801
  batch 400 loss: 0.7831210446357727
  batch 450 loss: 0.7791380852460861
  batch 500 loss: 0.7320283353328705
  batch 550 loss: 0.7909970939159393
  batch 600 loss: 0.8323151445388794
  batch 650 loss: 0.8150277662277222
  batch 700 loss: 0.817487188577652
  batch 750 loss: 0.8589178574085236
  batch 800 loss: 0.8236969327926635
  batch 850 loss: 0.7814822125434876
  batch 900 loss: 0.809608769416809
LOSS train 0.80961 valid 1.00605, valid PER 30.51%
EPOCH 16:
  batch 50 loss: 0.7701444482803345
  batch 100 loss: 0.7421453082561493
  batch 150 loss: 0.7347950196266174
  batch 200 loss: 0.7230932939052582
  batch 250 loss: 0.7761987328529358
  batch 300 loss: 0.7786332726478576
  batch 350 loss: 0.7752948033809662
  batch 400 loss: 0.7641184240579605
  batch 450 loss: 0.7804395884275437
  batch 500 loss: 0.7326238173246383
  batch 550 loss: 0.9253937005996704
  batch 600 loss: 0.914527394771576
  batch 650 loss: 0.8513707566261292
  batch 700 loss: 0.8043243849277496
  batch 750 loss: 0.8189537608623505
  batch 800 loss: 0.8083718651533127
  batch 850 loss: 0.8172216594219208
  batch 900 loss: 0.7910991549491883
LOSS train 0.79110 valid 0.96221, valid PER 28.39%
EPOCH 17:
  batch 50 loss: 0.7333894026279449
  batch 100 loss: 0.7335413855314254
  batch 150 loss: 0.7383494013547898
  batch 200 loss: 0.7920693945884705
  batch 250 loss: 0.7818048107624054
  batch 300 loss: 0.8203878396749497
  batch 350 loss: 0.8012397170066834
  batch 400 loss: 0.8962838017940521
  batch 450 loss: 0.8599237859249115
  batch 500 loss: 0.8637997055053711
  batch 550 loss: 0.8520270228385926
  batch 600 loss: 0.8672886300086975
  batch 650 loss: 0.8453113627433777
  batch 700 loss: 0.846500540971756
  batch 750 loss: 0.8275313985347748
  batch 800 loss: 0.9262937104701996
  batch 850 loss: 0.9432290768623353
  batch 900 loss: 0.857344388961792
LOSS train 0.85734 valid 1.04856, valid PER 32.67%
EPOCH 18:
  batch 50 loss: 0.9827416265010833
  batch 100 loss: 0.9370810055732727
  batch 150 loss: 0.8997166013717651
  batch 200 loss: 0.8544340288639068
  batch 250 loss: 0.850139639377594
  batch 300 loss: 0.8230152356624604
  batch 350 loss: 0.8381511580944061
  batch 400 loss: 0.780619443655014
  batch 450 loss: 0.828864336013794
  batch 500 loss: 0.7977823674678802
  batch 550 loss: 0.803320437669754
  batch 600 loss: 0.7586675322055817
  batch 650 loss: 0.7701078081130981
  batch 700 loss: 0.8070001685619355
  batch 750 loss: 0.7959435212612153
  batch 800 loss: 0.7886152410507202
  batch 850 loss: 0.7783303356170654
  batch 900 loss: 0.7854537868499756
LOSS train 0.78545 valid 0.99100, valid PER 30.60%
EPOCH 19:
  batch 50 loss: 0.685967321395874
  batch 100 loss: 0.6946958011388779
  batch 150 loss: 0.7290188527107239
  batch 200 loss: 0.7366774797439575
  batch 250 loss: 0.7947505819797516
  batch 300 loss: 0.761913400888443
  batch 350 loss: 0.7255267441272736
  batch 400 loss: 0.776151636838913
  batch 450 loss: 0.7764859664440155
  batch 500 loss: 0.7774046260118485
  batch 550 loss: 0.7507072257995605
  batch 600 loss: 0.734408614039421
  batch 650 loss: 0.7960422003269195
  batch 700 loss: 0.748677927851677
  batch 750 loss: 0.7486659705638885
  batch 800 loss: 0.767208080291748
  batch 850 loss: 0.7568845319747924
  batch 900 loss: 0.7771480858325959
LOSS train 0.77715 valid 0.98804, valid PER 30.23%
EPOCH 20:
  batch 50 loss: 0.7153477501869202
  batch 100 loss: 0.6787785416841507
  batch 150 loss: 0.6797028732299805
  batch 200 loss: 0.6977619922161102
  batch 250 loss: 0.7149200904369354
  batch 300 loss: 0.7422860342264176
  batch 350 loss: 0.6894806182384491
  batch 400 loss: 0.7617282277345657
  batch 450 loss: 0.7206930196285248
  batch 500 loss: 0.7237996125221252
  batch 550 loss: 0.7885696506500244
  batch 600 loss: 0.7238791984319687
  batch 650 loss: 0.7450803673267364
  batch 700 loss: 0.8014179944992066
  batch 750 loss: 0.7639465129375458
  batch 800 loss: 0.7635222989320755
  batch 850 loss: 0.7553984570503235
  batch 900 loss: 0.7502582025527954
LOSS train 0.75026 valid 0.97073, valid PER 28.90%
Training finished in 10.0 minutes.
Model saved to checkpoints/20230118_094914/model_16
Loading model from checkpoints/20230118_094914/model_16
SUB: 18.14%, DEL: 9.25%, INS: 3.45%, COR: 72.61%, PER: 30.84%
