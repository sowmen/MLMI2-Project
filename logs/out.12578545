Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0, schedule='true')
cuda:0
Total number of model parameters is 194728
EPOCH 1:
  batch 50 loss: 5.2014941358566285
  batch 100 loss: 3.3872625637054443
  batch 150 loss: 3.309821662902832
  batch 200 loss: 3.20837317943573
  batch 250 loss: 3.0521497631073
  batch 300 loss: 2.8450813770294188
  batch 350 loss: 2.706448140144348
  batch 400 loss: 2.6095193433761597
  batch 450 loss: 2.496804747581482
  batch 500 loss: 2.4042046165466306
  batch 550 loss: 2.310647420883179
  batch 600 loss: 2.2734926533699036
  batch 650 loss: 2.174074957370758
  batch 700 loss: 2.184046983718872
  batch 750 loss: 2.1033979535102842
  batch 800 loss: 2.0790493488311768
  batch 850 loss: 2.0058643794059754
  batch 900 loss: 1.9815281963348388
LOSS train 1.98153 valid 1.89776, valid PER 66.04%
EPOCH 2:
  batch 50 loss: 1.8902226090431213
  batch 100 loss: 1.8374100995063782
  batch 150 loss: 1.7949597597122193
  batch 200 loss: 1.7776527619361877
  batch 250 loss: 1.7721919441223144
  batch 300 loss: 1.7109558367729187
  batch 350 loss: 1.629327006340027
  batch 400 loss: 1.636614785194397
  batch 450 loss: 1.5550695490837096
  batch 500 loss: 1.5719889140129089
  batch 550 loss: 1.575512580871582
  batch 600 loss: 1.5119574022293092
  batch 650 loss: 1.5564711451530457
  batch 700 loss: 1.4855169081687927
  batch 750 loss: 1.4929673767089844
  batch 800 loss: 1.4396988463401794
  batch 850 loss: 1.434501323699951
  batch 900 loss: 1.4626190209388732
LOSS train 1.46262 valid 1.42531, valid PER 43.90%
EPOCH 3:
  batch 50 loss: 1.3918633651733399
  batch 100 loss: 1.3722150230407715
  batch 150 loss: 1.3804669904708862
  batch 200 loss: 1.3341667246818543
  batch 250 loss: 1.3243562078475952
  batch 300 loss: 1.325956780910492
  batch 350 loss: 1.3690505313873291
  batch 400 loss: 1.3350183391571044
  batch 450 loss: 1.3164281725883484
  batch 500 loss: 1.2892223167419434
  batch 550 loss: 1.3046803307533263
  batch 600 loss: 1.25511207818985
  batch 650 loss: 1.250489695072174
  batch 700 loss: 1.2614695632457733
  batch 750 loss: 1.3124906599521637
  batch 800 loss: 1.2322858583927154
  batch 850 loss: 1.2631139421463013
  batch 900 loss: 1.1932286047935485
LOSS train 1.19323 valid 1.26606, valid PER 38.11%
EPOCH 4:
  batch 50 loss: 1.1814728319644927
  batch 100 loss: 1.1983369195461273
  batch 150 loss: 1.1500805699825287
  batch 200 loss: 1.1893043911457062
  batch 250 loss: 1.1996501517295837
  batch 300 loss: 1.2175176572799682
  batch 350 loss: 1.115546246767044
  batch 400 loss: 1.1710673999786376
  batch 450 loss: 1.1641812813282013
  batch 500 loss: 1.1430668365955352
  batch 550 loss: 1.162223072052002
  batch 600 loss: 1.1930557322502136
  batch 650 loss: 1.1593422210216522
  batch 700 loss: 1.1289246654510499
  batch 750 loss: 1.1028269982337953
  batch 800 loss: 1.080004382133484
  batch 850 loss: 1.11152907371521
  batch 900 loss: 1.1462538433074951
LOSS train 1.14625 valid 1.13901, valid PER 35.21%
EPOCH 5:
  batch 50 loss: 1.0833658766746521
  batch 100 loss: 1.0518366801738739
  batch 150 loss: 1.1050119698047638
  batch 200 loss: 1.0393086659908295
  batch 250 loss: 1.0608878386020661
  batch 300 loss: 1.0688791835308076
  batch 350 loss: 1.050312365293503
  batch 400 loss: 1.0702860760688782
  batch 450 loss: 1.0408752870559692
  batch 500 loss: 1.0707102608680725
  batch 550 loss: 1.0136801314353943
  batch 600 loss: 1.0927895379066468
  batch 650 loss: 1.0512370085716247
  batch 700 loss: 1.1064163589477538
  batch 750 loss: 1.0188421428203582
  batch 800 loss: 1.0451533746719361
  batch 850 loss: 1.0712829220294953
  batch 900 loss: 1.0499203324317932
LOSS train 1.04992 valid 1.07082, valid PER 34.27%
EPOCH 6:
  batch 50 loss: 1.0317769014835358
  batch 100 loss: 0.9806660521030426
  batch 150 loss: 0.980427143573761
  batch 200 loss: 0.994148223400116
  batch 250 loss: 1.026965457201004
  batch 300 loss: 0.9906596469879151
  batch 350 loss: 0.9980249667167663
  batch 400 loss: 0.9831142687797546
  batch 450 loss: 1.005419167280197
  batch 500 loss: 0.9795731568336487
  batch 550 loss: 1.022128074169159
  batch 600 loss: 0.9788159394264221
  batch 650 loss: 1.0021096110343932
  batch 700 loss: 0.981599760055542
  batch 750 loss: 0.9747711062431336
  batch 800 loss: 0.9668520176410675
  batch 850 loss: 0.9566600680351257
  batch 900 loss: 0.9814542305469512
LOSS train 0.98145 valid 1.06546, valid PER 32.60%
EPOCH 7:
  batch 50 loss: 0.9593597519397735
  batch 100 loss: 0.9517522990703583
  batch 150 loss: 0.9239708793163299
  batch 200 loss: 0.9324581408500672
  batch 250 loss: 0.9189612543582917
  batch 300 loss: 0.9200238037109375
  batch 350 loss: 0.9248031604290009
  batch 400 loss: 0.9103966677188873
  batch 450 loss: 0.9142387390136719
  batch 500 loss: 0.9220212423801422
  batch 550 loss: 0.9260137665271759
  batch 600 loss: 0.9505500066280365
  batch 650 loss: 0.9408311021327972
  batch 700 loss: 0.9548787832260132
  batch 750 loss: 0.9093353021144867
  batch 800 loss: 0.9222010052204133
  batch 850 loss: 0.9441764760017395
  batch 900 loss: 0.9634674918651581
LOSS train 0.96347 valid 1.00075, valid PER 30.97%
EPOCH 8:
  batch 50 loss: 0.8758371460437775
  batch 100 loss: 0.8725549018383026
  batch 150 loss: 0.8906599020957947
  batch 200 loss: 0.8525313770771027
  batch 250 loss: 0.8827341401576996
  batch 300 loss: 0.83413360953331
  batch 350 loss: 0.9021198725700379
  batch 400 loss: 0.8573979699611664
  batch 450 loss: 0.8959956097602845
  batch 500 loss: 0.9288591802120209
  batch 550 loss: 0.8582119309902191
  batch 600 loss: 0.8847630763053894
  batch 650 loss: 0.9220861446857452
  batch 700 loss: 0.8680837476253509
  batch 750 loss: 0.8652822685241699
  batch 800 loss: 0.9021796774864197
  batch 850 loss: 0.892561161518097
  batch 900 loss: 0.9024348521232605
LOSS train 0.90243 valid 0.98922, valid PER 29.72%
EPOCH 9:
  batch 50 loss: 0.7989875066280365
  batch 100 loss: 0.8354102981090545
  batch 150 loss: 0.85436896443367
  batch 200 loss: 0.8036457109451294
  batch 250 loss: 0.8588926327228547
  batch 300 loss: 0.8604929947853088
  batch 350 loss: 0.8740099275112152
  batch 400 loss: 0.8625016582012176
  batch 450 loss: 0.8374945282936096
  batch 500 loss: 0.8293197214603424
  batch 550 loss: 0.8297465181350708
  batch 600 loss: 0.8754142820835114
  batch 650 loss: 0.8341116511821747
  batch 700 loss: 0.8204865741729737
  batch 750 loss: 0.827935745716095
  batch 800 loss: 0.8679603552818298
  batch 850 loss: 0.879507633447647
  batch 900 loss: 0.8315886187553406
LOSS train 0.83159 valid 0.99538, valid PER 29.46%
EPOCH 10:
  batch 50 loss: 0.7338712048530579
  batch 100 loss: 0.7377567106485367
  batch 150 loss: 0.7445648491382599
  batch 200 loss: 0.7479550820589066
  batch 250 loss: 0.7509994626045227
  batch 300 loss: 0.7019151902198791
  batch 350 loss: 0.7432815307378768
  batch 400 loss: 0.6993790745735169
  batch 450 loss: 0.7015191757678986
  batch 500 loss: 0.7373324817419052
  batch 550 loss: 0.7552046394348144
  batch 600 loss: 0.7217549383640289
  batch 650 loss: 0.7183943367004395
  batch 700 loss: 0.7295132738351822
  batch 750 loss: 0.7081344902515412
  batch 800 loss: 0.7285952472686767
  batch 850 loss: 0.7365862464904785
  batch 900 loss: 0.7498128926753997
LOSS train 0.74981 valid 0.90990, valid PER 28.41%
EPOCH 11:
  batch 50 loss: 0.6710824632644653
  batch 100 loss: 0.6575641989707947
  batch 150 loss: 0.6655965459346771
  batch 200 loss: 0.7179195088148117
  batch 250 loss: 0.7076669842004776
  batch 300 loss: 0.6640133333206176
  batch 350 loss: 0.7046761029958725
  batch 400 loss: 0.7112981939315796
  batch 450 loss: 0.7026016050577164
  batch 500 loss: 0.6644788229465485
  batch 550 loss: 0.6959486669301986
  batch 600 loss: 0.6890534418821335
  batch 650 loss: 0.7389160436391831
  batch 700 loss: 0.6825286829471588
  batch 750 loss: 0.682672039270401
  batch 800 loss: 0.7227686488628388
  batch 850 loss: 0.7368117380142212
  batch 900 loss: 0.7268330943584442
LOSS train 0.72683 valid 0.90367, valid PER 28.04%
EPOCH 12:
  batch 50 loss: 0.6624914836883545
  batch 100 loss: 0.6511577332019806
  batch 150 loss: 0.6260267394781113
  batch 200 loss: 0.6669417583942413
  batch 250 loss: 0.6861765569448471
  batch 300 loss: 0.6621671760082245
  batch 350 loss: 0.6600122892856598
  batch 400 loss: 0.6775118762254715
  batch 450 loss: 0.6898991221189499
  batch 500 loss: 0.6816713392734528
  batch 550 loss: 0.6526023989915848
  batch 600 loss: 0.6600587648153305
  batch 650 loss: 0.7027146589756011
  batch 700 loss: 0.684472862482071
  batch 750 loss: 0.6713320815563202
  batch 800 loss: 0.6696826189756393
  batch 850 loss: 0.7175007712841034
  batch 900 loss: 0.7013123285770416
LOSS train 0.70131 valid 0.89335, valid PER 27.40%
EPOCH 13:
  batch 50 loss: 0.627529866695404
  batch 100 loss: 0.6286108803749084
  batch 150 loss: 0.6273378425836563
  batch 200 loss: 0.6580371147394181
  batch 250 loss: 0.6473569929599762
  batch 300 loss: 0.6519078826904297
  batch 350 loss: 0.6449833530187606
  batch 400 loss: 0.6422822749614716
  batch 450 loss: 0.6556333678960801
  batch 500 loss: 0.6217908394336701
  batch 550 loss: 0.682068156003952
  batch 600 loss: 0.6456302064657211
  batch 650 loss: 0.6722631430625916
  batch 700 loss: 0.6825893467664719
  batch 750 loss: 0.6214153248071671
  batch 800 loss: 0.6431611227989197
  batch 850 loss: 0.6859985822439194
  batch 900 loss: 0.6772633004188537
LOSS train 0.67726 valid 0.90586, valid PER 27.05%
EPOCH 14:
  batch 50 loss: 0.5852201801538467
  batch 100 loss: 0.610518291592598
  batch 150 loss: 0.5931833535432816
  batch 200 loss: 0.5823238170146943
  batch 250 loss: 0.5721636968851089
  batch 300 loss: 0.601315279006958
  batch 350 loss: 0.5656677824258804
  batch 400 loss: 0.5743350946903228
  batch 450 loss: 0.5855084931850434
  batch 500 loss: 0.5798807185888291
  batch 550 loss: 0.599675886631012
  batch 600 loss: 0.5696223002672195
  batch 650 loss: 0.5909649521112442
  batch 700 loss: 0.6142929553985595
  batch 750 loss: 0.5804755741357803
  batch 800 loss: 0.5626976799964905
  batch 850 loss: 0.6072754549980164
  batch 900 loss: 0.5840022176504135
LOSS train 0.58400 valid 0.88551, valid PER 26.93%
EPOCH 15:
  batch 50 loss: 0.558972161412239
  batch 100 loss: 0.5489959824085235
  batch 150 loss: 0.5656436395645141
  batch 200 loss: 0.5885064655542374
  batch 250 loss: 0.5812395989894867
  batch 300 loss: 0.5550072216987609
  batch 350 loss: 0.5629355210065842
  batch 400 loss: 0.5791766142845154
  batch 450 loss: 0.5686868119239807
  batch 500 loss: 0.5399131494760513
  batch 550 loss: 0.5686830639839172
  batch 600 loss: 0.580719621181488
  batch 650 loss: 0.6029819029569626
  batch 700 loss: 0.596766636967659
  batch 750 loss: 0.5870757263898849
  batch 800 loss: 0.564032843708992
  batch 850 loss: 0.5415263950824738
  batch 900 loss: 0.5740510314702988
LOSS train 0.57405 valid 0.88784, valid PER 26.18%
EPOCH 16:
  batch 50 loss: 0.546479777097702
  batch 100 loss: 0.5160163873434067
  batch 150 loss: 0.5149716478586197
  batch 200 loss: 0.5266524571180343
  batch 250 loss: 0.542853189110756
  batch 300 loss: 0.536997834444046
  batch 350 loss: 0.5504385662078858
  batch 400 loss: 0.5494597244262696
  batch 450 loss: 0.5422111779451371
  batch 500 loss: 0.5293966650962829
  batch 550 loss: 0.5235153043270111
  batch 600 loss: 0.5303916436433792
  batch 650 loss: 0.5375234234333038
  batch 700 loss: 0.5274184155464172
  batch 750 loss: 0.5314141684770584
  batch 800 loss: 0.5452455884218216
  batch 850 loss: 0.5300613903999328
  batch 900 loss: 0.5436382150650024
LOSS train 0.54364 valid 0.88645, valid PER 26.30%
EPOCH 17:
  batch 50 loss: 0.5224929797649384
  batch 100 loss: 0.5237564492225647
  batch 150 loss: 0.514410907626152
  batch 200 loss: 0.499068398475647
  batch 250 loss: 0.5300579941272736
  batch 300 loss: 0.5325813448429108
  batch 350 loss: 0.4908051532506943
  batch 400 loss: 0.5400479847192764
  batch 450 loss: 0.5110429948568345
  batch 500 loss: 0.48878479361534116
  batch 550 loss: 0.504046842455864
  batch 600 loss: 0.5244170558452607
  batch 650 loss: 0.5113386559486389
  batch 700 loss: 0.5079824370145798
  batch 750 loss: 0.5088467353582382
  batch 800 loss: 0.49885732591152193
  batch 850 loss: 0.5268261361122132
  batch 900 loss: 0.4931859004497528
LOSS train 0.49319 valid 0.88447, valid PER 25.76%
EPOCH 18:
  batch 50 loss: 0.5199741631746292
  batch 100 loss: 0.5142465478181839
  batch 150 loss: 0.5263883608579636
  batch 200 loss: 0.5140647095441818
  batch 250 loss: 0.517013127207756
  batch 300 loss: 0.4855754977464676
  batch 350 loss: 0.5107444709539414
  batch 400 loss: 0.4980013972520828
  batch 450 loss: 0.5172085291147233
  batch 500 loss: 0.5080848807096481
  batch 550 loss: 0.5328439325094223
  batch 600 loss: 0.47830439746379855
  batch 650 loss: 0.49458658814430234
  batch 700 loss: 0.5256204861402511
  batch 750 loss: 0.4844181787967682
  batch 800 loss: 0.49206232488155366
  batch 850 loss: 0.4980229926109314
  batch 900 loss: 0.5223315387964249
LOSS train 0.52233 valid 0.88700, valid PER 25.73%
EPOCH 19:
  batch 50 loss: 0.4835216951370239
  batch 100 loss: 0.4901151245832443
  batch 150 loss: 0.495511115193367
  batch 200 loss: 0.4956443566083908
  batch 250 loss: 0.4996164757013321
  batch 300 loss: 0.516560737490654
  batch 350 loss: 0.48984614670276644
  batch 400 loss: 0.4953971666097641
  batch 450 loss: 0.5066320079565049
  batch 500 loss: 0.48685120582580566
  batch 550 loss: 0.4765945029258728
  batch 600 loss: 0.4948172062635422
  batch 650 loss: 0.5480220049619675
  batch 700 loss: 0.4718583697080612
  batch 750 loss: 0.4848392552137375
  batch 800 loss: 0.5124311465024948
  batch 850 loss: 0.5138558012247085
  batch 900 loss: 0.5000335329771042
LOSS train 0.50003 valid 0.89112, valid PER 25.69%
EPOCH 20:
  batch 50 loss: 0.49832343459129336
  batch 100 loss: 0.48750354945659635
  batch 150 loss: 0.4848129028081894
  batch 200 loss: 0.4964202392101288
  batch 250 loss: 0.4877350091934204
  batch 300 loss: 0.5089997529983521
  batch 350 loss: 0.47796361684799193
  batch 400 loss: 0.4968649125099182
  batch 450 loss: 0.49694235503673556
  batch 500 loss: 0.4606483232975006
  batch 550 loss: 0.5149290019273758
  batch 600 loss: 0.47437717735767365
  batch 650 loss: 0.49631584048271177
  batch 700 loss: 0.4857098668813705
  batch 750 loss: 0.46041543811559676
  batch 800 loss: 0.5077127295732499
  batch 850 loss: 0.4919139164686203
  batch 900 loss: 0.5121116578578949
LOSS train 0.51211 valid 0.89234, valid PER 25.75%
Training finished in 9.0 minutes.
Model saved to checkpoints/20230118_095302/model_17
Loading model from checkpoints/20230118_095302/model_17
SUB: 15.47%, DEL: 8.73%, INS: 3.31%, COR: 75.80%, PER: 27.51%
