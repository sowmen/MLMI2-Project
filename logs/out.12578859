Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0, schedule='true')
cuda:0
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 5.117972345352173
  batch 100 loss: 3.41899742603302
  batch 150 loss: 3.302749991416931
  batch 200 loss: 3.1860689306259156
  batch 250 loss: 3.0892204809188843
  batch 300 loss: 2.875135107040405
  batch 350 loss: 2.6989061069488525
  batch 400 loss: 2.585112509727478
  batch 450 loss: 2.4851496171951295
  batch 500 loss: 2.351036982536316
  batch 550 loss: 2.2739625549316407
  batch 600 loss: 2.2034322595596314
  batch 650 loss: 2.0986291766166687
  batch 700 loss: 2.077625391483307
  batch 750 loss: 1.987818112373352
  batch 800 loss: 1.9353586339950561
  batch 850 loss: 1.874827537536621
  batch 900 loss: 1.825724709033966
LOSS train 1.82572 valid 1.74342, valid PER 66.78%
EPOCH 2:
  batch 50 loss: 1.7452669715881348
  batch 100 loss: 1.6732788133621215
  batch 150 loss: 1.632778537273407
  batch 200 loss: 1.6395402216911317
  batch 250 loss: 1.6333886289596558
  batch 300 loss: 1.578714370727539
  batch 350 loss: 1.4895319795608521
  batch 400 loss: 1.487373676300049
  batch 450 loss: 1.4314646172523497
  batch 500 loss: 1.4440370297431946
  batch 550 loss: 1.4417974901199342
  batch 600 loss: 1.3859180092811585
  batch 650 loss: 1.4131610369682313
  batch 700 loss: 1.3395453238487243
  batch 750 loss: 1.3318950295448304
  batch 800 loss: 1.258863124847412
  batch 850 loss: 1.2710778415203094
  batch 900 loss: 1.2697363996505737
LOSS train 1.26974 valid 1.23724, valid PER 39.42%
EPOCH 3:
  batch 50 loss: 1.224530428647995
  batch 100 loss: 1.2021440720558167
  batch 150 loss: 1.1846636784076692
  batch 200 loss: 1.1754888248443605
  batch 250 loss: 1.1509196329116822
  batch 300 loss: 1.1448881554603576
  batch 350 loss: 1.186424252986908
  batch 400 loss: 1.1648718011379242
  batch 450 loss: 1.1273835468292237
  batch 500 loss: 1.1337780725955964
  batch 550 loss: 1.1273547863960267
  batch 600 loss: 1.0911025846004485
  batch 650 loss: 1.0765957832336426
  batch 700 loss: 1.0922585916519165
  batch 750 loss: 1.1507502806186676
  batch 800 loss: 1.0684593427181244
  batch 850 loss: 1.1061170017719268
  batch 900 loss: 1.0288113272190094
LOSS train 1.02881 valid 1.09503, valid PER 33.40%
EPOCH 4:
  batch 50 loss: 1.0284658038616181
  batch 100 loss: 1.033676164150238
  batch 150 loss: 0.9988302421569825
  batch 200 loss: 1.029257574081421
  batch 250 loss: 1.020388536453247
  batch 300 loss: 1.0424787926673889
  batch 350 loss: 0.9705122649669647
  batch 400 loss: 1.0201938760280609
  batch 450 loss: 0.9949650573730469
  batch 500 loss: 0.9885414695739746
  batch 550 loss: 1.000857844352722
  batch 600 loss: 1.0207962167263032
  batch 650 loss: 1.0003580808639527
  batch 700 loss: 0.9684344804286957
  batch 750 loss: 0.9603845989704132
  batch 800 loss: 0.935183779001236
  batch 850 loss: 0.9738112807273864
  batch 900 loss: 1.0054553818702698
LOSS train 1.00546 valid 0.98171, valid PER 30.73%
EPOCH 5:
  batch 50 loss: 0.9135475981235505
  batch 100 loss: 0.9113809823989868
  batch 150 loss: 0.9687230491638184
  batch 200 loss: 0.8918989527225495
  batch 250 loss: 0.9125940930843354
  batch 300 loss: 0.9236110508441925
  batch 350 loss: 0.8940550529956818
  batch 400 loss: 0.9172354710102081
  batch 450 loss: 0.9021515262126922
  batch 500 loss: 0.926173688173294
  batch 550 loss: 0.8674700450897217
  batch 600 loss: 0.9398137927055359
  batch 650 loss: 0.9055835509300232
  batch 700 loss: 0.9365836560726166
  batch 750 loss: 0.8702038633823395
  batch 800 loss: 0.9003650903701782
  batch 850 loss: 0.8915872645378112
  batch 900 loss: 0.902942556142807
LOSS train 0.90294 valid 0.91553, valid PER 28.10%
EPOCH 6:
  batch 50 loss: 0.866076648235321
  batch 100 loss: 0.829786981344223
  batch 150 loss: 0.8367584615945816
  batch 200 loss: 0.8397910118103027
  batch 250 loss: 0.8737000012397766
  batch 300 loss: 0.8537728202342987
  batch 350 loss: 0.853902176618576
  batch 400 loss: 0.8198647117614746
  batch 450 loss: 0.8407445538043976
  batch 500 loss: 0.8285400259494782
  batch 550 loss: 0.8756657910346984
  batch 600 loss: 0.8407852363586426
  batch 650 loss: 0.8553642725944519
  batch 700 loss: 0.8415660464763641
  batch 750 loss: 0.8308488154411315
  batch 800 loss: 0.8244947469234467
  batch 850 loss: 0.8249679899215698
  batch 900 loss: 0.8399991798400879
LOSS train 0.84000 valid 0.90632, valid PER 28.11%
EPOCH 7:
  batch 50 loss: 0.7968695843219757
  batch 100 loss: 0.8062843811511994
  batch 150 loss: 0.7691255450248718
  batch 200 loss: 0.7708000087738037
  batch 250 loss: 0.768851808309555
  batch 300 loss: 0.7662390542030334
  batch 350 loss: 0.781754812002182
  batch 400 loss: 0.7701150298118591
  batch 450 loss: 0.7988486635684967
  batch 500 loss: 0.7882058143615722
  batch 550 loss: 0.7619794535636902
  batch 600 loss: 0.7937791848182678
  batch 650 loss: 0.7758275508880615
  batch 700 loss: 0.8012598901987076
  batch 750 loss: 0.7742076015472412
  batch 800 loss: 0.7622166943550109
  batch 850 loss: 0.7907083755731583
  batch 900 loss: 0.8208820140361786
LOSS train 0.82088 valid 0.87043, valid PER 27.04%
EPOCH 8:
  batch 50 loss: 0.7405460721254349
  batch 100 loss: 0.7299845784902572
  batch 150 loss: 0.7286385810375213
  batch 200 loss: 0.7197866070270539
  batch 250 loss: 0.7248583430051804
  batch 300 loss: 0.6961698234081268
  batch 350 loss: 0.7757714462280273
  batch 400 loss: 0.7317460721731186
  batch 450 loss: 0.7496466135978699
  batch 500 loss: 0.762027633190155
  batch 550 loss: 0.7026926231384277
  batch 600 loss: 0.745776726603508
  batch 650 loss: 0.763869241476059
  batch 700 loss: 0.71891761302948
  batch 750 loss: 0.7241764271259308
  batch 800 loss: 0.7514459514617919
  batch 850 loss: 0.7340785539150239
  batch 900 loss: 0.7651681119203567
LOSS train 0.76517 valid 0.85259, valid PER 26.49%
EPOCH 9:
  batch 50 loss: 0.6633087956905365
  batch 100 loss: 0.6998087632656097
  batch 150 loss: 0.6909668242931366
  batch 200 loss: 0.6848485040664672
  batch 250 loss: 0.7042525202035904
  batch 300 loss: 0.7030067276954651
  batch 350 loss: 0.7228980135917663
  batch 400 loss: 0.6925865715742111
  batch 450 loss: 0.7105428051948547
  batch 500 loss: 0.6788617068529129
  batch 550 loss: 0.7120956897735595
  batch 600 loss: 0.7205275070667266
  batch 650 loss: 0.6764245182275772
  batch 700 loss: 0.678750347495079
  batch 750 loss: 0.686125192642212
  batch 800 loss: 0.7026820659637452
  batch 850 loss: 0.7219179874658584
  batch 900 loss: 0.6672223567962646
LOSS train 0.66722 valid 0.84507, valid PER 25.70%
EPOCH 10:
  batch 50 loss: 0.6220762377977371
  batch 100 loss: 0.6347954231500625
  batch 150 loss: 0.6515475684404373
  batch 200 loss: 0.666897383928299
  batch 250 loss: 0.6744721502065658
  batch 300 loss: 0.6385043662786484
  batch 350 loss: 0.6661893874406815
  batch 400 loss: 0.6237254208326339
  batch 450 loss: 0.6400223898887635
  batch 500 loss: 0.6574799573421478
  batch 550 loss: 0.6957459950447082
  batch 600 loss: 0.6602059954404831
  batch 650 loss: 0.6558011090755462
  batch 700 loss: 0.6658459627628326
  batch 750 loss: 0.6599080067873001
  batch 800 loss: 0.6704290980100631
  batch 850 loss: 0.6622531497478485
  batch 900 loss: 0.6802185797691345
LOSS train 0.68022 valid 0.80929, valid PER 25.60%
EPOCH 11:
  batch 50 loss: 0.6008178871870041
  batch 100 loss: 0.5576171696186065
  batch 150 loss: 0.5882578217983245
  batch 200 loss: 0.6362394040822983
  batch 250 loss: 0.6211393427848816
  batch 300 loss: 0.5856221389770507
  batch 350 loss: 0.6219212979078292
  batch 400 loss: 0.6301052314043045
  batch 450 loss: 0.6426991605758667
  batch 500 loss: 0.6149988347291946
  batch 550 loss: 0.6447299408912659
  batch 600 loss: 0.6242751377820969
  batch 650 loss: 0.6697956097126007
  batch 700 loss: 0.605975033044815
  batch 750 loss: 0.6143471962213516
  batch 800 loss: 0.6494938653707504
  batch 850 loss: 0.6551363939046859
  batch 900 loss: 0.6532532638311386
LOSS train 0.65325 valid 0.81391, valid PER 24.88%
EPOCH 12:
  batch 50 loss: 0.5613840246200561
  batch 100 loss: 0.5303218525648117
  batch 150 loss: 0.4844276517629623
  batch 200 loss: 0.5064907485246658
  batch 250 loss: 0.5135506403446197
  batch 300 loss: 0.519507412314415
  batch 350 loss: 0.5016577172279358
  batch 400 loss: 0.5223621207475663
  batch 450 loss: 0.5259271800518036
  batch 500 loss: 0.5323303878307343
  batch 550 loss: 0.48924184203147886
  batch 600 loss: 0.5116854399442673
  batch 650 loss: 0.5310573923587799
  batch 700 loss: 0.5284632873535157
  batch 750 loss: 0.5150261551141739
  batch 800 loss: 0.5118994203209877
  batch 850 loss: 0.5483449840545654
  batch 900 loss: 0.5450088167190552
LOSS train 0.54501 valid 0.76971, valid PER 23.41%
EPOCH 13:
  batch 50 loss: 0.47702120542526244
  batch 100 loss: 0.4572845849394798
  batch 150 loss: 0.4631477326154709
  batch 200 loss: 0.4914505791664123
  batch 250 loss: 0.4809682559967041
  batch 300 loss: 0.4769684547185898
  batch 350 loss: 0.4751116734743118
  batch 400 loss: 0.4935217618942261
  batch 450 loss: 0.48380431830883025
  batch 500 loss: 0.46230623960494993
  batch 550 loss: 0.5102036952972412
  batch 600 loss: 0.4770090639591217
  batch 650 loss: 0.5025706565380097
  batch 700 loss: 0.5028387123346328
  batch 750 loss: 0.4551075202226639
  batch 800 loss: 0.4787245786190033
  batch 850 loss: 0.5076458269357681
  batch 900 loss: 0.5115475249290466
LOSS train 0.51155 valid 0.78077, valid PER 23.64%
EPOCH 14:
  batch 50 loss: 0.43693040907382963
  batch 100 loss: 0.43555749237537383
  batch 150 loss: 0.41633632957935335
  batch 200 loss: 0.4023269757628441
  batch 250 loss: 0.41631649166345597
  batch 300 loss: 0.446518030166626
  batch 350 loss: 0.40802684903144837
  batch 400 loss: 0.4121113097667694
  batch 450 loss: 0.42523761510849
  batch 500 loss: 0.4327518796920776
  batch 550 loss: 0.4396569174528122
  batch 600 loss: 0.4079152512550354
  batch 650 loss: 0.42688980281352995
  batch 700 loss: 0.4410360872745514
  batch 750 loss: 0.4098392951488495
  batch 800 loss: 0.40335298627614974
  batch 850 loss: 0.4494674298167229
  batch 900 loss: 0.43916981399059296
LOSS train 0.43917 valid 0.76411, valid PER 22.75%
EPOCH 15:
  batch 50 loss: 0.39437661796808243
  batch 100 loss: 0.39521558821201325
  batch 150 loss: 0.39546880543231966
  batch 200 loss: 0.4067810189723968
  batch 250 loss: 0.42073580116033554
  batch 300 loss: 0.3879005628824234
  batch 350 loss: 0.3997599768638611
  batch 400 loss: 0.41067410469055177
  batch 450 loss: 0.40145267844200133
  batch 500 loss: 0.38646285951137543
  batch 550 loss: 0.396181053519249
  batch 600 loss: 0.41123071253299714
  batch 650 loss: 0.4217909288406372
  batch 700 loss: 0.417842208147049
  batch 750 loss: 0.41277863174676893
  batch 800 loss: 0.39445716053247454
  batch 850 loss: 0.4017842385172844
  batch 900 loss: 0.4100673294067383
LOSS train 0.41007 valid 0.77244, valid PER 22.77%
EPOCH 16:
  batch 50 loss: 0.3813135603070259
  batch 100 loss: 0.35692798644304274
  batch 150 loss: 0.3659113445878029
  batch 200 loss: 0.36582674741744997
  batch 250 loss: 0.37469679236412046
  batch 300 loss: 0.3792276957631111
  batch 350 loss: 0.3797549960017204
  batch 400 loss: 0.38456463724374773
  batch 450 loss: 0.3827484837174416
  batch 500 loss: 0.3515423932671547
  batch 550 loss: 0.35855375707149506
  batch 600 loss: 0.3590423020720482
  batch 650 loss: 0.37359804213047026
  batch 700 loss: 0.3598150098323822
  batch 750 loss: 0.3654359993338585
  batch 800 loss: 0.3791556519269943
  batch 850 loss: 0.36121694833040235
  batch 900 loss: 0.3724446579813957
LOSS train 0.37244 valid 0.77219, valid PER 22.81%
EPOCH 17:
  batch 50 loss: 0.35543647974729536
  batch 100 loss: 0.3584588772058487
  batch 150 loss: 0.3515999183058739
  batch 200 loss: 0.3445775431394577
  batch 250 loss: 0.35945928722620013
  batch 300 loss: 0.34838758409023285
  batch 350 loss: 0.32568259090185164
  batch 400 loss: 0.35874773770570756
  batch 450 loss: 0.3515791058540344
  batch 500 loss: 0.35210933178663256
  batch 550 loss: 0.34060457319021226
  batch 600 loss: 0.35111355751752854
  batch 650 loss: 0.3423411813378334
  batch 700 loss: 0.3531977626681328
  batch 750 loss: 0.3448674574494362
  batch 800 loss: 0.33387391626834867
  batch 850 loss: 0.35964386075735094
  batch 900 loss: 0.3386656638979912
LOSS train 0.33867 valid 0.77641, valid PER 22.74%
EPOCH 18:
  batch 50 loss: 0.3408588457107544
  batch 100 loss: 0.3399000158905983
  batch 150 loss: 0.35020941764116287
  batch 200 loss: 0.3452856144309044
  batch 250 loss: 0.35118244677782057
  batch 300 loss: 0.32948196470737456
  batch 350 loss: 0.3254907849431038
  batch 400 loss: 0.3253547590970993
  batch 450 loss: 0.3434851849079132
  batch 500 loss: 0.3353904429078102
  batch 550 loss: 0.34631810426712034
  batch 600 loss: 0.3164210891723633
  batch 650 loss: 0.32793293446302413
  batch 700 loss: 0.3550944292545319
  batch 750 loss: 0.33071758210659025
  batch 800 loss: 0.33045821756124494
  batch 850 loss: 0.3290709447860718
  batch 900 loss: 0.35061311841011045
LOSS train 0.35061 valid 0.77746, valid PER 22.74%
EPOCH 19:
  batch 50 loss: 0.32844500958919526
  batch 100 loss: 0.3251841962337494
  batch 150 loss: 0.324725162088871
  batch 200 loss: 0.3281019365787506
  batch 250 loss: 0.3227314960956573
  batch 300 loss: 0.33685576409101486
  batch 350 loss: 0.3222312059998512
  batch 400 loss: 0.3288788390159607
  batch 450 loss: 0.3483005115389824
  batch 500 loss: 0.3376492077112198
  batch 550 loss: 0.3242878520488739
  batch 600 loss: 0.32249452292919156
  batch 650 loss: 0.36452947586774825
  batch 700 loss: 0.3210210120677948
  batch 750 loss: 0.3202510440349579
  batch 800 loss: 0.33250510692596436
  batch 850 loss: 0.33467477083206176
  batch 900 loss: 0.3372111842036247
LOSS train 0.33721 valid 0.77918, valid PER 22.82%
EPOCH 20:
  batch 50 loss: 0.334406818151474
  batch 100 loss: 0.3256398475170135
  batch 150 loss: 0.310351981818676
  batch 200 loss: 0.33991869539022446
  batch 250 loss: 0.3396363419294357
  batch 300 loss: 0.33461748659610746
  batch 350 loss: 0.31720226019620895
  batch 400 loss: 0.32249478578567503
  batch 450 loss: 0.3224370804429054
  batch 500 loss: 0.3139131247997284
  batch 550 loss: 0.350805821120739
  batch 600 loss: 0.3136897364258766
  batch 650 loss: 0.3313571709394455
  batch 700 loss: 0.3271371859312058
  batch 750 loss: 0.3059783345460892
  batch 800 loss: 0.3375820690393448
  batch 850 loss: 0.33322319567203523
  batch 900 loss: 0.330844983458519
LOSS train 0.33084 valid 0.77951, valid PER 22.82%
Training finished in 12.0 minutes.
Model saved to checkpoints/20230118_095612/model_14
Loading model from checkpoints/20230118_095612/model_14
SUB: 15.21%, DEL: 7.59%, INS: 2.50%, COR: 77.20%, PER: 25.30%
