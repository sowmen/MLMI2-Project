Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0, schedule='false')
cuda:0
Total number of model parameters is 589992
EPOCH 1:
  batch 50 loss: 4.3941965675354
  batch 100 loss: 3.284748568534851
  batch 150 loss: 3.264264073371887
  batch 200 loss: 3.229462914466858
  batch 250 loss: 3.1915215492248534
  batch 300 loss: 3.002461953163147
  batch 350 loss: 2.919984984397888
  batch 400 loss: 2.769304385185242
  batch 450 loss: 2.724254264831543
  batch 500 loss: 2.6687059354782106
  batch 550 loss: 2.5079657888412474
  batch 600 loss: 2.444956159591675
  batch 650 loss: 2.3236273431777956
  batch 700 loss: 2.332012355327606
  batch 750 loss: 2.223632125854492
  batch 800 loss: 2.205831468105316
  batch 850 loss: 2.122885673046112
  batch 900 loss: 2.1024502110481262
LOSS train 2.10245 valid 1.93611, valid PER 64.88%
EPOCH 2:
  batch 50 loss: 1.9696519541740418
  batch 100 loss: 1.931185510158539
  batch 150 loss: 1.8766064882278441
  batch 200 loss: 1.84172687292099
  batch 250 loss: 1.7969807982444763
  batch 300 loss: 1.7342637634277345
  batch 350 loss: 1.6515875625610352
  batch 400 loss: 1.6329132223129272
  batch 450 loss: 1.5962414526939392
  batch 500 loss: 1.5581155180931092
  batch 550 loss: 1.5305570816993714
  batch 600 loss: 1.4844793200492858
  batch 650 loss: 1.5076344990730286
  batch 700 loss: 1.4728456783294677
  batch 750 loss: 1.4490694355964662
  batch 800 loss: 1.3748980116844178
  batch 850 loss: 1.3619695115089416
  batch 900 loss: 1.3745720720291137
LOSS train 1.37457 valid 1.31081, valid PER 39.61%
EPOCH 3:
  batch 50 loss: 1.3490059423446654
  batch 100 loss: 1.2797629523277283
  batch 150 loss: 1.301622862815857
  batch 200 loss: 1.275347695350647
  batch 250 loss: 1.25382275223732
  batch 300 loss: 1.2385657334327698
  batch 350 loss: 1.3117523407936096
  batch 400 loss: 1.277317043542862
  batch 450 loss: 1.2304756009578706
  batch 500 loss: 1.1874362885951997
  batch 550 loss: 1.2326062142848968
  batch 600 loss: 1.1822340762615204
  batch 650 loss: 1.1613886880874633
  batch 700 loss: 1.1773868143558501
  batch 750 loss: 1.2243722295761108
  batch 800 loss: 1.1422247362136841
  batch 850 loss: 1.178265688419342
  batch 900 loss: 1.0801574659347535
LOSS train 1.08016 valid 1.22218, valid PER 37.82%
EPOCH 4:
  batch 50 loss: 1.0856564521789551
  batch 100 loss: 1.0931043136119842
  batch 150 loss: 1.0639950084686278
  batch 200 loss: 1.0968192684650422
  batch 250 loss: 1.0991743624210357
  batch 300 loss: 1.1350607991218566
  batch 350 loss: 1.0212751054763793
  batch 400 loss: 1.0677314710617065
  batch 450 loss: 1.0572887134552003
  batch 500 loss: 1.0617993676662445
  batch 550 loss: 1.0543494653701782
  batch 600 loss: 1.1074020540714264
  batch 650 loss: 1.0768625926971436
  batch 700 loss: 1.0487913131713866
  batch 750 loss: 1.0016165697574615
  batch 800 loss: 0.9821504139900208
  batch 850 loss: 1.0036377537250518
  batch 900 loss: 1.0699312639236451
LOSS train 1.06993 valid 1.01536, valid PER 30.52%
EPOCH 5:
  batch 50 loss: 0.9605492436885834
  batch 100 loss: 0.9351985788345337
  batch 150 loss: 1.0050143611431122
  batch 200 loss: 0.940068575143814
  batch 250 loss: 0.9496530270576478
  batch 300 loss: 0.9537316727638244
  batch 350 loss: 0.9351244354248047
  batch 400 loss: 0.9655737793445587
  batch 450 loss: 0.9635726153850556
  batch 500 loss: 0.9594479191303253
  batch 550 loss: 0.9113637173175811
  batch 600 loss: 0.9740384614467621
  batch 650 loss: 0.9425889480113984
  batch 700 loss: 0.9852545893192292
  batch 750 loss: 0.923172105550766
  batch 800 loss: 0.9521462273597717
  batch 850 loss: 0.9524481058120727
  batch 900 loss: 0.9393421483039855
LOSS train 0.93934 valid 0.95929, valid PER 29.32%
EPOCH 6:
  batch 50 loss: 0.9137702870368958
  batch 100 loss: 0.8475555849075317
  batch 150 loss: 0.854592181444168
  batch 200 loss: 0.8707286298274994
  batch 250 loss: 0.9132560741901398
  batch 300 loss: 0.8923976171016693
  batch 350 loss: 0.8911977279186248
  batch 400 loss: 0.8679155623912811
  batch 450 loss: 0.9008041739463806
  batch 500 loss: 0.8708954167366028
  batch 550 loss: 0.9021061432361602
  batch 600 loss: 0.8533590126037598
  batch 650 loss: 0.885504869222641
  batch 700 loss: 0.8965926337242126
  batch 750 loss: 0.8737412810325622
  batch 800 loss: 0.8740727043151856
  batch 850 loss: 0.8573046576976776
  batch 900 loss: 0.850995032787323
LOSS train 0.85100 valid 0.93891, valid PER 28.33%
EPOCH 7:
  batch 50 loss: 0.828014622926712
  batch 100 loss: 0.8378043699264527
  batch 150 loss: 0.8016800951957702
  batch 200 loss: 0.7898082506656646
  batch 250 loss: 0.8280044269561767
  batch 300 loss: 0.7940347814559936
  batch 350 loss: 0.8298706698417664
  batch 400 loss: 0.812931649684906
  batch 450 loss: 0.809083793759346
  batch 500 loss: 0.7945464789867401
  batch 550 loss: 0.795083464384079
  batch 600 loss: 0.8214421808719635
  batch 650 loss: 0.8196379578113556
  batch 700 loss: 0.8363875019550323
  batch 750 loss: 0.7984512841701508
  batch 800 loss: 0.8165702414512634
  batch 850 loss: 0.8269290792942047
  batch 900 loss: 0.8378561902046203
LOSS train 0.83786 valid 0.89869, valid PER 27.97%
EPOCH 8:
  batch 50 loss: 0.7555503141880036
  batch 100 loss: 0.7384609389305115
  batch 150 loss: 0.7455094927549362
  batch 200 loss: 0.7368246066570282
  batch 250 loss: 0.748929500579834
  batch 300 loss: 0.7283744597434998
  batch 350 loss: 0.7971175837516785
  batch 400 loss: 0.733741905093193
  batch 450 loss: 0.7593316233158112
  batch 500 loss: 0.79028111577034
  batch 550 loss: 0.7139745688438416
  batch 600 loss: 0.7748576813936233
  batch 650 loss: 0.7977743804454803
  batch 700 loss: 0.7511314857006073
  batch 750 loss: 0.7337510883808136
  batch 800 loss: 0.7529335206747055
  batch 850 loss: 0.7484471338987351
  batch 900 loss: 0.7863347840309143
LOSS train 0.78633 valid 0.88382, valid PER 27.12%
EPOCH 9:
  batch 50 loss: 0.6669477087259292
  batch 100 loss: 0.6973028004169464
  batch 150 loss: 0.7047314125299454
  batch 200 loss: 0.6857791465520858
  batch 250 loss: 0.7081258690357208
  batch 300 loss: 0.7193163156509399
  batch 350 loss: 0.7295745432376861
  batch 400 loss: 0.7140810441970825
  batch 450 loss: 0.7200750279426574
  batch 500 loss: 0.6936662995815277
  batch 550 loss: 0.6963852483034134
  batch 600 loss: 0.7336309856176376
  batch 650 loss: 0.7187521374225616
  batch 700 loss: 0.6899487978219986
  batch 750 loss: 0.6988684141635895
  batch 800 loss: 0.7256148898601532
  batch 850 loss: 0.7922485250234604
  batch 900 loss: 0.7514334088563919
LOSS train 0.75143 valid 0.91839, valid PER 27.27%
EPOCH 10:
  batch 50 loss: 0.6661132580041885
  batch 100 loss: 0.6605963999032974
  batch 150 loss: 0.6765306621789933
  batch 200 loss: 0.6850544357299805
  batch 250 loss: 0.699600448012352
  batch 300 loss: 0.6556013512611389
  batch 350 loss: 0.6745528709888459
  batch 400 loss: 0.6418652737140655
  batch 450 loss: 0.65162995159626
  batch 500 loss: 0.7031590497493744
  batch 550 loss: 0.716406626701355
  batch 600 loss: 0.673042888045311
  batch 650 loss: 0.6557891982793808
  batch 700 loss: 0.6646806514263153
  batch 750 loss: 0.6587764620780945
  batch 800 loss: 0.6959243273735046
  batch 850 loss: 0.6774295365810394
  batch 900 loss: 0.7051574695110321
LOSS train 0.70516 valid 0.88279, valid PER 26.83%
EPOCH 11:
  batch 50 loss: 0.6012131029367447
  batch 100 loss: 0.5639849656820297
  batch 150 loss: 0.5870958626270294
  batch 200 loss: 0.6434718048572541
  batch 250 loss: 0.6534472209215164
  batch 300 loss: 0.6072855180501938
  batch 350 loss: 0.6429130738973617
  batch 400 loss: 0.6330084866285324
  batch 450 loss: 0.623069988489151
  batch 500 loss: 0.6146953016519546
  batch 550 loss: 0.644097980260849
  batch 600 loss: 0.6375685900449752
  batch 650 loss: 0.690280282497406
  batch 700 loss: 0.6168658077716828
  batch 750 loss: 0.6267649149894714
  batch 800 loss: 0.6579253697395324
  batch 850 loss: 0.6805579435825347
  batch 900 loss: 0.6729430550336838
LOSS train 0.67294 valid 0.83599, valid PER 25.54%
EPOCH 12:
  batch 50 loss: 0.596100947856903
  batch 100 loss: 0.5676178389787674
  batch 150 loss: 0.5560661286115647
  batch 200 loss: 0.5752378481626511
  batch 250 loss: 0.5827573513984681
  batch 300 loss: 0.5651599305868149
  batch 350 loss: 0.5684369474649429
  batch 400 loss: 0.606053119301796
  batch 450 loss: 0.6014910107851028
  batch 500 loss: 0.6301991212368011
  batch 550 loss: 0.5802622646093368
  batch 600 loss: 0.5965659534931183
  batch 650 loss: 0.6452137327194214
  batch 700 loss: 0.6120391279459
  batch 750 loss: 0.6012055480480194
  batch 800 loss: 0.6210330045223236
  batch 850 loss: 0.6586779367923736
  batch 900 loss: 0.6239367145299911
LOSS train 0.62394 valid 0.80513, valid PER 24.76%
EPOCH 13:
  batch 50 loss: 0.5252718257904053
  batch 100 loss: 0.512458770275116
  batch 150 loss: 0.5273335492610931
  batch 200 loss: 0.5625943505764007
  batch 250 loss: 0.5396688461303711
  batch 300 loss: 0.5459791004657746
  batch 350 loss: 0.5215781569480896
  batch 400 loss: 0.5574796867370605
  batch 450 loss: 0.58365667283535
  batch 500 loss: 0.5427432078123092
  batch 550 loss: 0.5885188913345337
  batch 600 loss: 0.5660848069190979
  batch 650 loss: 0.5828908389806747
  batch 700 loss: 0.5714464783668518
  batch 750 loss: 0.545769419670105
  batch 800 loss: 0.5626320785284042
  batch 850 loss: 0.5936372983455658
  batch 900 loss: 0.5954639321565628
LOSS train 0.59546 valid 0.83554, valid PER 24.60%
EPOCH 14:
  batch 50 loss: 0.49113684296607973
  batch 100 loss: 0.5012099814414978
  batch 150 loss: 0.5104944294691086
  batch 200 loss: 0.5038398057222366
  batch 250 loss: 0.496340628862381
  batch 300 loss: 0.5393307626247406
  batch 350 loss: 0.5084719783067704
  batch 400 loss: 0.5198173367977142
  batch 450 loss: 0.5279387885332107
  batch 500 loss: 0.5302013611793518
  batch 550 loss: 0.5491418349742889
  batch 600 loss: 0.5320643383264542
  batch 650 loss: 0.5576186698675155
  batch 700 loss: 0.5759307587146759
  batch 750 loss: 0.5341294413805008
  batch 800 loss: 0.5061689358949661
  batch 850 loss: 0.5495568108558655
  batch 900 loss: 0.5431498003005981
LOSS train 0.54315 valid 0.83326, valid PER 24.71%
EPOCH 15:
  batch 50 loss: 0.4583127921819687
  batch 100 loss: 0.4675125756859779
  batch 150 loss: 0.47907913982868194
  batch 200 loss: 0.4770998102426529
  batch 250 loss: 0.49831197679042816
  batch 300 loss: 0.46864366114139555
  batch 350 loss: 0.4843463218212128
  batch 400 loss: 0.4882276064157486
  batch 450 loss: 0.49056808471679686
  batch 500 loss: 0.4844377589225769
  batch 550 loss: 0.5028902477025986
  batch 600 loss: 0.5074626237154007
  batch 650 loss: 0.5314521890878677
  batch 700 loss: 0.5213518005609512
  batch 750 loss: 0.5080943024158477
  batch 800 loss: 0.5050588685274124
  batch 850 loss: 0.48176677882671354
  batch 900 loss: 0.5109365504980087
LOSS train 0.51094 valid 0.85304, valid PER 24.04%
EPOCH 16:
  batch 50 loss: 0.4595726299285889
  batch 100 loss: 0.42916542530059815
  batch 150 loss: 0.43589914828538895
  batch 200 loss: 0.44480056822299957
  batch 250 loss: 0.4567360812425613
  batch 300 loss: 0.44988842368125914
  batch 350 loss: 0.4608119946718216
  batch 400 loss: 0.4771034860610962
  batch 450 loss: 0.4710849261283874
  batch 500 loss: 0.4443763893842697
  batch 550 loss: 0.48629374444484713
  batch 600 loss: 0.4756097853183746
  batch 650 loss: 0.4802069813013077
  batch 700 loss: 0.46401664435863493
  batch 750 loss: 0.4995984309911728
  batch 800 loss: 0.489612977206707
  batch 850 loss: 0.48555006086826324
  batch 900 loss: 0.464346809387207
LOSS train 0.46435 valid 0.85716, valid PER 23.75%
EPOCH 17:
  batch 50 loss: 0.39057462066411974
  batch 100 loss: 0.4120874184370041
  batch 150 loss: 0.418023162484169
  batch 200 loss: 0.40374322652816774
  batch 250 loss: 0.41872006833553316
  batch 300 loss: 0.43602016150951384
  batch 350 loss: 0.4202877312898636
  batch 400 loss: 0.45425518721342084
  batch 450 loss: 0.43706785917282104
  batch 500 loss: 0.44364209949970246
  batch 550 loss: 0.44279290318489073
  batch 600 loss: 0.4694842892885208
  batch 650 loss: 0.4627665293216705
  batch 700 loss: 0.4561315762996674
  batch 750 loss: 0.439405777156353
  batch 800 loss: 0.44795758575201033
  batch 850 loss: 0.4709122934937477
  batch 900 loss: 0.4622600191831589
LOSS train 0.46226 valid 0.89304, valid PER 24.82%
EPOCH 18:
  batch 50 loss: 0.38730276495218274
  batch 100 loss: 0.38179971754550934
  batch 150 loss: 0.4401949137449265
  batch 200 loss: 0.3999810612201691
  batch 250 loss: 0.40966958820819854
  batch 300 loss: 0.4038561788201332
  batch 350 loss: 0.4149423411488533
  batch 400 loss: 0.41486599773168564
  batch 450 loss: 0.42963751435279846
  batch 500 loss: 0.41428433537483217
  batch 550 loss: 0.4313895058631897
  batch 600 loss: 0.4072791624069214
  batch 650 loss: 0.40567880153656005
  batch 700 loss: 0.4254134380817413
  batch 750 loss: 0.40967398792505266
  batch 800 loss: 0.4242557457089424
  batch 850 loss: 0.44822686672210693
  batch 900 loss: 0.4664115983247757
LOSS train 0.46641 valid 0.89746, valid PER 24.76%
EPOCH 19:
  batch 50 loss: 0.3576202005147934
  batch 100 loss: 0.3526474130153656
  batch 150 loss: 0.3435689598321915
  batch 200 loss: 0.37984385818243027
  batch 250 loss: 0.3726868537068367
  batch 300 loss: 0.4012089115381241
  batch 350 loss: 0.37833970069885253
  batch 400 loss: 0.3800228944420814
  batch 450 loss: 0.40308100342750547
  batch 500 loss: 0.4168398278951645
  batch 550 loss: 0.38154059529304507
  batch 600 loss: 0.38501040130853653
  batch 650 loss: 0.4284582149982452
  batch 700 loss: 0.4021804976463318
  batch 750 loss: 0.4076813107728958
  batch 800 loss: 0.43734241485595704
  batch 850 loss: 0.40687752187252046
  batch 900 loss: 0.4224058499932289
LOSS train 0.42241 valid 0.90671, valid PER 24.54%
EPOCH 20:
  batch 50 loss: 0.3455477666854858
  batch 100 loss: 0.33227876007556917
  batch 150 loss: 0.3437698829174042
  batch 200 loss: 0.3342174923419952
  batch 250 loss: 0.3464300963282585
  batch 300 loss: 0.3665846613049507
  batch 350 loss: 0.3440416008234024
  batch 400 loss: 0.36651106297969815
  batch 450 loss: 0.37597869426012037
  batch 500 loss: 0.34482027977705004
  batch 550 loss: 0.39682964742183685
  batch 600 loss: 0.3938276347517967
  batch 650 loss: 0.4311553752422333
  batch 700 loss: 0.4007089763879776
  batch 750 loss: 0.3980857449769974
  batch 800 loss: 0.40372469782829284
  batch 850 loss: 0.422904417514801
  batch 900 loss: 0.3990590560436249
LOSS train 0.39906 valid 0.91891, valid PER 24.75%
Training finished in 12.0 minutes.
Model saved to checkpoints/20230118_095946/model_12
Loading model from checkpoints/20230118_095946/model_12
SUB: 15.58%, DEL: 8.31%, INS: 2.21%, COR: 76.10%, PER: 26.11%
