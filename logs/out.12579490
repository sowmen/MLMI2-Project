Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0, schedule='false')
cuda:0
Total number of model parameters is 2240552
EPOCH 1:
  batch 50 loss: 4.480451531410218
  batch 100 loss: 3.036760296821594
  batch 150 loss: 2.893550295829773
  batch 200 loss: 2.816670632362366
  batch 250 loss: 2.61572847366333
  batch 300 loss: 2.4601792430877687
  batch 350 loss: 2.3499044466018675
  batch 400 loss: 2.345321536064148
  batch 450 loss: 2.3069192457199095
  batch 500 loss: 2.133575873374939
  batch 550 loss: 2.107434241771698
  batch 600 loss: 2.0350398206710816
  batch 650 loss: 1.9411529803276062
  batch 700 loss: 1.9423598885536193
  batch 750 loss: 1.9027200269699096
  batch 800 loss: 1.835759425163269
  batch 850 loss: 1.8243794345855713
  batch 900 loss: 1.7592856669425965
LOSS train 1.75929 valid 1.70421, valid PER 63.86%
EPOCH 2:
  batch 50 loss: 1.7573502135276795
  batch 100 loss: 1.6412229061126709
  batch 150 loss: 1.6860610270500183
  batch 200 loss: 1.6425478219985963
  batch 250 loss: 1.651375970840454
  batch 300 loss: 1.5873737835884094
  batch 350 loss: 1.5355422377586365
  batch 400 loss: 1.53106858253479
  batch 450 loss: 1.4854021787643432
  batch 500 loss: 1.5265144300460816
  batch 550 loss: 1.5201030468940735
  batch 600 loss: 1.4625834465026855
  batch 650 loss: 1.5031888937950135
  batch 700 loss: 1.459698624610901
  batch 750 loss: 1.4525257492065429
  batch 800 loss: 1.3967877769470214
  batch 850 loss: 1.4180325603485107
  batch 900 loss: 1.4105640769004821
LOSS train 1.41056 valid 1.37270, valid PER 45.56%
EPOCH 3:
  batch 50 loss: 1.375219805240631
  batch 100 loss: 1.3377807712554932
  batch 150 loss: 1.3495364022254943
  batch 200 loss: 1.3102826404571533
  batch 250 loss: 1.310372849702835
  batch 300 loss: 1.2977133107185364
  batch 350 loss: 1.365809407234192
  batch 400 loss: 1.321767828464508
  batch 450 loss: 1.292916669845581
  batch 500 loss: 1.2677216458320617
  batch 550 loss: 1.2899787282943727
  batch 600 loss: 1.2433963441848754
  batch 650 loss: 1.2271578538417816
  batch 700 loss: 1.2632774496078492
  batch 750 loss: 1.2981808841228486
  batch 800 loss: 1.2165294289588928
  batch 850 loss: 1.2332292759418488
  batch 900 loss: 1.1551418817043304
LOSS train 1.15514 valid 1.24865, valid PER 37.73%
EPOCH 4:
  batch 50 loss: 1.1476293480396271
  batch 100 loss: 1.1794521355628966
  batch 150 loss: 1.1189851582050323
  batch 200 loss: 1.161473172903061
  batch 250 loss: 1.1606369364261626
  batch 300 loss: 1.1528183352947234
  batch 350 loss: 1.0762381327152253
  batch 400 loss: 1.1337956583499909
  batch 450 loss: 1.1103502690792084
  batch 500 loss: 1.0918303561210632
  batch 550 loss: 1.109939957857132
  batch 600 loss: 1.1392241191864014
  batch 650 loss: 1.1085734260082245
  batch 700 loss: 1.1078484952449799
  batch 750 loss: 1.067129567861557
  batch 800 loss: 1.0418172252178193
  batch 850 loss: 1.0701314282417298
  batch 900 loss: 1.123500943183899
LOSS train 1.12350 valid 1.07769, valid PER 33.72%
EPOCH 5:
  batch 50 loss: 1.0179617238044738
  batch 100 loss: 1.0186753106117248
  batch 150 loss: 1.0798149800300598
  batch 200 loss: 0.9998123109340668
  batch 250 loss: 1.0093281865119934
  batch 300 loss: 1.023880614042282
  batch 350 loss: 1.008390680551529
  batch 400 loss: 1.009586342573166
  batch 450 loss: 0.9834971082210541
  batch 500 loss: 1.028965960741043
  batch 550 loss: 0.971151465177536
  batch 600 loss: 1.0569012439250947
  batch 650 loss: 0.9964003622531891
  batch 700 loss: 1.0395076203346252
  batch 750 loss: 0.9515807890892028
  batch 800 loss: 1.0185472321510316
  batch 850 loss: 0.9849498224258423
  batch 900 loss: 0.9880319285392761
LOSS train 0.98803 valid 1.02838, valid PER 31.72%
EPOCH 6:
  batch 50 loss: 0.9655160391330719
  batch 100 loss: 0.9121053647994996
  batch 150 loss: 0.9035558497905731
  batch 200 loss: 0.9195900130271911
  batch 250 loss: 0.9728974175453186
  batch 300 loss: 0.9635466301441192
  batch 350 loss: 0.9551456379890442
  batch 400 loss: 0.935892825126648
  batch 450 loss: 0.9601299011707306
  batch 500 loss: 0.9401681900024415
  batch 550 loss: 0.9701925075054169
  batch 600 loss: 0.9113047349452973
  batch 650 loss: 0.9435538530349732
  batch 700 loss: 0.9580724334716797
  batch 750 loss: 0.926276445388794
  batch 800 loss: 0.9264765977859497
  batch 850 loss: 0.8893822872638703
  batch 900 loss: 0.930274053812027
LOSS train 0.93027 valid 1.00226, valid PER 30.46%
EPOCH 7:
  batch 50 loss: 0.8902683830261231
  batch 100 loss: 0.8785355579853058
  batch 150 loss: 0.8456645357608795
  batch 200 loss: 0.8531383293867111
  batch 250 loss: 0.8500623655319214
  batch 300 loss: 0.8454922592639923
  batch 350 loss: 0.8914900422096252
  batch 400 loss: 0.8771198523044587
  batch 450 loss: 0.862284106016159
  batch 500 loss: 0.8688542175292969
  batch 550 loss: 0.8485106766223908
  batch 600 loss: 0.8738678932189942
  batch 650 loss: 0.8419744801521302
  batch 700 loss: 0.8858733105659485
  batch 750 loss: 0.8595797455310822
  batch 800 loss: 0.8617224788665772
  batch 850 loss: 0.8766938197612762
  batch 900 loss: 0.8793565881252289
LOSS train 0.87936 valid 0.96138, valid PER 29.52%
EPOCH 8:
  batch 50 loss: 0.8093170869350433
  batch 100 loss: 0.7798851442337036
  batch 150 loss: 0.7964030587673188
  batch 200 loss: 0.7889756441116333
  batch 250 loss: 0.8061422598361969
  batch 300 loss: 0.7726389563083649
  batch 350 loss: 0.8422564148902894
  batch 400 loss: 0.8007101130485534
  batch 450 loss: 0.8162084138393402
  batch 500 loss: 0.8452399802207947
  batch 550 loss: 0.7918647575378418
  batch 600 loss: 0.8324983775615692
  batch 650 loss: 0.850428410768509
  batch 700 loss: 0.7879740983247757
  batch 750 loss: 0.8209268343448639
  batch 800 loss: 0.8401027977466583
  batch 850 loss: 0.8620860743522644
  batch 900 loss: 0.86061488032341
LOSS train 0.86061 valid 0.95892, valid PER 29.00%
EPOCH 9:
  batch 50 loss: 0.7346617436408996
  batch 100 loss: 0.7410519117116928
  batch 150 loss: 0.7664897507429123
  batch 200 loss: 0.7366763365268707
  batch 250 loss: 0.7667495757341385
  batch 300 loss: 0.7818515241146088
  batch 350 loss: 0.7874929565191269
  batch 400 loss: 0.7619821512699128
  batch 450 loss: 0.7789964759349823
  batch 500 loss: 0.739115653038025
  batch 550 loss: 0.7813061660528183
  batch 600 loss: 0.7990689039230346
  batch 650 loss: 0.7622101712226867
  batch 700 loss: 0.7606315833330154
  batch 750 loss: 0.7687370079755783
  batch 800 loss: 0.7860400903224946
  batch 850 loss: 0.79866730093956
  batch 900 loss: 0.7637553608417511
LOSS train 0.76376 valid 0.94000, valid PER 27.94%
EPOCH 10:
  batch 50 loss: 0.6843362981081009
  batch 100 loss: 0.6815292984247208
  batch 150 loss: 0.7046440017223358
  batch 200 loss: 0.7177791893482208
  batch 250 loss: 0.7209031289815903
  batch 300 loss: 0.6784727382659912
  batch 350 loss: 0.7114308738708496
  batch 400 loss: 0.670889904499054
  batch 450 loss: 0.7052986288070678
  batch 500 loss: 0.7565376913547516
  batch 550 loss: 0.7676240992546082
  batch 600 loss: 0.7454918694496154
  batch 650 loss: 0.7443998116254806
  batch 700 loss: 0.7627990627288819
  batch 750 loss: 0.7260757780075073
  batch 800 loss: 0.745499529838562
  batch 850 loss: 0.74245849609375
  batch 900 loss: 0.7476614665985107
LOSS train 0.74766 valid 0.93631, valid PER 28.38%
EPOCH 11:
  batch 50 loss: 0.6533399957418442
  batch 100 loss: 0.6156093323230744
  batch 150 loss: 0.6428356516361237
  batch 200 loss: 0.6860786598920822
  batch 250 loss: 0.6856143826246262
  batch 300 loss: 0.6487529468536377
  batch 350 loss: 0.6551433038711548
  batch 400 loss: 0.6853553307056427
  batch 450 loss: 0.7000917875766755
  batch 500 loss: 0.6787619245052338
  batch 550 loss: 0.6828991168737412
  batch 600 loss: 0.6599361687898636
  batch 650 loss: 0.7314768052101135
  batch 700 loss: 0.6844604700803757
  batch 750 loss: 0.692684320807457
  batch 800 loss: 0.7295694953203201
  batch 850 loss: 0.7361972856521607
  batch 900 loss: 0.7146484053134918
LOSS train 0.71465 valid 0.90501, valid PER 26.70%
EPOCH 12:
  batch 50 loss: 0.6149048739671708
  batch 100 loss: 0.613219627737999
  batch 150 loss: 0.569068169593811
  batch 200 loss: 0.6035703283548355
  batch 250 loss: 0.6381241083145142
  batch 300 loss: 0.6072502845525741
  batch 350 loss: 0.5988006597757339
  batch 400 loss: 0.6339657920598983
  batch 450 loss: 0.6526977825164795
  batch 500 loss: 0.6416178351640701
  batch 550 loss: 0.615627070069313
  batch 600 loss: 0.6322358405590057
  batch 650 loss: 0.6458065211772919
  batch 700 loss: 0.6522765266895294
  batch 750 loss: 0.6283813536167144
  batch 800 loss: 0.6336452394723893
  batch 850 loss: 0.695665180683136
  batch 900 loss: 0.6697291493415832
LOSS train 0.66973 valid 0.92937, valid PER 27.24%
EPOCH 13:
  batch 50 loss: 0.5537254136800766
  batch 100 loss: 0.5724918919801713
  batch 150 loss: 0.5563091379404068
  batch 200 loss: 0.5924541056156158
  batch 250 loss: 0.5751416343450546
  batch 300 loss: 0.5834055668115616
  batch 350 loss: 0.5769664347171783
  batch 400 loss: 0.6056347590684891
  batch 450 loss: 0.5914728915691376
  batch 500 loss: 0.5906685709953308
  batch 550 loss: 0.6273475021123887
  batch 600 loss: 0.6017208003997803
  batch 650 loss: 0.6265075290203095
  batch 700 loss: 0.6383881109952927
  batch 750 loss: 0.5858454436063767
  batch 800 loss: 0.6021864610910416
  batch 850 loss: 0.6372088819742203
  batch 900 loss: 0.6366605967283249
LOSS train 0.63666 valid 0.95705, valid PER 26.97%
EPOCH 14:
  batch 50 loss: 0.547770825624466
  batch 100 loss: 0.5292102539539337
  batch 150 loss: 0.5296674519777298
  batch 200 loss: 0.5255282491445541
  batch 250 loss: 0.5415759646892547
  batch 300 loss: 0.5717726182937622
  batch 350 loss: 0.5064201939105988
  batch 400 loss: 0.541779562830925
  batch 450 loss: 0.5495906239748001
  batch 500 loss: 0.5686217248439789
  batch 550 loss: 0.5939160174131394
  batch 600 loss: 0.562857745885849
  batch 650 loss: 0.5729204910993576
  batch 700 loss: 0.6144982975721359
  batch 750 loss: 0.5892622655630112
  batch 800 loss: 0.5686046069860459
  batch 850 loss: 0.5999328112602234
  batch 900 loss: 0.5961857444047928
LOSS train 0.59619 valid 0.93144, valid PER 26.85%
EPOCH 15:
  batch 50 loss: 0.48245001494884493
  batch 100 loss: 0.480357757806778
  batch 150 loss: 0.5101891028881073
  batch 200 loss: 0.504288232922554
  batch 250 loss: 0.5380695062875748
  batch 300 loss: 0.5559764355421066
  batch 350 loss: 0.536349760890007
  batch 400 loss: 0.5258558803796768
  batch 450 loss: 0.5025500321388244
  batch 500 loss: 0.5112361800670624
  batch 550 loss: 0.5237362194061279
  batch 600 loss: 0.5607423180341721
  batch 650 loss: 0.5559696829319001
  batch 700 loss: 0.5536441314220428
  batch 750 loss: 0.5384852206707
  batch 800 loss: 0.5283609104156494
  batch 850 loss: 0.5261531925201416
  batch 900 loss: 0.5318910640478134
LOSS train 0.53189 valid 0.94563, valid PER 25.99%
EPOCH 16:
  batch 50 loss: 0.4551834124326706
  batch 100 loss: 0.4560288947820663
  batch 150 loss: 0.468912935256958
  batch 200 loss: 0.43581812441349027
  batch 250 loss: 0.4657829225063324
  batch 300 loss: 0.465457079410553
  batch 350 loss: 0.48951796889305116
  batch 400 loss: 0.4949687385559082
  batch 450 loss: 0.5018726426362992
  batch 500 loss: 0.4513030505180359
  batch 550 loss: 0.4622581392526627
  batch 600 loss: 0.4807786309719086
  batch 650 loss: 0.5177640837430953
  batch 700 loss: 0.5032803583145141
  batch 750 loss: 0.5228689152002335
  batch 800 loss: 0.5281051623821259
  batch 850 loss: 0.5276823318004609
  batch 900 loss: 0.5094550025463104
LOSS train 0.50946 valid 0.98277, valid PER 26.39%
EPOCH 17:
  batch 50 loss: 0.4255133992433548
  batch 100 loss: 0.4132904997467995
  batch 150 loss: 0.41996305108070375
  batch 200 loss: 0.4102973154187202
  batch 250 loss: 0.4350702702999115
  batch 300 loss: 0.4441114944219589
  batch 350 loss: 0.4384558883309364
  batch 400 loss: 0.4873207646608353
  batch 450 loss: 0.5290965861082078
  batch 500 loss: 0.4939268445968628
  batch 550 loss: 0.48170257270336153
  batch 600 loss: 0.509476563334465
  batch 650 loss: 0.4785285252332687
  batch 700 loss: 0.5006944227218628
  batch 750 loss: 0.475552396774292
  batch 800 loss: 0.4719861400127411
  batch 850 loss: 0.48062790036201475
  batch 900 loss: 0.48791232228279113
LOSS train 0.48791 valid 0.98160, valid PER 26.47%
EPOCH 18:
  batch 50 loss: 0.400108223259449
  batch 100 loss: 0.38256343126297
  batch 150 loss: 0.41211806297302245
  batch 200 loss: 0.41089712500572206
  batch 250 loss: 0.41765644133090973
  batch 300 loss: 0.41264592587947846
  batch 350 loss: 0.4194772779941559
  batch 400 loss: 0.43481920957565307
  batch 450 loss: 0.4458949589729309
  batch 500 loss: 0.43289356529712675
  batch 550 loss: 0.44761738538742063
  batch 600 loss: 0.41038916647434237
  batch 650 loss: 0.4209175169467926
  batch 700 loss: 0.43715281307697296
  batch 750 loss: 0.4498509046435356
  batch 800 loss: 0.44731915056705474
  batch 850 loss: 0.4549136936664581
  batch 900 loss: 0.44631351590156554
LOSS train 0.44631 valid 0.99425, valid PER 27.07%
EPOCH 19:
  batch 50 loss: 0.35180985540151594
  batch 100 loss: 0.3477540177106857
  batch 150 loss: 0.3492261618375778
  batch 200 loss: 0.3743357166647911
  batch 250 loss: 0.35092312186956404
  batch 300 loss: 0.3690196189284325
  batch 350 loss: 0.3731983387470245
  batch 400 loss: 0.38106783479452133
  batch 450 loss: 0.4099098452925682
  batch 500 loss: 0.3718367263674736
  batch 550 loss: 0.36640785753726957
  batch 600 loss: 0.385203592479229
  batch 650 loss: 0.4419651365280151
  batch 700 loss: 0.4613451239466667
  batch 750 loss: 0.4581942504644394
  batch 800 loss: 0.4771413069963455
  batch 850 loss: 0.4606535071134567
  batch 900 loss: 0.47221853375434875
LOSS train 0.47222 valid 0.99763, valid PER 26.68%
EPOCH 20:
  batch 50 loss: 0.33838158667087553
  batch 100 loss: 0.32611952304840086
  batch 150 loss: 0.35127356857061387
  batch 200 loss: 0.35928656250238417
  batch 250 loss: 0.37411458909511563
  batch 300 loss: 0.37756525695323945
  batch 350 loss: 0.3780508890748024
  batch 400 loss: 0.4265631401538849
  batch 450 loss: 0.433271626830101
  batch 500 loss: 0.3883324632048607
  batch 550 loss: 0.42838560193777087
  batch 600 loss: 0.40293638229370116
  batch 650 loss: 0.4124228638410568
  batch 700 loss: 0.41255705773830414
  batch 750 loss: 0.4085687631368637
  batch 800 loss: 0.4212752681970596
  batch 850 loss: 0.402753646671772
  batch 900 loss: 0.4118694230914116
LOSS train 0.41187 valid 1.05356, valid PER 27.71%
Training finished in 16.0 minutes.
Model saved to checkpoints/20230118_100620/model_11
Loading model from checkpoints/20230118_100620/model_11
SUB: 15.60%, DEL: 10.82%, INS: 2.59%, COR: 73.59%, PER: 29.00%
