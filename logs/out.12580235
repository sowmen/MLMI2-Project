Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0, schedule='true')
cuda:0
Total number of model parameters is 589992
EPOCH 1:
  batch 50 loss: 5.2972459936141965
  batch 100 loss: 3.46484929561615
  batch 150 loss: 3.3469053077697755
  batch 200 loss: 3.2598557662963867
  batch 250 loss: 3.2525441074371337
  batch 300 loss: 3.1650044298171998
  batch 350 loss: 3.0856646633148195
  batch 400 loss: 2.9018338394165037
  batch 450 loss: 2.8406873655319216
  batch 500 loss: 2.722760195732117
  batch 550 loss: 2.624941339492798
  batch 600 loss: 2.5439368534088134
  batch 650 loss: 2.420835604667664
  batch 700 loss: 2.378376455307007
  batch 750 loss: 2.2988366198539736
  batch 800 loss: 2.271900222301483
  batch 850 loss: 2.1982399892807005
  batch 900 loss: 2.156350836753845
LOSS train 2.15635 valid 2.08841, valid PER 68.32%
EPOCH 2:
  batch 50 loss: 2.06125079870224
  batch 100 loss: 1.9824039840698242
  batch 150 loss: 1.9199171233177186
  batch 200 loss: 1.8899313449859618
  batch 250 loss: 1.8631279063224793
  batch 300 loss: 1.8002060532569886
  batch 350 loss: 1.6996095371246338
  batch 400 loss: 1.6856101489067077
  batch 450 loss: 1.6137387084960937
  batch 500 loss: 1.6213771986961365
  batch 550 loss: 1.6046157002449035
  batch 600 loss: 1.5427885365486145
  batch 650 loss: 1.5540641093254088
  batch 700 loss: 1.4931026792526245
  batch 750 loss: 1.4786575984954835
  batch 800 loss: 1.4054465794563293
  batch 850 loss: 1.411069598197937
  batch 900 loss: 1.4334540033340455
LOSS train 1.43345 valid 1.36713, valid PER 40.64%
EPOCH 3:
  batch 50 loss: 1.3697084927558898
  batch 100 loss: 1.3393633627891541
  batch 150 loss: 1.335059139728546
  batch 200 loss: 1.323627791404724
  batch 250 loss: 1.2803516054153443
  batch 300 loss: 1.2863956785202026
  batch 350 loss: 1.3151682376861573
  batch 400 loss: 1.2961351823806764
  batch 450 loss: 1.2808317518234253
  batch 500 loss: 1.2263117516040802
  batch 550 loss: 1.2485601615905761
  batch 600 loss: 1.2181100356578827
  batch 650 loss: 1.199956600666046
  batch 700 loss: 1.2091116964817048
  batch 750 loss: 1.2502754378318786
  batch 800 loss: 1.1805727338790895
  batch 850 loss: 1.2187029385566712
  batch 900 loss: 1.1313953113555908
LOSS train 1.13140 valid 1.20563, valid PER 36.79%
EPOCH 4:
  batch 50 loss: 1.1314502692222594
  batch 100 loss: 1.14722265958786
  batch 150 loss: 1.1022096192836761
  batch 200 loss: 1.135741513967514
  batch 250 loss: 1.1218579971790315
  batch 300 loss: 1.155731393098831
  batch 350 loss: 1.0627505731582643
  batch 400 loss: 1.1114890134334565
  batch 450 loss: 1.0864132714271546
  batch 500 loss: 1.0792584109306336
  batch 550 loss: 1.0851035749912261
  batch 600 loss: 1.1203745818138122
  batch 650 loss: 1.1012089240550995
  batch 700 loss: 1.0508229041099548
  batch 750 loss: 1.041924706697464
  batch 800 loss: 1.0186068439483642
  batch 850 loss: 1.0413893043994904
  batch 900 loss: 1.100564649105072
LOSS train 1.10056 valid 1.05750, valid PER 32.14%
EPOCH 5:
  batch 50 loss: 0.9993611586093902
  batch 100 loss: 0.991665108203888
  batch 150 loss: 1.037471090555191
  batch 200 loss: 0.9718547582626342
  batch 250 loss: 0.9836671566963195
  batch 300 loss: 1.002602024078369
  batch 350 loss: 0.984995448589325
  batch 400 loss: 1.0084834575653077
  batch 450 loss: 0.9800132656097412
  batch 500 loss: 1.0194150137901306
  batch 550 loss: 0.9287547600269318
  batch 600 loss: 1.0120247793197632
  batch 650 loss: 0.9711106812953949
  batch 700 loss: 1.020405044555664
  batch 750 loss: 0.9645860648155212
  batch 800 loss: 0.9673130440711976
  batch 850 loss: 0.9692270302772522
  batch 900 loss: 0.9629923772811889
LOSS train 0.96299 valid 1.00186, valid PER 30.42%
EPOCH 6:
  batch 50 loss: 0.9610521447658539
  batch 100 loss: 0.8973733913898468
  batch 150 loss: 0.8977534008026123
  batch 200 loss: 0.9051595103740692
  batch 250 loss: 0.9314675009250641
  batch 300 loss: 0.9089004290103913
  batch 350 loss: 0.8985146582126617
  batch 400 loss: 0.9001055860519409
  batch 450 loss: 0.9211274003982544
  batch 500 loss: 0.9054423189163208
  batch 550 loss: 0.9330447876453399
  batch 600 loss: 0.9034509909152985
  batch 650 loss: 0.9004487013816833
  batch 700 loss: 0.905729912519455
  batch 750 loss: 0.9010639584064484
  batch 800 loss: 0.8968281960487365
  batch 850 loss: 0.885507527589798
  batch 900 loss: 0.8859744036197662
LOSS train 0.88597 valid 0.97260, valid PER 29.20%
EPOCH 7:
  batch 50 loss: 0.8783409333229065
  batch 100 loss: 0.8731612694263459
  batch 150 loss: 0.8268999326229095
  batch 200 loss: 0.8321938323974609
  batch 250 loss: 0.832190945148468
  batch 300 loss: 0.8344632971286774
  batch 350 loss: 0.847619503736496
  batch 400 loss: 0.826863843202591
  batch 450 loss: 0.8312816262245178
  batch 500 loss: 0.8496472203731537
  batch 550 loss: 0.838136591911316
  batch 600 loss: 0.8512806844711304
  batch 650 loss: 0.8309601294994354
  batch 700 loss: 0.86172638297081
  batch 750 loss: 0.8327180302143097
  batch 800 loss: 0.8321864980459214
  batch 850 loss: 0.8439008390903473
  batch 900 loss: 0.8704257845878601
LOSS train 0.87043 valid 0.89908, valid PER 27.98%
EPOCH 8:
  batch 50 loss: 0.7822512811422349
  batch 100 loss: 0.7668830323219299
  batch 150 loss: 0.786384391784668
  batch 200 loss: 0.7695222330093384
  batch 250 loss: 0.7815063452720642
  batch 300 loss: 0.7475795662403106
  batch 350 loss: 0.8118397152423859
  batch 400 loss: 0.7678090697526931
  batch 450 loss: 0.7930807483196258
  batch 500 loss: 0.8205505502223969
  batch 550 loss: 0.7655048191547393
  batch 600 loss: 0.8117326271533966
  batch 650 loss: 0.8302090299129486
  batch 700 loss: 0.7740376031398774
  batch 750 loss: 0.7684930300712586
  batch 800 loss: 0.7879991191625595
  batch 850 loss: 0.7677055644989014
  batch 900 loss: 0.7935159170627594
LOSS train 0.79352 valid 0.86820, valid PER 27.07%
EPOCH 9:
  batch 50 loss: 0.7209069758653641
  batch 100 loss: 0.7321761059761047
  batch 150 loss: 0.7487585771083832
  batch 200 loss: 0.7190585112571717
  batch 250 loss: 0.7446991568803787
  batch 300 loss: 0.7290924406051635
  batch 350 loss: 0.7692294663190842
  batch 400 loss: 0.7325834459066392
  batch 450 loss: 0.7388536942005157
  batch 500 loss: 0.7094909489154816
  batch 550 loss: 0.7318866693973541
  batch 600 loss: 0.7564900755882263
  batch 650 loss: 0.7230244088172912
  batch 700 loss: 0.7150387930870056
  batch 750 loss: 0.7246845448017121
  batch 800 loss: 0.7417025226354599
  batch 850 loss: 0.7731187534332276
  batch 900 loss: 0.7126541084051132
LOSS train 0.71265 valid 0.86262, valid PER 26.70%
EPOCH 10:
  batch 50 loss: 0.6563613629341125
  batch 100 loss: 0.6670526993274689
  batch 150 loss: 0.6830326515436173
  batch 200 loss: 0.707391494512558
  batch 250 loss: 0.7073111778497696
  batch 300 loss: 0.6817134219408035
  batch 350 loss: 0.6823715186119079
  batch 400 loss: 0.6674409228563308
  batch 450 loss: 0.654914162158966
  batch 500 loss: 0.7179144525527954
  batch 550 loss: 0.7175464451313018
  batch 600 loss: 0.7004863846302033
  batch 650 loss: 0.6789189094305038
  batch 700 loss: 0.6905782103538514
  batch 750 loss: 0.6820877909660339
  batch 800 loss: 0.7067284744977951
  batch 850 loss: 0.7050495630502701
  batch 900 loss: 0.7136922609806061
LOSS train 0.71369 valid 0.83940, valid PER 26.40%
EPOCH 11:
  batch 50 loss: 0.613590903878212
  batch 100 loss: 0.6027888077497482
  batch 150 loss: 0.6328842264413833
  batch 200 loss: 0.6642791843414306
  batch 250 loss: 0.6678944861888886
  batch 300 loss: 0.6113999855518341
  batch 350 loss: 0.6416457515954971
  batch 400 loss: 0.6600453495979309
  batch 450 loss: 0.6591006219387054
  batch 500 loss: 0.6339325910806656
  batch 550 loss: 0.646973208785057
  batch 600 loss: 0.6386038148403168
  batch 650 loss: 0.706728817820549
  batch 700 loss: 0.6287989991903306
  batch 750 loss: 0.6403628861904145
  batch 800 loss: 0.6927487897872925
  batch 850 loss: 0.6947093069553375
  batch 900 loss: 0.7031253737211227
LOSS train 0.70313 valid 0.83433, valid PER 25.78%
EPOCH 12:
  batch 50 loss: 0.6175155806541442
  batch 100 loss: 0.6098929411172866
  batch 150 loss: 0.5636779868602753
  batch 200 loss: 0.605433521270752
  batch 250 loss: 0.6276190257072449
  batch 300 loss: 0.6053003865480423
  batch 350 loss: 0.5990182256698608
  batch 400 loss: 0.6366640275716782
  batch 450 loss: 0.6274796241521835
  batch 500 loss: 0.6280364888906479
  batch 550 loss: 0.5796687763929367
  batch 600 loss: 0.6239217674732208
  batch 650 loss: 0.6387792330980301
  batch 700 loss: 0.6210034531354904
  batch 750 loss: 0.6160337823629379
  batch 800 loss: 0.6019622653722763
  batch 850 loss: 0.6726091307401657
  batch 900 loss: 0.6427190440893173
LOSS train 0.64272 valid 0.81987, valid PER 25.41%
EPOCH 13:
  batch 50 loss: 0.548539370894432
  batch 100 loss: 0.5591549396514892
  batch 150 loss: 0.5492963308095932
  batch 200 loss: 0.5868731987476349
  batch 250 loss: 0.5794458392262459
  batch 300 loss: 0.5888065850734711
  batch 350 loss: 0.5757161253690719
  batch 400 loss: 0.594697340130806
  batch 450 loss: 0.5956928652524948
  batch 500 loss: 0.5531267023086548
  batch 550 loss: 0.6017343360185623
  batch 600 loss: 0.5740428906679154
  batch 650 loss: 0.6022719913721084
  batch 700 loss: 0.5984349536895752
  batch 750 loss: 0.5659247672557831
  batch 800 loss: 0.5819670397043228
  batch 850 loss: 0.5962483245134353
  batch 900 loss: 0.6054310780763626
LOSS train 0.60543 valid 0.81563, valid PER 24.76%
EPOCH 14:
  batch 50 loss: 0.5245144903659821
  batch 100 loss: 0.5374618178606033
  batch 150 loss: 0.5308454722166062
  batch 200 loss: 0.5169802844524384
  batch 250 loss: 0.5357967072725296
  batch 300 loss: 0.5638828814029694
  batch 350 loss: 0.5388909024000168
  batch 400 loss: 0.52444897711277
  batch 450 loss: 0.5447144109010696
  batch 500 loss: 0.553064906001091
  batch 550 loss: 0.5571357250213623
  batch 600 loss: 0.536406729221344
  batch 650 loss: 0.5664172393083572
  batch 700 loss: 0.5827773118019104
  batch 750 loss: 0.5448138648271561
  batch 800 loss: 0.5325426578521728
  batch 850 loss: 0.5700606328248977
  batch 900 loss: 0.5614309030771255
LOSS train 0.56143 valid 0.83205, valid PER 24.42%
EPOCH 15:
  batch 50 loss: 0.4564371743798256
  batch 100 loss: 0.43655747711658477
  batch 150 loss: 0.43633795499801636
  batch 200 loss: 0.4430640712380409
  batch 250 loss: 0.4659502387046814
  batch 300 loss: 0.422880417406559
  batch 350 loss: 0.42921593904495237
  batch 400 loss: 0.4279131728410721
  batch 450 loss: 0.43456380188465116
  batch 500 loss: 0.40956595957279207
  batch 550 loss: 0.4344140291213989
  batch 600 loss: 0.44610210597515104
  batch 650 loss: 0.4523382049798965
  batch 700 loss: 0.4496919810771942
  batch 750 loss: 0.44705812811851503
  batch 800 loss: 0.4239128923416138
  batch 850 loss: 0.4102747517824173
  batch 900 loss: 0.43424278378486636
LOSS train 0.43424 valid 0.79095, valid PER 22.85%
EPOCH 16:
  batch 50 loss: 0.40062870144844054
  batch 100 loss: 0.37292809337377547
  batch 150 loss: 0.38456256687641144
  batch 200 loss: 0.38929397612810135
  batch 250 loss: 0.4086435616016388
  batch 300 loss: 0.403357048034668
  batch 350 loss: 0.4020970243215561
  batch 400 loss: 0.40562683254480364
  batch 450 loss: 0.4006900864839554
  batch 500 loss: 0.393340028822422
  batch 550 loss: 0.39146112114191056
  batch 600 loss: 0.39683988034725187
  batch 650 loss: 0.40413419157266617
  batch 700 loss: 0.3964790707826614
  batch 750 loss: 0.39828894287347794
  batch 800 loss: 0.41204609513282775
  batch 850 loss: 0.4011371165513992
  batch 900 loss: 0.404650439620018
LOSS train 0.40465 valid 0.80575, valid PER 23.01%
EPOCH 17:
  batch 50 loss: 0.34177064925432205
  batch 100 loss: 0.3372474393248558
  batch 150 loss: 0.33230179637670515
  batch 200 loss: 0.33376781225204466
  batch 250 loss: 0.3307651895284653
  batch 300 loss: 0.3409028092026711
  batch 350 loss: 0.3151243704557419
  batch 400 loss: 0.3515929946303368
  batch 450 loss: 0.3301139450073242
  batch 500 loss: 0.32444497138261796
  batch 550 loss: 0.3189955538511276
  batch 600 loss: 0.34444366067647936
  batch 650 loss: 0.33120098322629926
  batch 700 loss: 0.33466692388057706
  batch 750 loss: 0.33265100240707396
  batch 800 loss: 0.31287322133779527
  batch 850 loss: 0.35429253458976745
  batch 900 loss: 0.31696192592382433
LOSS train 0.31696 valid 0.81834, valid PER 22.84%
EPOCH 18:
  batch 50 loss: 0.29753381609916685
  batch 100 loss: 0.29406247586011885
  batch 150 loss: 0.31010074019432066
  batch 200 loss: 0.29678224414587023
  batch 250 loss: 0.3044701036810875
  batch 300 loss: 0.2810629989206791
  batch 350 loss: 0.29024600714445115
  batch 400 loss: 0.278200559169054
  batch 450 loss: 0.29961499601602554
  batch 500 loss: 0.2978014004230499
  batch 550 loss: 0.302262619137764
  batch 600 loss: 0.26862712383270265
  batch 650 loss: 0.2830455157160759
  batch 700 loss: 0.3029902231693268
  batch 750 loss: 0.27879397064447403
  batch 800 loss: 0.2786061164736748
  batch 850 loss: 0.2818268284201622
  batch 900 loss: 0.3075917914509773
LOSS train 0.30759 valid 0.82625, valid PER 22.79%
EPOCH 19:
  batch 50 loss: 0.26027369290590285
  batch 100 loss: 0.2616841846704483
  batch 150 loss: 0.2553086411952972
  batch 200 loss: 0.2650661709904671
  batch 250 loss: 0.27229176700115204
  batch 300 loss: 0.2800356847047806
  batch 350 loss: 0.2593074494600296
  batch 400 loss: 0.2586890861392021
  batch 450 loss: 0.28156566351652146
  batch 500 loss: 0.2743710400164127
  batch 550 loss: 0.26320561379194257
  batch 600 loss: 0.2604227769374847
  batch 650 loss: 0.2948279079794884
  batch 700 loss: 0.26950940787792205
  batch 750 loss: 0.2526230013370514
  batch 800 loss: 0.2827521052956581
  batch 850 loss: 0.26674040645360947
  batch 900 loss: 0.2804138380289078
LOSS train 0.28041 valid 0.83713, valid PER 22.66%
EPOCH 20:
  batch 50 loss: 0.26677900850772857
  batch 100 loss: 0.25374703377485275
  batch 150 loss: 0.24460585325956344
  batch 200 loss: 0.25407141402363775
  batch 250 loss: 0.2615730446577072
  batch 300 loss: 0.26319041013717653
  batch 350 loss: 0.24325984895229338
  batch 400 loss: 0.25790991336107255
  batch 450 loss: 0.2634451100230217
  batch 500 loss: 0.2381460766494274
  batch 550 loss: 0.27691857993602753
  batch 600 loss: 0.24702306121587753
  batch 650 loss: 0.26271436363458633
  batch 700 loss: 0.24340272933244705
  batch 750 loss: 0.23636507943272592
  batch 800 loss: 0.2760463958978653
  batch 850 loss: 0.2645998656749725
  batch 900 loss: 0.259885419011116
LOSS train 0.25989 valid 0.84015, valid PER 22.66%
Training finished in 10.0 minutes.
Model saved to checkpoints/20230118_101621/model_15
Loading model from checkpoints/20230118_101621/model_15
SUB: 14.79%, DEL: 7.99%, INS: 2.59%, COR: 77.21%, PER: 25.37%
