Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0, schedule='true')
cuda:0
Total number of model parameters is 2240552
EPOCH 1:
  batch 50 loss: 5.125112299919128
  batch 100 loss: 3.291358003616333
  batch 150 loss: 3.023873834609985
  batch 200 loss: 2.7683107233047486
  batch 250 loss: 2.6009688234329222
  batch 300 loss: 2.4311524486541747
  batch 350 loss: 2.296772780418396
  batch 400 loss: 2.260803792476654
  batch 450 loss: 2.1750128650665284
  batch 500 loss: 2.0815017867088317
  batch 550 loss: 2.0289335680007934
  batch 600 loss: 1.9635520792007446
  batch 650 loss: 1.894788808822632
  batch 700 loss: 1.8817843198776245
  batch 750 loss: 1.8211378407478334
  batch 800 loss: 1.7982421827316284
  batch 850 loss: 1.754731216430664
  batch 900 loss: 1.745226628780365
LOSS train 1.74523 valid 1.69347, valid PER 57.21%
EPOCH 2:
  batch 50 loss: 1.6932701230049134
  batch 100 loss: 1.634786648750305
  batch 150 loss: 1.6134295868873596
  batch 200 loss: 1.6209366202354432
  batch 250 loss: 1.6002221560478211
  batch 300 loss: 1.5677248120307923
  batch 350 loss: 1.479736671447754
  batch 400 loss: 1.4991052722930909
  batch 450 loss: 1.4584065985679626
  batch 500 loss: 1.4901993441581727
  batch 550 loss: 1.4960528206825257
  batch 600 loss: 1.4381984782218933
  batch 650 loss: 1.4480689072608948
  batch 700 loss: 1.4231423330307007
  batch 750 loss: 1.398894350528717
  batch 800 loss: 1.352399649620056
  batch 850 loss: 1.3512362909317017
  batch 900 loss: 1.393014781475067
LOSS train 1.39301 valid 1.35521, valid PER 41.20%
EPOCH 3:
  batch 50 loss: 1.3275875902175904
  batch 100 loss: 1.3050885343551635
  batch 150 loss: 1.289438819885254
  batch 200 loss: 1.2610376906394958
  batch 250 loss: 1.260242770910263
  batch 300 loss: 1.2524410271644593
  batch 350 loss: 1.3043627691268922
  batch 400 loss: 1.2708747708797454
  batch 450 loss: 1.2296495997905732
  batch 500 loss: 1.2149390912055968
  batch 550 loss: 1.2373733603954316
  batch 600 loss: 1.198634227514267
  batch 650 loss: 1.1839633333683013
  batch 700 loss: 1.2102850759029389
  batch 750 loss: 1.2478261768817902
  batch 800 loss: 1.1769925558567047
  batch 850 loss: 1.2020555210113526
  batch 900 loss: 1.1275326812267303
LOSS train 1.12753 valid 1.19954, valid PER 36.52%
EPOCH 4:
  batch 50 loss: 1.1281061911582946
  batch 100 loss: 1.1348959708213806
  batch 150 loss: 1.0952674114704133
  batch 200 loss: 1.1290217125415802
  batch 250 loss: 1.1305891728401185
  batch 300 loss: 1.1207393312454224
  batch 350 loss: 1.063873519897461
  batch 400 loss: 1.1031186842918397
  batch 450 loss: 1.0957508981227875
  batch 500 loss: 1.0769382810592651
  batch 550 loss: 1.0952135670185088
  batch 600 loss: 1.1201110398769378
  batch 650 loss: 1.082146439552307
  batch 700 loss: 1.0596924722194672
  batch 750 loss: 1.0338903331756593
  batch 800 loss: 1.018070605993271
  batch 850 loss: 1.068410837650299
  batch 900 loss: 1.0800554537773133
LOSS train 1.08006 valid 1.10325, valid PER 34.28%
EPOCH 5:
  batch 50 loss: 0.9910755944252014
  batch 100 loss: 0.9923357737064361
  batch 150 loss: 1.050434627532959
  batch 200 loss: 0.9678083455562592
  batch 250 loss: 0.9783105671405792
  batch 300 loss: 0.9813478457927703
  batch 350 loss: 0.9781449151039123
  batch 400 loss: 0.9789646685123443
  batch 450 loss: 0.9752846372127533
  batch 500 loss: 0.993377616405487
  batch 550 loss: 0.9476193594932556
  batch 600 loss: 1.0307930719852447
  batch 650 loss: 0.9709772431850433
  batch 700 loss: 1.016392903327942
  batch 750 loss: 0.9513630604743958
  batch 800 loss: 0.9674657344818115
  batch 850 loss: 0.9532692778110504
  batch 900 loss: 0.9845744109153748
LOSS train 0.98457 valid 1.02131, valid PER 31.33%
EPOCH 6:
  batch 50 loss: 0.9454516923427582
  batch 100 loss: 0.891243863105774
  batch 150 loss: 0.880654194355011
  batch 200 loss: 0.9036341679096221
  batch 250 loss: 0.9275605964660645
  batch 300 loss: 0.9220703017711639
  batch 350 loss: 0.9026716315746307
  batch 400 loss: 0.8961751914024353
  batch 450 loss: 0.919577625989914
  batch 500 loss: 0.9105678248405457
  batch 550 loss: 0.925898209810257
  batch 600 loss: 0.8761892688274383
  batch 650 loss: 0.8934029269218445
  batch 700 loss: 0.8940177249908448
  batch 750 loss: 0.8815133202075959
  batch 800 loss: 0.8866942358016968
  batch 850 loss: 0.8765617442131043
  batch 900 loss: 0.896092232465744
LOSS train 0.89609 valid 1.01477, valid PER 30.74%
EPOCH 7:
  batch 50 loss: 0.8443727278709412
  batch 100 loss: 0.8431589567661285
  batch 150 loss: 0.8319739127159118
  batch 200 loss: 0.8151562595367432
  batch 250 loss: 0.8131695306301117
  batch 300 loss: 0.8175512909889221
  batch 350 loss: 0.8365587449073791
  batch 400 loss: 0.8331719863414765
  batch 450 loss: 0.8390340077877044
  batch 500 loss: 0.8272913134098053
  batch 550 loss: 0.836108865737915
  batch 600 loss: 0.8342020118236542
  batch 650 loss: 0.8189773070812225
  batch 700 loss: 0.8514005482196808
  batch 750 loss: 0.8226218819618225
  batch 800 loss: 0.8204701828956604
  batch 850 loss: 0.8350345909595489
  batch 900 loss: 0.855009982585907
LOSS train 0.85501 valid 0.96289, valid PER 29.24%
EPOCH 8:
  batch 50 loss: 0.746114581823349
  batch 100 loss: 0.7558887249231339
  batch 150 loss: 0.7673406982421875
  batch 200 loss: 0.7562217879295349
  batch 250 loss: 0.7657135397195816
  batch 300 loss: 0.7270549553632736
  batch 350 loss: 0.805769978761673
  batch 400 loss: 0.7589046895503998
  batch 450 loss: 0.7756015646457672
  batch 500 loss: 0.7986663830280304
  batch 550 loss: 0.7440033555030823
  batch 600 loss: 0.7877361857891083
  batch 650 loss: 0.8124388194084168
  batch 700 loss: 0.7477455800771713
  batch 750 loss: 0.7677291744947433
  batch 800 loss: 0.7899623721837997
  batch 850 loss: 0.7706991803646087
  batch 900 loss: 0.7730398279428482
LOSS train 0.77304 valid 0.95688, valid PER 28.24%
EPOCH 9:
  batch 50 loss: 0.6699969911575318
  batch 100 loss: 0.7077086007595063
  batch 150 loss: 0.7021783977746964
  batch 200 loss: 0.6907539319992065
  batch 250 loss: 0.7136217921972274
  batch 300 loss: 0.711779328584671
  batch 350 loss: 0.7410584062337875
  batch 400 loss: 0.7213992249965667
  batch 450 loss: 0.704745448231697
  batch 500 loss: 0.7057639408111572
  batch 550 loss: 0.7312693578004837
  batch 600 loss: 0.7284176409244537
  batch 650 loss: 0.7131952130794526
  batch 700 loss: 0.6836939513683319
  batch 750 loss: 0.7073884803056717
  batch 800 loss: 0.7292723095417023
  batch 850 loss: 0.7370660424232482
  batch 900 loss: 0.6959179407358169
LOSS train 0.69592 valid 0.93336, valid PER 27.96%
EPOCH 10:
  batch 50 loss: 0.6058345651626587
  batch 100 loss: 0.6240993058681488
  batch 150 loss: 0.6475259238481521
  batch 200 loss: 0.6709528702497483
  batch 250 loss: 0.6713097286224365
  batch 300 loss: 0.6452139353752137
  batch 350 loss: 0.6488602215051651
  batch 400 loss: 0.6239161550998688
  batch 450 loss: 0.6314607036113739
  batch 500 loss: 0.6621516585350037
  batch 550 loss: 0.6908786243200302
  batch 600 loss: 0.6627036994695663
  batch 650 loss: 0.6518515425920487
  batch 700 loss: 0.6894054961204529
  batch 750 loss: 0.6525176501274109
  batch 800 loss: 0.6729587388038635
  batch 850 loss: 0.6731635713577271
  batch 900 loss: 0.6825147551298142
LOSS train 0.68251 valid 0.93635, valid PER 27.49%
EPOCH 11:
  batch 50 loss: 0.5199157118797302
  batch 100 loss: 0.49621683478355405
  batch 150 loss: 0.50989941239357
  batch 200 loss: 0.5539851063489913
  batch 250 loss: 0.5353610754013062
  batch 300 loss: 0.500975393652916
  batch 350 loss: 0.5235170906782151
  batch 400 loss: 0.5414435303211212
  batch 450 loss: 0.5359810394048691
  batch 500 loss: 0.508425366282463
  batch 550 loss: 0.5175401985645294
  batch 600 loss: 0.5105757030844689
  batch 650 loss: 0.5532523787021637
  batch 700 loss: 0.5018657231330872
  batch 750 loss: 0.50937206864357
  batch 800 loss: 0.5418506777286529
  batch 850 loss: 0.5449727994203567
  batch 900 loss: 0.5414005619287491
LOSS train 0.54140 valid 0.89159, valid PER 25.66%
EPOCH 12:
  batch 50 loss: 0.46582784831523893
  batch 100 loss: 0.464001105427742
  batch 150 loss: 0.4235171982645988
  batch 200 loss: 0.45519491314888
  batch 250 loss: 0.4630256849527359
  batch 300 loss: 0.45570852249860766
  batch 350 loss: 0.4508282828330994
  batch 400 loss: 0.4770360952615738
  batch 450 loss: 0.48210169702768324
  batch 500 loss: 0.4765905731916428
  batch 550 loss: 0.45000804603099825
  batch 600 loss: 0.48002477288246154
  batch 650 loss: 0.48662278085947036
  batch 700 loss: 0.5007487177848816
  batch 750 loss: 0.4615087872743607
  batch 800 loss: 0.4876601612567902
  batch 850 loss: 0.5200523257255554
  batch 900 loss: 0.5127487981319427
LOSS train 0.51275 valid 0.90975, valid PER 25.86%
EPOCH 13:
  batch 50 loss: 0.4057842963933945
  batch 100 loss: 0.39673317164182664
  batch 150 loss: 0.3839055401086807
  batch 200 loss: 0.39789145976305007
  batch 250 loss: 0.3876130223274231
  batch 300 loss: 0.3858347192406654
  batch 350 loss: 0.38529385685920714
  batch 400 loss: 0.4006330364942551
  batch 450 loss: 0.3810310512781143
  batch 500 loss: 0.3831628918647766
  batch 550 loss: 0.398558406829834
  batch 600 loss: 0.3845517006516457
  batch 650 loss: 0.4031750375032425
  batch 700 loss: 0.4238850921392441
  batch 750 loss: 0.3705049133300781
  batch 800 loss: 0.3819781318306923
  batch 850 loss: 0.4056613659858704
  batch 900 loss: 0.40979472011327744
LOSS train 0.40979 valid 0.92002, valid PER 25.68%
EPOCH 14:
  batch 50 loss: 0.3454034373164177
  batch 100 loss: 0.3387340947985649
  batch 150 loss: 0.3468345156311989
  batch 200 loss: 0.33085457146167757
  batch 250 loss: 0.34246373921632767
  batch 300 loss: 0.36884145736694335
  batch 350 loss: 0.3275407502055168
  batch 400 loss: 0.3440317726135254
  batch 450 loss: 0.3395072841644287
  batch 500 loss: 0.3425592178106308
  batch 550 loss: 0.36000255078077315
  batch 600 loss: 0.32687889605760573
  batch 650 loss: 0.35352967262268065
  batch 700 loss: 0.36997982412576674
  batch 750 loss: 0.3255878487229347
  batch 800 loss: 0.33202501505613324
  batch 850 loss: 0.3589039957523346
  batch 900 loss: 0.34923869878053665
LOSS train 0.34924 valid 0.92202, valid PER 25.52%
EPOCH 15:
  batch 50 loss: 0.31294277906417844
  batch 100 loss: 0.3153586179018021
  batch 150 loss: 0.3236533436179161
  batch 200 loss: 0.32876230984926225
  batch 250 loss: 0.34189577460289
  batch 300 loss: 0.30606263428926467
  batch 350 loss: 0.3138510191440582
  batch 400 loss: 0.32712382942438123
  batch 450 loss: 0.3082038488984108
  batch 500 loss: 0.30370320707559584
  batch 550 loss: 0.312265602350235
  batch 600 loss: 0.32390144139528276
  batch 650 loss: 0.3397531008720398
  batch 700 loss: 0.3349574628472328
  batch 750 loss: 0.324620401263237
  batch 800 loss: 0.30433929443359375
  batch 850 loss: 0.30261961579322816
  batch 900 loss: 0.3205595189332962
LOSS train 0.32056 valid 0.93231, valid PER 25.48%
EPOCH 16:
  batch 50 loss: 0.3108116489648819
  batch 100 loss: 0.3024831128120422
  batch 150 loss: 0.3161842504143715
  batch 200 loss: 0.29455568075180055
  batch 250 loss: 0.3165857967734337
  batch 300 loss: 0.30303395181894305
  batch 350 loss: 0.3022712230682373
  batch 400 loss: 0.31556542426347733
  batch 450 loss: 0.30856801450252536
  batch 500 loss: 0.2907857260107994
  batch 550 loss: 0.2918200704455376
  batch 600 loss: 0.29315794199705125
  batch 650 loss: 0.31093415528535845
  batch 700 loss: 0.2914128604531288
  batch 750 loss: 0.3067044159770012
  batch 800 loss: 0.30675490260124205
  batch 850 loss: 0.3051203969120979
  batch 900 loss: 0.3162216141819954
LOSS train 0.31622 valid 0.93683, valid PER 25.45%
EPOCH 17:
  batch 50 loss: 0.30264010667800906
  batch 100 loss: 0.30442709028720855
  batch 150 loss: 0.29600692123174666
  batch 200 loss: 0.293144913315773
  batch 250 loss: 0.32226689845323564
  batch 300 loss: 0.3009387385845184
  batch 350 loss: 0.28914510041475294
  batch 400 loss: 0.30856070458889007
  batch 450 loss: 0.3029556015133858
  batch 500 loss: 0.28285161793231967
  batch 550 loss: 0.29150928527116776
  batch 600 loss: 0.3047873359918594
  batch 650 loss: 0.2915071332454681
  batch 700 loss: 0.2954743683338165
  batch 750 loss: 0.28955270707607267
  batch 800 loss: 0.28079227328300477
  batch 850 loss: 0.3020353478193283
  batch 900 loss: 0.28640061408281325
LOSS train 0.28640 valid 0.93912, valid PER 25.33%
EPOCH 18:
  batch 50 loss: 0.2841387444734573
  batch 100 loss: 0.3008099642395973
  batch 150 loss: 0.3077798077464104
  batch 200 loss: 0.2958820790052414
  batch 250 loss: 0.30881736367940904
  batch 300 loss: 0.2841491983830929
  batch 350 loss: 0.2860376638174057
  batch 400 loss: 0.2870437628030777
  batch 450 loss: 0.3056550115346909
  batch 500 loss: 0.2928649738430977
  batch 550 loss: 0.31040114983916284
  batch 600 loss: 0.29265869975090025
  batch 650 loss: 0.27088706254959105
  batch 700 loss: 0.3000198593735695
  batch 750 loss: 0.28621110767126084
  batch 800 loss: 0.29277781426906585
  batch 850 loss: 0.28881563574075697
  batch 900 loss: 0.2957664209604263
LOSS train 0.29577 valid 0.93994, valid PER 25.45%
EPOCH 19:
  batch 50 loss: 0.30017040520906446
  batch 100 loss: 0.2809913378953934
  batch 150 loss: 0.28327078104019165
  batch 200 loss: 0.2954718914628029
  batch 250 loss: 0.2899745494127274
  batch 300 loss: 0.29668308347463607
  batch 350 loss: 0.2748326677083969
  batch 400 loss: 0.2877072405815124
  batch 450 loss: 0.3041792172193527
  batch 500 loss: 0.2862601527571678
  batch 550 loss: 0.2863448071479797
  batch 600 loss: 0.2824710923433304
  batch 650 loss: 0.31543551027774813
  batch 700 loss: 0.2934333771467209
  batch 750 loss: 0.28109197109937667
  batch 800 loss: 0.3068754905462265
  batch 850 loss: 0.30361444503068924
  batch 900 loss: 0.29175684720277784
LOSS train 0.29176 valid 0.94074, valid PER 25.41%
EPOCH 20:
  batch 50 loss: 0.30265799790620806
  batch 100 loss: 0.2887313485145569
  batch 150 loss: 0.2840561145544052
  batch 200 loss: 0.2857913503050804
  batch 250 loss: 0.29010147482156756
  batch 300 loss: 0.3001018667221069
  batch 350 loss: 0.2782217484712601
  batch 400 loss: 0.29794167637825014
  batch 450 loss: 0.293196918964386
  batch 500 loss: 0.27366158276796343
  batch 550 loss: 0.31609115362167356
  batch 600 loss: 0.2871989133954048
  batch 650 loss: 0.28999733179807663
  batch 700 loss: 0.29516123086214063
  batch 750 loss: 0.2749436390399933
  batch 800 loss: 0.30968875050544736
  batch 850 loss: 0.29077619016170503
  batch 900 loss: 0.2872456204891205
LOSS train 0.28725 valid 0.94121, valid PER 25.40%
Training finished in 15.0 minutes.
Model saved to checkpoints/20230118_102248/model_11
Loading model from checkpoints/20230118_102248/model_11
SUB: 15.39%, DEL: 9.65%, INS: 3.03%, COR: 74.96%, PER: 28.07%
