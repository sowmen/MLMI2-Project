Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0, schedule='true')
cuda:0
Total number of model parameters is 9044520
EPOCH 1:
  batch 50 loss: 5.173125100135803
  batch 100 loss: 3.4759171628952026
  batch 150 loss: 3.3511480617523195
  batch 200 loss: 3.2554446268081665
  batch 250 loss: 3.223055682182312
  batch 300 loss: 3.1389404916763306
  batch 350 loss: 2.9529342365264895
  batch 400 loss: 2.8389792919158934
  batch 450 loss: 2.7509058713912964
  batch 500 loss: 2.6154845714569093
  batch 550 loss: 2.502635111808777
  batch 600 loss: 2.440497794151306
  batch 650 loss: 2.3232742500305177
  batch 700 loss: 2.3159812593460085
  batch 750 loss: 2.2363616013526917
  batch 800 loss: 2.2042848205566408
  batch 850 loss: 2.1373385262489317
  batch 900 loss: 2.060545127391815
LOSS train 2.06055 valid 1.99428, valid PER 58.01%
EPOCH 2:
  batch 50 loss: 1.9365023612976073
  batch 100 loss: 1.882305727005005
  batch 150 loss: 1.8199044728279115
  batch 200 loss: 1.8183481693267822
  batch 250 loss: 1.818325538635254
  batch 300 loss: 1.741668918132782
  batch 350 loss: 1.65309246301651
  batch 400 loss: 1.6459021115303039
  batch 450 loss: 1.5987587332725526
  batch 500 loss: 1.603610544204712
  batch 550 loss: 1.5931376552581786
  batch 600 loss: 1.5404277348518371
  batch 650 loss: 1.5576783800125122
  batch 700 loss: 1.4877935647964478
  batch 750 loss: 1.476592218875885
  batch 800 loss: 1.413954701423645
  batch 850 loss: 1.404537844657898
  batch 900 loss: 1.422562894821167
LOSS train 1.42256 valid 1.36882, valid PER 40.44%
EPOCH 3:
  batch 50 loss: 1.3779377245903015
  batch 100 loss: 1.3390438950061798
  batch 150 loss: 1.3379371690750121
  batch 200 loss: 1.3120074367523193
  batch 250 loss: 1.3028426885604858
  batch 300 loss: 1.3035948491096496
  batch 350 loss: 1.3339560008049012
  batch 400 loss: 1.292938278913498
  batch 450 loss: 1.2813066530227661
  batch 500 loss: 1.2424215734004975
  batch 550 loss: 1.261649533510208
  batch 600 loss: 1.2148916518688202
  batch 650 loss: 1.1910869801044464
  batch 700 loss: 1.2023321866989136
  batch 750 loss: 1.2576133251190185
  batch 800 loss: 1.1755681562423705
  batch 850 loss: 1.2123242342472076
  batch 900 loss: 1.1248756217956544
LOSS train 1.12488 valid 1.19453, valid PER 36.58%
EPOCH 4:
  batch 50 loss: 1.1265262389183044
  batch 100 loss: 1.1322434282302856
  batch 150 loss: 1.1000046491622926
  batch 200 loss: 1.1305996143817902
  batch 250 loss: 1.119584676027298
  batch 300 loss: 1.1474722802639008
  batch 350 loss: 1.06453169465065
  batch 400 loss: 1.1170900762081146
  batch 450 loss: 1.0902234077453614
  batch 500 loss: 1.0759862399101257
  batch 550 loss: 1.0771416091918946
  batch 600 loss: 1.1028647375106813
  batch 650 loss: 1.0774797296524048
  batch 700 loss: 1.0302348983287812
  batch 750 loss: 1.0232505345344542
  batch 800 loss: 1.013542321920395
  batch 850 loss: 1.0203493118286133
  batch 900 loss: 1.077949446439743
LOSS train 1.07795 valid 1.03155, valid PER 31.69%
EPOCH 5:
  batch 50 loss: 0.984494823217392
  batch 100 loss: 0.9666233563423157
  batch 150 loss: 1.0230512154102325
  batch 200 loss: 0.9542874217033386
  batch 250 loss: 0.9604972743988037
  batch 300 loss: 0.9798578953742981
  batch 350 loss: 0.954882218837738
  batch 400 loss: 0.981559419631958
  batch 450 loss: 0.960218151807785
  batch 500 loss: 0.9941638898849487
  batch 550 loss: 0.9002866196632385
  batch 600 loss: 0.9853762257099151
  batch 650 loss: 0.9415488016605377
  batch 700 loss: 0.9932954978942871
  batch 750 loss: 0.9277911627292633
  batch 800 loss: 0.9441220319271088
  batch 850 loss: 0.9448647594451904
  batch 900 loss: 0.9377785563468933
LOSS train 0.93778 valid 0.97182, valid PER 30.24%
EPOCH 6:
  batch 50 loss: 0.9312756395339966
  batch 100 loss: 0.8723328769207
  batch 150 loss: 0.8561879110336303
  batch 200 loss: 0.8776262772083282
  batch 250 loss: 0.9035621011257171
  batch 300 loss: 0.8691576480865478
  batch 350 loss: 0.8755599963665008
  batch 400 loss: 0.8606121754646301
  batch 450 loss: 0.8785769128799439
  batch 500 loss: 0.8678535270690918
  batch 550 loss: 0.8924522495269775
  batch 600 loss: 0.8606775605678558
  batch 650 loss: 0.8550851666927337
  batch 700 loss: 0.8688169085979461
  batch 750 loss: 0.8444921922683716
  batch 800 loss: 0.8696636939048767
  batch 850 loss: 0.839061428308487
  batch 900 loss: 0.8501852238178254
LOSS train 0.85019 valid 0.91123, valid PER 27.34%
EPOCH 7:
  batch 50 loss: 0.8180867338180542
  batch 100 loss: 0.8133655703067779
  batch 150 loss: 0.7760992407798767
  batch 200 loss: 0.7959456938505173
  batch 250 loss: 0.7833824610710144
  batch 300 loss: 0.7782815456390381
  batch 350 loss: 0.7902357423305512
  batch 400 loss: 0.7963917553424835
  batch 450 loss: 0.7806604790687561
  batch 500 loss: 0.7939740872383118
  batch 550 loss: 0.7899025011062623
  batch 600 loss: 0.8068755197525025
  batch 650 loss: 0.7810873019695282
  batch 700 loss: 0.8122888278961181
  batch 750 loss: 0.7843213093280792
  batch 800 loss: 0.7935154354572296
  batch 850 loss: 0.7831299114227295
  batch 900 loss: 0.8090005445480347
LOSS train 0.80900 valid 0.89923, valid PER 28.07%
EPOCH 8:
  batch 50 loss: 0.7196851229667663
  batch 100 loss: 0.6989503538608551
  batch 150 loss: 0.7273587965965271
  batch 200 loss: 0.7167181146144866
  batch 250 loss: 0.719597864151001
  batch 300 loss: 0.6797027510404586
  batch 350 loss: 0.7567374503612518
  batch 400 loss: 0.7035655242204666
  batch 450 loss: 0.7433993756771088
  batch 500 loss: 0.7515974557399749
  batch 550 loss: 0.6811060607433319
  batch 600 loss: 0.7451095342636108
  batch 650 loss: 0.7511891341209411
  batch 700 loss: 0.71834812104702
  batch 750 loss: 0.7115828347206116
  batch 800 loss: 0.7304975670576096
  batch 850 loss: 0.719173281788826
  batch 900 loss: 0.7433015185594559
LOSS train 0.74330 valid 0.86233, valid PER 25.96%
EPOCH 9:
  batch 50 loss: 0.6173597264289856
  batch 100 loss: 0.6664122092723846
  batch 150 loss: 0.6741726005077362
  batch 200 loss: 0.63775326192379
  batch 250 loss: 0.6649402838945389
  batch 300 loss: 0.6707647049427032
  batch 350 loss: 0.6859202128648758
  batch 400 loss: 0.6413761329650879
  batch 450 loss: 0.6569093286991119
  batch 500 loss: 0.6645691061019897
  batch 550 loss: 0.6719814985990524
  batch 600 loss: 0.692929277420044
  batch 650 loss: 0.6669213247299194
  batch 700 loss: 0.6449087643623352
  batch 750 loss: 0.6366380459070206
  batch 800 loss: 0.6730353987216949
  batch 850 loss: 0.6967468565702438
  batch 900 loss: 0.6468639034032821
LOSS train 0.64686 valid 0.88216, valid PER 25.35%
EPOCH 10:
  batch 50 loss: 0.5282269585132598
  batch 100 loss: 0.5168092197179794
  batch 150 loss: 0.5207586014270782
  batch 200 loss: 0.5498074465990066
  batch 250 loss: 0.538876965045929
  batch 300 loss: 0.4865277993679047
  batch 350 loss: 0.5114114099740982
  batch 400 loss: 0.4747831040620804
  batch 450 loss: 0.48541123151779175
  batch 500 loss: 0.5384446561336518
  batch 550 loss: 0.536794879436493
  batch 600 loss: 0.5113626143336296
  batch 650 loss: 0.510023592710495
  batch 700 loss: 0.5214750176668167
  batch 750 loss: 0.5162257355451584
  batch 800 loss: 0.5285299688577652
  batch 850 loss: 0.5139450651407241
  batch 900 loss: 0.5292677974700928
LOSS train 0.52927 valid 0.78003, valid PER 23.70%
EPOCH 11:
  batch 50 loss: 0.443881556391716
  batch 100 loss: 0.4224680981040001
  batch 150 loss: 0.4337656292319298
  batch 200 loss: 0.47788972318172457
  batch 250 loss: 0.4715051534771919
  batch 300 loss: 0.43759324789047244
  batch 350 loss: 0.4598834139108658
  batch 400 loss: 0.4814830583333969
  batch 450 loss: 0.4744252532720566
  batch 500 loss: 0.43426946878433226
  batch 550 loss: 0.4695196616649628
  batch 600 loss: 0.44438846319913866
  batch 650 loss: 0.5047673797607422
  batch 700 loss: 0.45897004663944246
  batch 750 loss: 0.4586337673664093
  batch 800 loss: 0.485821328163147
  batch 850 loss: 0.4856796228885651
  batch 900 loss: 0.4841286301612854
LOSS train 0.48413 valid 0.78891, valid PER 23.18%
EPOCH 12:
  batch 50 loss: 0.3851598834991455
  batch 100 loss: 0.3758331736922264
  batch 150 loss: 0.33661464184522627
  batch 200 loss: 0.36687386184930804
  batch 250 loss: 0.3758141562342644
  batch 300 loss: 0.3730772268772125
  batch 350 loss: 0.3507222455739975
  batch 400 loss: 0.3703559082746506
  batch 450 loss: 0.38621753990650176
  batch 500 loss: 0.3687036177515984
  batch 550 loss: 0.347538688480854
  batch 600 loss: 0.38412788152694705
  batch 650 loss: 0.3847210377454758
  batch 700 loss: 0.38116659492254257
  batch 750 loss: 0.3622110345959663
  batch 800 loss: 0.37533328637480734
  batch 850 loss: 0.40950723111629483
  batch 900 loss: 0.3853312695026398
LOSS train 0.38533 valid 0.77824, valid PER 22.16%
EPOCH 13:
  batch 50 loss: 0.3324221992492676
  batch 100 loss: 0.32686120480298997
  batch 150 loss: 0.32328289359807966
  batch 200 loss: 0.3475660580396652
  batch 250 loss: 0.3327959761023521
  batch 300 loss: 0.33583626598119737
  batch 350 loss: 0.3206506744027138
  batch 400 loss: 0.338759078681469
  batch 450 loss: 0.3406365871429443
  batch 500 loss: 0.3279357892274857
  batch 550 loss: 0.350690298974514
  batch 600 loss: 0.3355184108018875
  batch 650 loss: 0.3594579255580902
  batch 700 loss: 0.3463949343562126
  batch 750 loss: 0.3173675560951233
  batch 800 loss: 0.3340009933710098
  batch 850 loss: 0.35252502501010896
  batch 900 loss: 0.36410465478897097
LOSS train 0.36410 valid 0.78958, valid PER 22.19%
EPOCH 14:
  batch 50 loss: 0.29644391030073164
  batch 100 loss: 0.2884924140572548
  batch 150 loss: 0.282020657658577
  batch 200 loss: 0.27396457761526105
  batch 250 loss: 0.2841832187771797
  batch 300 loss: 0.3080446165800095
  batch 350 loss: 0.2782582426071167
  batch 400 loss: 0.28005366280674937
  batch 450 loss: 0.28631485551595687
  batch 500 loss: 0.2822762602567673
  batch 550 loss: 0.2992728650569916
  batch 600 loss: 0.27040097951889036
  batch 650 loss: 0.2846358880400658
  batch 700 loss: 0.30878972381353376
  batch 750 loss: 0.2756906265020371
  batch 800 loss: 0.27565990418195724
  batch 850 loss: 0.3057898008823395
  batch 900 loss: 0.2926979538798332
LOSS train 0.29270 valid 0.80163, valid PER 22.07%
EPOCH 15:
  batch 50 loss: 0.2611276748776436
  batch 100 loss: 0.25438727259635924
  batch 150 loss: 0.25544641524553297
  batch 200 loss: 0.2602529765665531
  batch 250 loss: 0.27566510111093523
  batch 300 loss: 0.25482238322496414
  batch 350 loss: 0.2491869419813156
  batch 400 loss: 0.26503422409296035
  batch 450 loss: 0.25204385370016097
  batch 500 loss: 0.24755796253681184
  batch 550 loss: 0.2615482383966446
  batch 600 loss: 0.2613158118724823
  batch 650 loss: 0.28477561235427856
  batch 700 loss: 0.2744959369301796
  batch 750 loss: 0.2598157569766045
  batch 800 loss: 0.2592271989583969
  batch 850 loss: 0.23903986036777497
  batch 900 loss: 0.25484165728092195
LOSS train 0.25484 valid 0.80774, valid PER 21.87%
EPOCH 16:
  batch 50 loss: 0.2453409905731678
  batch 100 loss: 0.23319330036640168
  batch 150 loss: 0.2434037896990776
  batch 200 loss: 0.23802347779273986
  batch 250 loss: 0.24654366597533225
  batch 300 loss: 0.24638645678758622
  batch 350 loss: 0.25098679631948473
  batch 400 loss: 0.2514377382397652
  batch 450 loss: 0.2598445054888725
  batch 500 loss: 0.23127997368574144
  batch 550 loss: 0.23390401646494866
  batch 600 loss: 0.23838775277137755
  batch 650 loss: 0.24475120574235917
  batch 700 loss: 0.24761879935860634
  batch 750 loss: 0.24478895246982574
  batch 800 loss: 0.25258012384176254
  batch 850 loss: 0.23855715721845627
  batch 900 loss: 0.2502569302916527
LOSS train 0.25026 valid 0.81311, valid PER 21.86%
EPOCH 17:
  batch 50 loss: 0.23532825887203215
  batch 100 loss: 0.24842475354671478
  batch 150 loss: 0.24955949693918228
  batch 200 loss: 0.23357029035687446
  batch 250 loss: 0.23962659910321235
  batch 300 loss: 0.2393097199499607
  batch 350 loss: 0.22888579502701759
  batch 400 loss: 0.2489563237130642
  batch 450 loss: 0.2364201256632805
  batch 500 loss: 0.22342581897974015
  batch 550 loss: 0.22472626209259033
  batch 600 loss: 0.23483040541410447
  batch 650 loss: 0.23651359736919403
  batch 700 loss: 0.2333512319624424
  batch 750 loss: 0.23377600684762
  batch 800 loss: 0.22324485316872597
  batch 850 loss: 0.2463119301199913
  batch 900 loss: 0.2290554802119732
LOSS train 0.22906 valid 0.81622, valid PER 21.86%
EPOCH 18:
  batch 50 loss: 0.23401110455393792
  batch 100 loss: 0.2402055774629116
  batch 150 loss: 0.24815315783023834
  batch 200 loss: 0.23218109220266342
  batch 250 loss: 0.25039037346839904
  batch 300 loss: 0.22651910781860352
  batch 350 loss: 0.22006849363446235
  batch 400 loss: 0.2249135223031044
  batch 450 loss: 0.23664481312036514
  batch 500 loss: 0.23249586299061775
  batch 550 loss: 0.24483020827174187
  batch 600 loss: 0.21870018929243087
  batch 650 loss: 0.22182496041059493
  batch 700 loss: 0.24050301879644395
  batch 750 loss: 0.22006304681301117
  batch 800 loss: 0.22567300483584404
  batch 850 loss: 0.22464047715067864
  batch 900 loss: 0.24152301013469696
LOSS train 0.24152 valid 0.81632, valid PER 21.90%
EPOCH 19:
  batch 50 loss: 0.2367072282731533
  batch 100 loss: 0.23144418209791184
  batch 150 loss: 0.22346726566553116
  batch 200 loss: 0.23042330592870713
  batch 250 loss: 0.21994811907410622
  batch 300 loss: 0.23921136796474457
  batch 350 loss: 0.2187220999598503
  batch 400 loss: 0.22515936106443404
  batch 450 loss: 0.24423602759838103
  batch 500 loss: 0.23663044586777687
  batch 550 loss: 0.22342902958393096
  batch 600 loss: 0.21389752641320228
  batch 650 loss: 0.2518182528018951
  batch 700 loss: 0.22455821618437766
  batch 750 loss: 0.22382151156663896
  batch 800 loss: 0.2344655264914036
  batch 850 loss: 0.23318652153015137
  batch 900 loss: 0.2396576836705208
LOSS train 0.23966 valid 0.81734, valid PER 21.85%
EPOCH 20:
  batch 50 loss: 0.2411341467499733
  batch 100 loss: 0.22828382432460784
  batch 150 loss: 0.227533228546381
  batch 200 loss: 0.22113512098789215
  batch 250 loss: 0.23014322012662888
  batch 300 loss: 0.24179764449596405
  batch 350 loss: 0.21995424687862397
  batch 400 loss: 0.23024061053991318
  batch 450 loss: 0.23514702737331392
  batch 500 loss: 0.21426306694746017
  batch 550 loss: 0.24206478148698807
  batch 600 loss: 0.22093142539262772
  batch 650 loss: 0.23520916372537612
  batch 700 loss: 0.2247870922088623
  batch 750 loss: 0.21358374528586865
  batch 800 loss: 0.2420360803604126
  batch 850 loss: 0.22731352254748344
  batch 900 loss: 0.2344397084414959
LOSS train 0.23444 valid 0.81803, valid PER 21.87%
Training finished in 23.0 minutes.
Model saved to checkpoints/20230118_103341/model_12
Loading model from checkpoints/20230118_103341/model_12
SUB: 14.11%, DEL: 6.97%, INS: 2.92%, COR: 78.92%, PER: 24.00%
