Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0, schedule='true')
cuda:0
Total number of model parameters is 8540200
EPOCH 1:
  batch 50 loss: 5.106293320655823
  batch 100 loss: 3.4155256366729736
  batch 150 loss: 3.2796434688568117
  batch 200 loss: 3.1323310470581056
  batch 250 loss: 2.9481216526031493
  batch 300 loss: 2.7183159399032593
  batch 350 loss: 2.5743243360519408
  batch 400 loss: 2.509644193649292
  batch 450 loss: 2.398639888763428
  batch 500 loss: 2.2962940645217897
  batch 550 loss: 2.221136226654053
  batch 600 loss: 2.1512969303131104
  batch 650 loss: 2.0482472562789917
  batch 700 loss: 2.0302553606033324
  batch 750 loss: 1.944970853328705
  batch 800 loss: 1.913736355304718
  batch 850 loss: 1.8569218516349792
  batch 900 loss: 1.8241657400131226
LOSS train 1.82417 valid 1.73864, valid PER 65.38%
EPOCH 2:
  batch 50 loss: 1.751584243774414
  batch 100 loss: 1.6745534491539003
  batch 150 loss: 1.6351885080337525
  batch 200 loss: 1.6538777756690979
  batch 250 loss: 1.6400915503501892
  batch 300 loss: 1.5760694980621337
  batch 350 loss: 1.5090062928199768
  batch 400 loss: 1.4945766091346742
  batch 450 loss: 1.4501444435119628
  batch 500 loss: 1.4705365204811096
  batch 550 loss: 1.4674519038200378
  batch 600 loss: 1.403724799156189
  batch 650 loss: 1.4138580250740052
  batch 700 loss: 1.352088840007782
  batch 750 loss: 1.345953048467636
  batch 800 loss: 1.2727821731567384
  batch 850 loss: 1.274873424768448
  batch 900 loss: 1.2793607139587402
LOSS train 1.27936 valid 1.24161, valid PER 39.76%
EPOCH 3:
  batch 50 loss: 1.2326881778240204
  batch 100 loss: 1.2273162305355072
  batch 150 loss: 1.1847148740291595
  batch 200 loss: 1.1726478278636931
  batch 250 loss: 1.1582086277008057
  batch 300 loss: 1.1476242387294768
  batch 350 loss: 1.195436371564865
  batch 400 loss: 1.171681309938431
  batch 450 loss: 1.133440251350403
  batch 500 loss: 1.1139988827705383
  batch 550 loss: 1.1102952182292938
  batch 600 loss: 1.1045440113544465
  batch 650 loss: 1.0862771117687224
  batch 700 loss: 1.096016345024109
  batch 750 loss: 1.144098801612854
  batch 800 loss: 1.0652255856990813
  batch 850 loss: 1.107807352542877
  batch 900 loss: 1.0261394023895263
LOSS train 1.02614 valid 1.09614, valid PER 33.82%
EPOCH 4:
  batch 50 loss: 1.0070025289058686
  batch 100 loss: 1.0284482669830322
  batch 150 loss: 1.0004124188423156
  batch 200 loss: 1.0160857653617859
  batch 250 loss: 1.0282818019390105
  batch 300 loss: 1.0196057879924774
  batch 350 loss: 0.9664534938335418
  batch 400 loss: 0.9921512186527253
  batch 450 loss: 0.9810802602767944
  batch 500 loss: 0.9719619953632355
  batch 550 loss: 0.9842954039573669
  batch 600 loss: 0.9995510053634643
  batch 650 loss: 0.9825869131088257
  batch 700 loss: 0.9615729141235352
  batch 750 loss: 0.9458796274662018
  batch 800 loss: 0.921054984331131
  batch 850 loss: 0.9427907180786133
  batch 900 loss: 0.9866087257862091
LOSS train 0.98661 valid 0.95326, valid PER 30.40%
EPOCH 5:
  batch 50 loss: 0.8971781647205352
  batch 100 loss: 0.8814364552497864
  batch 150 loss: 0.9411690330505371
  batch 200 loss: 0.8573635268211365
  batch 250 loss: 0.8978844523429871
  batch 300 loss: 0.886987431049347
  batch 350 loss: 0.87256991147995
  batch 400 loss: 0.9048192393779755
  batch 450 loss: 0.8801528346538544
  batch 500 loss: 0.8973694086074829
  batch 550 loss: 0.8411862909793854
  batch 600 loss: 0.914193241596222
  batch 650 loss: 0.8752258813381195
  batch 700 loss: 0.9291694414615631
  batch 750 loss: 0.8420703709125519
  batch 800 loss: 0.8713275420665741
  batch 850 loss: 0.8632381963729858
  batch 900 loss: 0.8602811944484711
LOSS train 0.86028 valid 0.90995, valid PER 28.59%
EPOCH 6:
  batch 50 loss: 0.8636263644695282
  batch 100 loss: 0.794661294221878
  batch 150 loss: 0.774547643661499
  batch 200 loss: 0.804061371088028
  batch 250 loss: 0.8437140488624573
  batch 300 loss: 0.8240013211965561
  batch 350 loss: 0.8103702753782273
  batch 400 loss: 0.790847761631012
  batch 450 loss: 0.8118439745903016
  batch 500 loss: 0.804136061668396
  batch 550 loss: 0.8147151911258698
  batch 600 loss: 0.794601583480835
  batch 650 loss: 0.7967268341779709
  batch 700 loss: 0.7945241737365722
  batch 750 loss: 0.7958645796775818
  batch 800 loss: 0.7923381292819976
  batch 850 loss: 0.7788095664978028
  batch 900 loss: 0.791296832561493
LOSS train 0.79130 valid 0.87386, valid PER 27.16%
EPOCH 7:
  batch 50 loss: 0.7568579685688018
  batch 100 loss: 0.7596734535694122
  batch 150 loss: 0.7365392404794693
  batch 200 loss: 0.7281392645835877
  batch 250 loss: 0.7359163618087768
  batch 300 loss: 0.7259611463546753
  batch 350 loss: 0.7396847784519196
  batch 400 loss: 0.7327948582172393
  batch 450 loss: 0.7294281876087189
  batch 500 loss: 0.7504668617248536
  batch 550 loss: 0.7217831110954285
  batch 600 loss: 0.7362043821811676
  batch 650 loss: 0.7323857128620148
  batch 700 loss: 0.7708301949501037
  batch 750 loss: 0.7284680771827697
  batch 800 loss: 0.73147325694561
  batch 850 loss: 0.7559803712368012
  batch 900 loss: 0.7771559834480286
LOSS train 0.77716 valid 0.85463, valid PER 26.63%
EPOCH 8:
  batch 50 loss: 0.6753962397575378
  batch 100 loss: 0.6581200617551803
  batch 150 loss: 0.6794807422161102
  batch 200 loss: 0.6700765985250473
  batch 250 loss: 0.6810180592536926
  batch 300 loss: 0.634863338470459
  batch 350 loss: 0.7147519451379776
  batch 400 loss: 0.6626827597618103
  batch 450 loss: 0.7074894118309021
  batch 500 loss: 0.7120955288410187
  batch 550 loss: 0.6547744989395141
  batch 600 loss: 0.7193044412136078
  batch 650 loss: 0.713679775595665
  batch 700 loss: 0.6622959280014038
  batch 750 loss: 0.670504138469696
  batch 800 loss: 0.6825031900405883
  batch 850 loss: 0.669271919131279
  batch 900 loss: 0.6873279160261154
LOSS train 0.68733 valid 0.81444, valid PER 25.13%
EPOCH 9:
  batch 50 loss: 0.592691770195961
  batch 100 loss: 0.6261599963903427
  batch 150 loss: 0.6230037891864777
  batch 200 loss: 0.6046728056669235
  batch 250 loss: 0.630957596898079
  batch 300 loss: 0.6242823392152786
  batch 350 loss: 0.659863486289978
  batch 400 loss: 0.6173734742403031
  batch 450 loss: 0.6240225666761399
  batch 500 loss: 0.6102049088478089
  batch 550 loss: 0.6489575117826462
  batch 600 loss: 0.6489467459917069
  batch 650 loss: 0.6315156960487366
  batch 700 loss: 0.6060768431425094
  batch 750 loss: 0.6216823863983154
  batch 800 loss: 0.6450378799438476
  batch 850 loss: 0.6596385234594345
  batch 900 loss: 0.5905237066745758
LOSS train 0.59052 valid 0.82999, valid PER 25.23%
EPOCH 10:
  batch 50 loss: 0.5060640037059784
  batch 100 loss: 0.49742520332336426
  batch 150 loss: 0.5038706761598587
  batch 200 loss: 0.5139946419000626
  batch 250 loss: 0.5213864183425904
  batch 300 loss: 0.48743708491325377
  batch 350 loss: 0.5052450263500213
  batch 400 loss: 0.4690541344881058
  batch 450 loss: 0.4774646627902985
  batch 500 loss: 0.5067109578847885
  batch 550 loss: 0.5070756781101227
  batch 600 loss: 0.49768241167068483
  batch 650 loss: 0.4920196110010147
  batch 700 loss: 0.5013856571912766
  batch 750 loss: 0.48725889265537264
  batch 800 loss: 0.5076084542274475
  batch 850 loss: 0.49152281165122985
  batch 900 loss: 0.5028556036949158
LOSS train 0.50286 valid 0.74728, valid PER 23.02%
EPOCH 11:
  batch 50 loss: 0.42236748278141023
  batch 100 loss: 0.39586140513420104
  batch 150 loss: 0.4192420902848244
  batch 200 loss: 0.4569635450839996
  batch 250 loss: 0.4531101167201996
  batch 300 loss: 0.4176126539707184
  batch 350 loss: 0.44125529706478117
  batch 400 loss: 0.4552985143661499
  batch 450 loss: 0.45678312301635743
  batch 500 loss: 0.4335357290506363
  batch 550 loss: 0.4568051707744598
  batch 600 loss: 0.4346936851739883
  batch 650 loss: 0.49433679282665255
  batch 700 loss: 0.43393776297569275
  batch 750 loss: 0.44340669572353364
  batch 800 loss: 0.45361319482326506
  batch 850 loss: 0.475988430082798
  batch 900 loss: 0.4714287847280502
LOSS train 0.47143 valid 0.75601, valid PER 22.41%
EPOCH 12:
  batch 50 loss: 0.39000854432582854
  batch 100 loss: 0.37851054400205614
  batch 150 loss: 0.3274571713805199
  batch 200 loss: 0.3649994912743568
  batch 250 loss: 0.3715290558338165
  batch 300 loss: 0.360771823823452
  batch 350 loss: 0.3467886033654213
  batch 400 loss: 0.3678629186749458
  batch 450 loss: 0.3767442819476128
  batch 500 loss: 0.36768915683031084
  batch 550 loss: 0.3419147929549217
  batch 600 loss: 0.3683862647414207
  batch 650 loss: 0.3712141087651253
  batch 700 loss: 0.3811356845498085
  batch 750 loss: 0.3555400148034096
  batch 800 loss: 0.3746128770709038
  batch 850 loss: 0.39947075933218
  batch 900 loss: 0.3770664808154106
LOSS train 0.37707 valid 0.74305, valid PER 21.84%
EPOCH 13:
  batch 50 loss: 0.32740010499954225
  batch 100 loss: 0.3186131310462952
  batch 150 loss: 0.31873830407857895
  batch 200 loss: 0.3410444840788841
  batch 250 loss: 0.3286797094345093
  batch 300 loss: 0.3303525596857071
  batch 350 loss: 0.3190435770153999
  batch 400 loss: 0.3415727087855339
  batch 450 loss: 0.3320398244261742
  batch 500 loss: 0.3318611425161362
  batch 550 loss: 0.3540932279825211
  batch 600 loss: 0.3242041203379631
  batch 650 loss: 0.3548734658956528
  batch 700 loss: 0.35511929005384446
  batch 750 loss: 0.3118940949440002
  batch 800 loss: 0.33808126717805864
  batch 850 loss: 0.3541118738055229
  batch 900 loss: 0.3584634524583816
LOSS train 0.35846 valid 0.75281, valid PER 21.84%
EPOCH 14:
  batch 50 loss: 0.29432816326618194
  batch 100 loss: 0.29890199661254885
  batch 150 loss: 0.28622733980417253
  batch 200 loss: 0.27113604962825777
  batch 250 loss: 0.2888843223452568
  batch 300 loss: 0.3136450922489166
  batch 350 loss: 0.2834944573044777
  batch 400 loss: 0.2860051053762436
  batch 450 loss: 0.2823164340853691
  batch 500 loss: 0.290614560842514
  batch 550 loss: 0.30092759042978284
  batch 600 loss: 0.2689413815736771
  batch 650 loss: 0.3009855794906616
  batch 700 loss: 0.3050178074836731
  batch 750 loss: 0.2805875062942505
  batch 800 loss: 0.2858172956109047
  batch 850 loss: 0.3071882128715515
  batch 900 loss: 0.295692857503891
LOSS train 0.29569 valid 0.75809, valid PER 21.84%
EPOCH 15:
  batch 50 loss: 0.2665990248322487
  batch 100 loss: 0.2576515120267868
  batch 150 loss: 0.2649458146095276
  batch 200 loss: 0.2661596018075943
  batch 250 loss: 0.2879036256670952
  batch 300 loss: 0.25582662999629974
  batch 350 loss: 0.26168595656752586
  batch 400 loss: 0.26858532935380935
  batch 450 loss: 0.25555082693696024
  batch 500 loss: 0.2610227978229523
  batch 550 loss: 0.2636019468307495
  batch 600 loss: 0.27173894703388213
  batch 650 loss: 0.2839763379096985
  batch 700 loss: 0.2830933055281639
  batch 750 loss: 0.26125524371862413
  batch 800 loss: 0.25364937901496887
  batch 850 loss: 0.24802468091249466
  batch 900 loss: 0.2627537442743778
LOSS train 0.26275 valid 0.76194, valid PER 21.67%
EPOCH 16:
  batch 50 loss: 0.24697473555803298
  batch 100 loss: 0.24295927256345748
  batch 150 loss: 0.23907085925340651
  batch 200 loss: 0.24721009254455567
  batch 250 loss: 0.2549240627884865
  batch 300 loss: 0.2578018802404404
  batch 350 loss: 0.2548026631772518
  batch 400 loss: 0.2674794140458107
  batch 450 loss: 0.256895956993103
  batch 500 loss: 0.2356874454021454
  batch 550 loss: 0.24201065585017204
  batch 600 loss: 0.2431390580534935
  batch 650 loss: 0.26464688539505005
  batch 700 loss: 0.23956007480621339
  batch 750 loss: 0.25342605024576187
  batch 800 loss: 0.2603665095567703
  batch 850 loss: 0.246967853307724
  batch 900 loss: 0.2564848294854164
LOSS train 0.25648 valid 0.76456, valid PER 21.69%
EPOCH 17:
  batch 50 loss: 0.24881089568138123
  batch 100 loss: 0.24800738483667373
  batch 150 loss: 0.2451490880548954
  batch 200 loss: 0.24460496574640275
  batch 250 loss: 0.2562821826338768
  batch 300 loss: 0.24260603070259093
  batch 350 loss: 0.23291010066866874
  batch 400 loss: 0.25895678758621216
  batch 450 loss: 0.24641977459192277
  batch 500 loss: 0.23674528777599335
  batch 550 loss: 0.238505799472332
  batch 600 loss: 0.23981757268309592
  batch 650 loss: 0.2374875482916832
  batch 700 loss: 0.2409308123588562
  batch 750 loss: 0.24469829723238945
  batch 800 loss: 0.2261611472070217
  batch 850 loss: 0.24760471120476724
  batch 900 loss: 0.2308621820807457
LOSS train 0.23086 valid 0.76660, valid PER 21.69%
EPOCH 18:
  batch 50 loss: 0.2474736413359642
  batch 100 loss: 0.2392963045835495
  batch 150 loss: 0.26088202312588693
  batch 200 loss: 0.24166916713118552
  batch 250 loss: 0.2517962035536766
  batch 300 loss: 0.23224444478750228
  batch 350 loss: 0.23023251816630363
  batch 400 loss: 0.23689917236566543
  batch 450 loss: 0.2447218681871891
  batch 500 loss: 0.24362026989459992
  batch 550 loss: 0.24616832211613654
  batch 600 loss: 0.2213654774427414
  batch 650 loss: 0.2281401512026787
  batch 700 loss: 0.2426518103480339
  batch 750 loss: 0.22559660091996192
  batch 800 loss: 0.2297408974170685
  batch 850 loss: 0.23161790996789933
  batch 900 loss: 0.24569006353616715
LOSS train 0.24569 valid 0.76685, valid PER 21.72%
EPOCH 19:
  batch 50 loss: 0.23884016424417495
  batch 100 loss: 0.24085191503167153
  batch 150 loss: 0.22638349786400794
  batch 200 loss: 0.23598554849624634
  batch 250 loss: 0.23252755224704744
  batch 300 loss: 0.24638745486736296
  batch 350 loss: 0.22854841232299805
  batch 400 loss: 0.23134419500827788
  batch 450 loss: 0.25140205174684527
  batch 500 loss: 0.24040129363536836
  batch 550 loss: 0.23333731591701506
  batch 600 loss: 0.2263627952337265
  batch 650 loss: 0.2562893739342689
  batch 700 loss: 0.23747214168310166
  batch 750 loss: 0.22899657487869263
  batch 800 loss: 0.24010265856981278
  batch 850 loss: 0.2358704149723053
  batch 900 loss: 0.23812320053577424
LOSS train 0.23812 valid 0.76739, valid PER 21.66%
EPOCH 20:
  batch 50 loss: 0.24451613619923593
  batch 100 loss: 0.22785663664340972
  batch 150 loss: 0.22770806461572646
  batch 200 loss: 0.23511752873659134
  batch 250 loss: 0.23871251702308655
  batch 300 loss: 0.2469536255300045
  batch 350 loss: 0.23058140709996222
  batch 400 loss: 0.23982097670435906
  batch 450 loss: 0.2360790875554085
  batch 500 loss: 0.21983865186572074
  batch 550 loss: 0.2515379539132118
  batch 600 loss: 0.22984350740909576
  batch 650 loss: 0.23957104235887527
  batch 700 loss: 0.23198328226804732
  batch 750 loss: 0.22362879186868667
  batch 800 loss: 0.24448639988899232
  batch 850 loss: 0.24164633989334106
  batch 900 loss: 0.23453470215201377
LOSS train 0.23453 valid 0.76783, valid PER 21.64%
Training finished in 26.0 minutes.
Model saved to checkpoints/20230118_113952/model_12
Loading model from checkpoints/20230118_113952/model_12
SUB: 13.92%, DEL: 6.94%, INS: 2.50%, COR: 79.15%, PER: 23.36%
