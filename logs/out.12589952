Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.0, clip_norm=0.0, schedule='false')
cuda:0
Total number of model parameters is 83496
EPOCH 1:
  batch 50 loss: 4.1314401531219485
  batch 100 loss: 3.2952369451522827
  batch 150 loss: 3.2125195026397706
  batch 200 loss: 3.0345269775390626
  batch 250 loss: 2.8266790771484374
  batch 300 loss: 2.633804383277893
  batch 350 loss: 2.4993526268005373
  batch 400 loss: 2.4298356008529662
  batch 450 loss: 2.3436490297317505
  batch 500 loss: 2.2136911797523497
  batch 550 loss: 2.1574394369125365
  batch 600 loss: 2.108227782249451
  batch 650 loss: 2.0553999066352846
  batch 700 loss: 2.0385848355293272
  batch 750 loss: 1.9741507983207702
  batch 800 loss: 1.9546223258972169
  batch 850 loss: 1.8984559178352356
  batch 900 loss: 1.8794408750534057
LOSS train 1.87944 valid 1.87442, valid PER 68.43%
EPOCH 2:
  batch 50 loss: 1.8325913166999817
  batch 100 loss: 1.7631332874298096
  batch 150 loss: 1.733012306690216
  batch 200 loss: 1.7495942783355714
  batch 250 loss: 1.7317978715896607
  batch 300 loss: 1.7023418569564819
  batch 350 loss: 1.6149934363365173
  batch 400 loss: 1.6162663292884827
  batch 450 loss: 1.5563413357734681
  batch 500 loss: 1.5989385771751403
  batch 550 loss: 1.58862535238266
  batch 600 loss: 1.5319629549980163
  batch 650 loss: 1.581041226387024
  batch 700 loss: 1.5357903361320495
  batch 750 loss: 1.5168822002410889
  batch 800 loss: 1.4659587788581847
  batch 850 loss: 1.4672837114334107
  batch 900 loss: 1.486940987110138
LOSS train 1.48694 valid 1.46752, valid PER 47.34%
EPOCH 3:
  batch 50 loss: 1.4422112059593202
  batch 100 loss: 1.4505481123924255
  batch 150 loss: 1.4241733789443969
  batch 200 loss: 1.419403507709503
  batch 250 loss: 1.3949847364425658
  batch 300 loss: 1.4071938729286193
  batch 350 loss: 1.4393280911445618
  batch 400 loss: 1.4424281883239747
  batch 450 loss: 1.4027334260940552
  batch 500 loss: 1.4198706316947938
  batch 550 loss: 1.4120203423500062
  batch 600 loss: 1.3692689967155456
  batch 650 loss: 1.3289625334739685
  batch 700 loss: 1.3492607140541077
  batch 750 loss: 1.415348973274231
  batch 800 loss: 1.3596548545360565
  batch 850 loss: 1.374892108440399
  batch 900 loss: 1.3313150882720948
LOSS train 1.33132 valid 1.36518, valid PER 43.41%
EPOCH 4:
  batch 50 loss: 1.290506591796875
  batch 100 loss: 1.3261047840118407
  batch 150 loss: 1.2813638043403626
  batch 200 loss: 1.338204505443573
  batch 250 loss: 1.3268087577819825
  batch 300 loss: 1.3611475801467896
  batch 350 loss: 1.2443483924865724
  batch 400 loss: 1.3035780847072602
  batch 450 loss: 1.3509547758102416
  batch 500 loss: 1.2927994346618652
  batch 550 loss: 1.3330831408500672
  batch 600 loss: 1.3138001108169555
  batch 650 loss: 1.3032015252113343
  batch 700 loss: 1.2764418363571166
  batch 750 loss: 1.2720853567123414
  batch 800 loss: 1.2750258338451386
  batch 850 loss: 1.2762941873073579
  batch 900 loss: 1.3168715119361878
LOSS train 1.31687 valid 1.30341, valid PER 39.64%
EPOCH 5:
  batch 50 loss: 1.251031950712204
  batch 100 loss: 1.2394201624393464
  batch 150 loss: 1.2509563529491425
  batch 200 loss: 1.1975537574291228
  batch 250 loss: 1.2196704411506654
  batch 300 loss: 1.213943738937378
  batch 350 loss: 1.1970072817802428
  batch 400 loss: 1.241527497768402
  batch 450 loss: 1.216804826259613
  batch 500 loss: 1.2220740723609924
  batch 550 loss: 1.1885725224018098
  batch 600 loss: 1.29262859582901
  batch 650 loss: 1.235093319416046
  batch 700 loss: 1.2788487267494202
  batch 750 loss: 1.188460785150528
  batch 800 loss: 1.2170227229595185
  batch 850 loss: 1.2258783221244811
  batch 900 loss: 1.2409054303169251
LOSS train 1.24091 valid 1.24314, valid PER 38.44%
EPOCH 6:
  batch 50 loss: 1.2120838510990142
  batch 100 loss: 1.1814562785625458
  batch 150 loss: 1.1762768363952636
  batch 200 loss: 1.244807449579239
  batch 250 loss: 1.2250451803207398
  batch 300 loss: 1.2360101866722106
  batch 350 loss: 1.1946889519691468
  batch 400 loss: 1.192062427997589
  batch 450 loss: 1.2143456399440766
  batch 500 loss: 1.2154552233219147
  batch 550 loss: 1.2721245336532592
  batch 600 loss: 1.1839563345909119
  batch 650 loss: 1.204254378080368
  batch 700 loss: 1.2063963437080383
  batch 750 loss: 1.2454291689395904
  batch 800 loss: 1.2299514651298522
  batch 850 loss: 1.1879265809059143
  batch 900 loss: 1.2711735928058625
LOSS train 1.27117 valid 1.63684, valid PER 50.79%
EPOCH 7:
  batch 50 loss: 1.410112943649292
  batch 100 loss: 1.3279204082489013
  batch 150 loss: 1.2613466811180114
  batch 200 loss: 1.2085626423358917
  batch 250 loss: 1.2117249727249146
  batch 300 loss: 1.179657562971115
  batch 350 loss: 1.1802407264709474
  batch 400 loss: 1.1729718136787415
  batch 450 loss: 1.1804491257667542
  batch 500 loss: 1.1575462317466736
  batch 550 loss: 1.1525264549255372
  batch 600 loss: 1.1646465635299683
  batch 650 loss: 1.1653575491905213
  batch 700 loss: 1.224756691455841
  batch 750 loss: 1.1563473963737487
  batch 800 loss: 1.1367771553993224
  batch 850 loss: 1.205739051103592
  batch 900 loss: 1.2141413640975953
LOSS train 1.21414 valid 1.34533, valid PER 39.71%
EPOCH 8:
  batch 50 loss: 1.1717288517951965
  batch 100 loss: 1.1205736267566682
  batch 150 loss: 1.1139778840541839
  batch 200 loss: 1.0979671037197114
  batch 250 loss: 1.1117338967323303
  batch 300 loss: 1.0739910888671875
  batch 350 loss: 1.1560737073421479
  batch 400 loss: 1.1011074209213256
  batch 450 loss: 1.160642157793045
  batch 500 loss: 1.159703222513199
  batch 550 loss: 1.111090875864029
  batch 600 loss: 1.1213013446331024
  batch 650 loss: 1.1527478885650635
  batch 700 loss: 1.1061140727996825
  batch 750 loss: 1.116072268486023
  batch 800 loss: 1.11971497297287
  batch 850 loss: 1.197797257900238
  batch 900 loss: 1.1334678399562836
LOSS train 1.13347 valid 1.18317, valid PER 36.81%
EPOCH 9:
  batch 50 loss: 1.1140351843833924
  batch 100 loss: 1.1971345770359039
  batch 150 loss: 1.1708781504631043
  batch 200 loss: 1.0817077004909514
  batch 250 loss: 1.1311430907249451
  batch 300 loss: 1.1445682621002198
  batch 350 loss: 1.1638716065883636
  batch 400 loss: 1.1308268547058105
  batch 450 loss: 1.1399686646461487
  batch 500 loss: 1.0732261908054352
  batch 550 loss: 1.1688237249851228
  batch 600 loss: 1.1631647121906281
  batch 650 loss: 1.1289588141441345
  batch 700 loss: 1.1359556579589845
  batch 750 loss: 1.1667443883419037
  batch 800 loss: 1.1469274854660034
  batch 850 loss: 1.1802716159820557
  batch 900 loss: 1.1445527827739717
LOSS train 1.14455 valid 1.26494, valid PER 37.24%
EPOCH 10:
  batch 50 loss: 1.1637215399742127
  batch 100 loss: 1.1275317502021789
  batch 150 loss: 1.1864296197891235
  batch 200 loss: 1.1263550448417663
  batch 250 loss: 1.2321078777313232
  batch 300 loss: 1.1128605544567107
  batch 350 loss: 1.1864615833759309
  batch 400 loss: 1.1136620807647706
  batch 450 loss: 1.1401657390594482
  batch 500 loss: 1.3030684876441956
  batch 550 loss: 1.2566251015663148
  batch 600 loss: 1.1508887207508087
  batch 650 loss: 1.151792484521866
  batch 700 loss: 1.212774738073349
  batch 750 loss: 1.1296664571762085
  batch 800 loss: 1.1458827078342437
  batch 850 loss: 1.138994048833847
  batch 900 loss: 1.1858020842075347
LOSS train 1.18580 valid 1.24386, valid PER 38.41%
EPOCH 11:
  batch 50 loss: 1.1146815621852875
  batch 100 loss: 1.092991441488266
  batch 150 loss: 1.0966493153572083
  batch 200 loss: 1.1382953763008117
  batch 250 loss: 1.126178833246231
  batch 300 loss: 1.1387874066829682
  batch 350 loss: 1.171873744726181
  batch 400 loss: 1.174074308872223
  batch 450 loss: 1.114175477027893
  batch 500 loss: 1.085457991361618
  batch 550 loss: 1.1186077249050141
  batch 600 loss: 1.0901916432380676
  batch 650 loss: 1.1227810096740722
  batch 700 loss: 1.059170935153961
  batch 750 loss: 1.0804445576667785
  batch 800 loss: 1.1241927814483643
  batch 850 loss: 1.1275681054592133
  batch 900 loss: 1.1218181025981904
LOSS train 1.12182 valid 1.15489, valid PER 35.44%
EPOCH 12:
  batch 50 loss: 1.1176266825199128
  batch 100 loss: 1.0916168153285981
  batch 150 loss: 1.0910735964775085
  batch 200 loss: 1.1084154641628265
  batch 250 loss: 1.0669913232326507
  batch 300 loss: 1.0600027859210968
  batch 350 loss: 1.0671415865421294
  batch 400 loss: 1.0991723108291627
  batch 450 loss: 1.093808617591858
  batch 500 loss: 1.1042394483089446
  batch 550 loss: 1.5362474846839904
  batch 600 loss: 1.3408916044235228
  batch 650 loss: 1.32745321393013
  batch 700 loss: 1.2805685639381408
  batch 750 loss: 1.1989999437332153
  batch 800 loss: 1.1683687496185302
  batch 850 loss: 1.2057131850719451
  batch 900 loss: 1.2130281472206115
LOSS train 1.21303 valid 1.21112, valid PER 37.27%
EPOCH 13:
  batch 50 loss: 1.2318564546108246
  batch 100 loss: 1.3003063297271729
  batch 150 loss: 1.1776587069034576
  batch 200 loss: 1.1691294729709625
  batch 250 loss: 1.2268121004104615
  batch 300 loss: 1.22710329413414
  batch 350 loss: 1.2053891849517822
  batch 400 loss: 1.1803881216049195
  batch 450 loss: 1.195767571926117
  batch 500 loss: 1.1426592707633971
  batch 550 loss: 1.1646175384521484
  batch 600 loss: 1.1442256665229797
  batch 650 loss: 1.1492664444446563
  batch 700 loss: 1.2065481281280517
  batch 750 loss: 1.1472839176654817
  batch 800 loss: 1.1523123157024384
  batch 850 loss: 1.2356999433040619
  batch 900 loss: 1.2239286410808563
LOSS train 1.22393 valid 1.24522, valid PER 37.79%
EPOCH 14:
  batch 50 loss: 1.1649568808078765
  batch 100 loss: 1.2119403743743897
  batch 150 loss: 1.1850019931793212
  batch 200 loss: 1.2859002673625946
  batch 250 loss: 1.305996515750885
  batch 300 loss: 1.3664375352859497
  batch 350 loss: 1.4526962161064148
  batch 400 loss: 1.413892719745636
  batch 450 loss: 1.3684476280212403
  batch 500 loss: 1.3568085956573486
  batch 550 loss: 1.3234000253677367
  batch 600 loss: 1.2841838121414184
  batch 650 loss: 1.322795830965042
  batch 700 loss: 1.2938802695274354
  batch 750 loss: 1.2017598938941956
  batch 800 loss: 1.241807428598404
  batch 850 loss: 1.2448490428924561
  batch 900 loss: 1.231922175884247
LOSS train 1.23192 valid 1.25077, valid PER 38.79%
EPOCH 15:
  batch 50 loss: 1.2162628197669982
  batch 100 loss: 1.19387722492218
  batch 150 loss: 1.1922999584674836
  batch 200 loss: 1.2056231331825256
  batch 250 loss: 1.2551993012428284
  batch 300 loss: 1.3043124508857726
  batch 350 loss: 1.2347841560840607
  batch 400 loss: 1.258639681339264
  batch 450 loss: 1.2456459534168243
  batch 500 loss: 1.189638397693634
  batch 550 loss: 1.5938956618309021
  batch 600 loss: 1.4248174905776978
  batch 650 loss: 1.3359114027023316
  batch 700 loss: 1.3008710837364197
  batch 750 loss: 1.2463824093341827
  batch 800 loss: 1.2040148389339447
  batch 850 loss: 1.2244893133640289
  batch 900 loss: 1.216299821138382
LOSS train 1.21630 valid 1.26301, valid PER 38.14%
EPOCH 16:
  batch 50 loss: 1.236338552236557
  batch 100 loss: 1.1734515857696532
  batch 150 loss: 1.1507953333854675
  batch 200 loss: 1.153351616859436
  batch 250 loss: 1.234997069835663
  batch 300 loss: 1.1808872818946838
  batch 350 loss: 1.203058898448944
  batch 400 loss: 1.1542895078659057
  batch 450 loss: 1.2095840287208557
  batch 500 loss: 1.1278939855098724
  batch 550 loss: 1.1840671575069428
  batch 600 loss: 1.175444233417511
  batch 650 loss: 1.2204684472084046
  batch 700 loss: 1.1752996969223022
  batch 750 loss: 1.1967707633972169
  batch 800 loss: 1.163222062587738
  batch 850 loss: 1.1520203983783721
  batch 900 loss: 1.16688756108284
LOSS train 1.16689 valid 1.25257, valid PER 38.23%
EPOCH 17:
  batch 50 loss: 1.1493626523017884
  batch 100 loss: 1.1501583456993103
  batch 150 loss: 1.2004782140254975
  batch 200 loss: 1.1464290714263916
  batch 250 loss: 1.231170425415039
  batch 300 loss: 1.2049537265300752
  batch 350 loss: 1.2438577866554261
  batch 400 loss: 1.254745581150055
  batch 450 loss: 1.291811615228653
  batch 500 loss: 1.204578274488449
  batch 550 loss: 1.1971170377731324
  batch 600 loss: 1.272637002468109
  batch 650 loss: 1.1872175014019013
  batch 700 loss: 1.1739017367362976
  batch 750 loss: 1.1532611048221588
  batch 800 loss: 1.1285367608070374
  batch 850 loss: 1.2018036913871766
  batch 900 loss: 1.2407267212867736
LOSS train 1.24073 valid 1.23088, valid PER 37.30%
EPOCH 18:
  batch 50 loss: 1.1763407099246979
  batch 100 loss: 1.1738649201393128
  batch 150 loss: 1.186270570755005
  batch 200 loss: 1.1486120212078095
  batch 250 loss: 1.1428601908683778
  batch 300 loss: 1.1769790279865264
  batch 350 loss: 1.2211797153949737
  batch 400 loss: 1.1381461024284363
  batch 450 loss: 1.1907295906543731
  batch 500 loss: 1.1639619398117065
  batch 550 loss: 1.1385315072536468
  batch 600 loss: 1.1326051723957062
  batch 650 loss: 1.194136732816696
  batch 700 loss: 1.1978888750076293
  batch 750 loss: 1.1389781832695007
  batch 800 loss: 1.2130746364593505
  batch 850 loss: 1.2752886784076691
  batch 900 loss: 1.2452243959903717
LOSS train 1.24522 valid 1.37148, valid PER 42.04%
EPOCH 19:
  batch 50 loss: 1.3729062724113463
  batch 100 loss: 1.373288836479187
  batch 150 loss: 1.5483586049079896
  batch 200 loss: 1.4584963703155518
  batch 250 loss: 1.4054157686233522
  batch 300 loss: 1.315504981279373
  batch 350 loss: 1.221668026447296
  batch 400 loss: 1.3587378478050232
  batch 450 loss: 1.3747185468673706
  batch 500 loss: 1.298720018863678
  batch 550 loss: 1.3060568070411682
  batch 600 loss: 1.2675487899780273
  batch 650 loss: 1.3000466561317443
  batch 700 loss: 1.2222131371498108
  batch 750 loss: 1.2361965107917785
  batch 800 loss: 1.2607716608047486
  batch 850 loss: 1.3055617463588716
  batch 900 loss: 1.308325617313385
LOSS train 1.30833 valid 1.39065, valid PER 42.97%
EPOCH 20:
  batch 50 loss: 1.3533435583114624
  batch 100 loss: 1.34841237783432
  batch 150 loss: 1.345525062084198
  batch 200 loss: 1.309565315246582
  batch 250 loss: 1.278084818124771
  batch 300 loss: 1.3410817325115203
  batch 350 loss: 1.3249915301799775
  batch 400 loss: 1.373881709575653
  batch 450 loss: 1.3472741532325745
  batch 500 loss: 1.303331091403961
  batch 550 loss: 1.5176750230789184
  batch 600 loss: 1.5402930855751038
  batch 650 loss: 1.612746787071228
  batch 700 loss: 1.5588414788246154
  batch 750 loss: 1.4569992351531982
  batch 800 loss: 1.5008636498451233
  batch 850 loss: 1.4487286448478698
  batch 900 loss: 1.4587527322769165
LOSS train 1.45875 valid 1.48146, valid PER 43.63%
Training finished in 8.0 minutes.
Model saved to checkpoints/20230118_121159/model_11
Loading model from checkpoints/20230118_121159/model_11
SUB: 21.02%, DEL: 13.93%, INS: 2.06%, COR: 65.05%, PER: 37.01%
