Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout_prob=0.4, clip_norm=1.0, schedule='true')
cuda:0
Total number of model parameters is 3221544
EPOCH 1:
  batch 50 loss: 5.191611580848694
  batch 100 loss: 3.3982057046890257
  batch 150 loss: 3.293415174484253
  batch 200 loss: 3.219431309700012
  batch 250 loss: 3.1198118114471436
  batch 300 loss: 2.9149224519729615
  batch 350 loss: 2.776418476104736
  batch 400 loss: 2.677284560203552
  batch 450 loss: 2.570718626976013
  batch 500 loss: 2.4440415954589843
  batch 550 loss: 2.36180926322937
  batch 600 loss: 2.276203815937042
  batch 650 loss: 2.173963553905487
  batch 700 loss: 2.1612367844581604
  batch 750 loss: 2.0797235321998597
  batch 800 loss: 2.0502815985679628
  batch 850 loss: 1.9818771886825561
  batch 900 loss: 1.938176462650299
LOSS train 1.93818 valid 1.84558, valid PER 64.84%
EPOCH 2:
  batch 50 loss: 1.861622908115387
  batch 100 loss: 1.7815065336227418
  batch 150 loss: 1.7346586799621582
  batch 200 loss: 1.7573864722251893
  batch 250 loss: 1.7352464365959168
  batch 300 loss: 1.6991471195220946
  batch 350 loss: 1.5962598514556885
  batch 400 loss: 1.5869766974449158
  batch 450 loss: 1.5272519516944885
  batch 500 loss: 1.5553745937347412
  batch 550 loss: 1.5577518486976623
  batch 600 loss: 1.5129268813133239
  batch 650 loss: 1.5365286183357239
  batch 700 loss: 1.4753948092460631
  batch 750 loss: 1.4581489658355713
  batch 800 loss: 1.4145371294021607
  batch 850 loss: 1.401004421710968
  batch 900 loss: 1.4258157062530517
LOSS train 1.42582 valid 1.43450, valid PER 45.55%
EPOCH 3:
  batch 50 loss: 1.3806663680076598
  batch 100 loss: 1.3610096430778504
  batch 150 loss: 1.3401021695137023
  batch 200 loss: 1.3218189978599548
  batch 250 loss: 1.3225977432727813
  batch 300 loss: 1.3046190309524537
  batch 350 loss: 1.3616492414474488
  batch 400 loss: 1.313178713321686
  batch 450 loss: 1.2723238122463227
  batch 500 loss: 1.2948766660690307
  batch 550 loss: 1.2652684032917023
  batch 600 loss: 1.2482405531406402
  batch 650 loss: 1.2151260411739349
  batch 700 loss: 1.236162818670273
  batch 750 loss: 1.2712121140956878
  batch 800 loss: 1.2292113924026489
  batch 850 loss: 1.268910266160965
  batch 900 loss: 1.2027401781082154
LOSS train 1.20274 valid 1.21173, valid PER 37.80%
EPOCH 4:
  batch 50 loss: 1.167675290107727
  batch 100 loss: 1.2030975103378296
  batch 150 loss: 1.1404786479473115
  batch 200 loss: 1.147215062379837
  batch 250 loss: 1.1715999495983125
  batch 300 loss: 1.1824209022521972
  batch 350 loss: 1.117220287322998
  batch 400 loss: 1.1419963216781617
  batch 450 loss: 1.1841548955440522
  batch 500 loss: 1.1207548415660857
  batch 550 loss: 1.1510298883914947
  batch 600 loss: 1.178061571121216
  batch 650 loss: 1.15341059923172
  batch 700 loss: 1.1022554659843444
  batch 750 loss: 1.0933776342868804
  batch 800 loss: 1.0596375823020936
  batch 850 loss: 1.1352016460895538
  batch 900 loss: 1.1482999122142792
LOSS train 1.14830 valid 1.11202, valid PER 35.40%
EPOCH 5:
  batch 50 loss: 1.0588518977165222
  batch 100 loss: 1.0562180864810944
  batch 150 loss: 1.0970855498313903
  batch 200 loss: 1.0091288924217223
  batch 250 loss: 1.020972615480423
  batch 300 loss: 1.04449937582016
  batch 350 loss: 1.055680935382843
  batch 400 loss: 1.0635248124599457
  batch 450 loss: 1.0386982762813568
  batch 500 loss: 1.0491545224189758
  batch 550 loss: 1.0054417049884796
  batch 600 loss: 1.0704759991168975
  batch 650 loss: 1.0234721684455872
  batch 700 loss: 1.0754900968074799
  batch 750 loss: 1.0067130088806153
  batch 800 loss: 1.0304301083087921
  batch 850 loss: 1.0373264265060425
  batch 900 loss: 1.0213970398902894
LOSS train 1.02140 valid 1.06426, valid PER 33.72%
EPOCH 6:
  batch 50 loss: 1.0113282334804534
  batch 100 loss: 0.9646177899837494
  batch 150 loss: 0.9768149936199189
  batch 200 loss: 0.9888160943984985
  batch 250 loss: 1.0254112148284913
  batch 300 loss: 0.9934201455116272
  batch 350 loss: 0.9582756960391998
  batch 400 loss: 0.9511119592189788
  batch 450 loss: 1.0105231726169586
  batch 500 loss: 0.9883239710330963
  batch 550 loss: 0.9940138781070709
  batch 600 loss: 0.9802768349647522
  batch 650 loss: 1.0091475069522857
  batch 700 loss: 0.9796991765499115
  batch 750 loss: 0.9654264068603515
  batch 800 loss: 0.9587330889701843
  batch 850 loss: 0.9294982171058654
  batch 900 loss: 0.9469770324230194
LOSS train 0.94698 valid 0.99192, valid PER 30.73%
EPOCH 7:
  batch 50 loss: 0.9134561145305633
  batch 100 loss: 0.9620549714565277
  batch 150 loss: 0.9119078242778778
  batch 200 loss: 0.9510041034221649
  batch 250 loss: 0.9115387380123139
  batch 300 loss: 0.8762777853012085
  batch 350 loss: 0.8915249156951904
  batch 400 loss: 0.8892968738079071
  batch 450 loss: 0.8988886845111846
  batch 500 loss: 0.9051450884342194
  batch 550 loss: 0.9098638677597046
  batch 600 loss: 0.9539398491382599
  batch 650 loss: 0.9176752448081971
  batch 700 loss: 0.9119949543476105
  batch 750 loss: 0.9010836565494538
  batch 800 loss: 0.9109742546081543
  batch 850 loss: 0.9296890413761139
  batch 900 loss: 0.9949510872364045
LOSS train 0.99495 valid 0.99672, valid PER 31.14%
EPOCH 8:
  batch 50 loss: 0.8508777511119843
  batch 100 loss: 0.8074163520336151
  batch 150 loss: 0.7840383291244507
  batch 200 loss: 0.7837343275547027
  batch 250 loss: 0.776383473277092
  batch 300 loss: 0.7386214852333068
  batch 350 loss: 0.8165273356437683
  batch 400 loss: 0.7763644289970398
  batch 450 loss: 0.7949684441089631
  batch 500 loss: 0.8151971423625946
  batch 550 loss: 0.7412836575508117
  batch 600 loss: 0.7790132105350495
  batch 650 loss: 0.8072773253917694
  batch 700 loss: 0.7663837713003159
  batch 750 loss: 0.7773824918270111
  batch 800 loss: 0.7790640205144882
  batch 850 loss: 0.768021622300148
  batch 900 loss: 0.7744294166564941
LOSS train 0.77443 valid 0.88775, valid PER 28.24%
EPOCH 9:
  batch 50 loss: 0.6982800984382629
  batch 100 loss: 0.755341670513153
  batch 150 loss: 0.7553267240524292
  batch 200 loss: 0.6987230783700943
  batch 250 loss: 0.7408917069435119
  batch 300 loss: 0.7487294459342957
  batch 350 loss: 0.7817493510246277
  batch 400 loss: 0.7803350150585174
  batch 450 loss: 0.7570339846611023
  batch 500 loss: 0.7223015379905701
  batch 550 loss: 0.7660427224636078
  batch 600 loss: 0.7591681826114655
  batch 650 loss: 0.7328667032718659
  batch 700 loss: 0.712055252790451
  batch 750 loss: 0.7598817509412765
  batch 800 loss: 0.7601497733592987
  batch 850 loss: 0.7805441427230835
  batch 900 loss: 0.7109201872348785
LOSS train 0.71092 valid 0.88764, valid PER 27.67%
EPOCH 10:
  batch 50 loss: 0.6850883013010025
  batch 100 loss: 0.6895031994581222
  batch 150 loss: 0.727421840429306
  batch 200 loss: 0.7500800549983978
  batch 250 loss: 0.7306882381439209
  batch 300 loss: 0.6777385866641998
  batch 350 loss: 0.6963034868240356
  batch 400 loss: 0.6698088908195495
  batch 450 loss: 0.6741311889886856
  batch 500 loss: 0.7110690647363662
  batch 550 loss: 0.7160091602802277
  batch 600 loss: 0.6947234398126603
  batch 650 loss: 0.6965339934825897
  batch 700 loss: 0.6923681020736694
  batch 750 loss: 0.6805730044841767
  batch 800 loss: 0.7110083276033401
  batch 850 loss: 0.7176159965991974
  batch 900 loss: 0.7277285897731781
LOSS train 0.72773 valid 0.86603, valid PER 27.62%
EPOCH 11:
  batch 50 loss: 0.6417349117994309
  batch 100 loss: 0.6121425431966782
  batch 150 loss: 0.6446905612945557
  batch 200 loss: 0.6849995547533035
  batch 250 loss: 0.6778808611631394
  batch 300 loss: 0.6456238853931427
  batch 350 loss: 0.6843796002864838
  batch 400 loss: 0.6666027331352233
  batch 450 loss: 0.6749245029687881
  batch 500 loss: 0.6315609610080719
  batch 550 loss: 0.6679053395986557
  batch 600 loss: 0.649829438328743
  batch 650 loss: 0.7328711456060409
  batch 700 loss: 0.6445280700922013
  batch 750 loss: 0.6545903027057648
  batch 800 loss: 0.6851226538419724
  batch 850 loss: 0.700231910943985
  batch 900 loss: 0.6947396540641785
LOSS train 0.69474 valid 0.85629, valid PER 26.66%
EPOCH 12:
  batch 50 loss: 0.6562837064266205
  batch 100 loss: 0.6290961921215057
  batch 150 loss: 0.5923336625099183
  batch 200 loss: 0.6367856776714325
  batch 250 loss: 0.6421576136350632
  batch 300 loss: 0.6145021259784699
  batch 350 loss: 0.6268212050199509
  batch 400 loss: 0.6520260936021804
  batch 450 loss: 0.6371394377946854
  batch 500 loss: 0.6512781184911728
  batch 550 loss: 0.6114306837320328
  batch 600 loss: 0.6314469552040101
  batch 650 loss: 0.6516826230287552
  batch 700 loss: 0.6399343758821487
  batch 750 loss: 0.6297859013080597
  batch 800 loss: 0.6139628398418426
  batch 850 loss: 0.6574164736270904
  batch 900 loss: 0.6666466909646988
LOSS train 0.66665 valid 0.83967, valid PER 26.58%
EPOCH 13:
  batch 50 loss: 0.5822276401519776
  batch 100 loss: 0.5965858995914459
  batch 150 loss: 0.5881384092569352
  batch 200 loss: 0.6028241676092148
  batch 250 loss: 0.6010171145200729
  batch 300 loss: 0.5907229918241501
  batch 350 loss: 0.5808836448192597
  batch 400 loss: 0.5946013796329498
  batch 450 loss: 0.6217344158887863
  batch 500 loss: 0.5805482017993927
  batch 550 loss: 0.6018326473236084
  batch 600 loss: 0.5914417117834091
  batch 650 loss: 0.6209885329008102
  batch 700 loss: 0.626740179657936
  batch 750 loss: 0.582659963965416
  batch 800 loss: 0.6021520733833313
  batch 850 loss: 0.6514501494169235
  batch 900 loss: 0.6450059825181961
LOSS train 0.64501 valid 0.83725, valid PER 25.92%
EPOCH 14:
  batch 50 loss: 0.5697154730558396
  batch 100 loss: 0.5795841145515442
  batch 150 loss: 0.5639410305023194
  batch 200 loss: 0.5645545542240142
  batch 250 loss: 0.5678122735023499
  batch 300 loss: 0.5914272147417069
  batch 350 loss: 0.5530680400133133
  batch 400 loss: 0.5630638563632965
  batch 450 loss: 0.5735785716772079
  batch 500 loss: 0.5835803872346879
  batch 550 loss: 0.5848490190505982
  batch 600 loss: 0.5597432643175125
  batch 650 loss: 0.5989588862657547
  batch 700 loss: 0.5893186402320861
  batch 750 loss: 0.5658048534393311
  batch 800 loss: 0.5458530956506729
  batch 850 loss: 0.6041439032554626
  batch 900 loss: 0.5880501282215118
LOSS train 0.58805 valid 0.85395, valid PER 26.51%
EPOCH 15:
  batch 50 loss: 0.5273427426815033
  batch 100 loss: 0.5088620269298554
  batch 150 loss: 0.5128667151927948
  batch 200 loss: 0.5120939028263092
  batch 250 loss: 0.5218715453147889
  batch 300 loss: 0.49139514803886414
  batch 350 loss: 0.49894734859466555
  batch 400 loss: 0.5020042955875397
  batch 450 loss: 0.4978049653768539
  batch 500 loss: 0.4813829416036606
  batch 550 loss: 0.5084040027856827
  batch 600 loss: 0.5219241553544998
  batch 650 loss: 0.5237685930728913
  batch 700 loss: 0.5275863403081894
  batch 750 loss: 0.5206924319267273
  batch 800 loss: 0.49596520125865934
  batch 850 loss: 0.47696315348148344
  batch 900 loss: 0.5004185712337494
LOSS train 0.50042 valid 0.81647, valid PER 25.60%
EPOCH 16:
  batch 50 loss: 0.4867680209875107
  batch 100 loss: 0.4585360437631607
  batch 150 loss: 0.4804578602313995
  batch 200 loss: 0.4646423202753067
  batch 250 loss: 0.48880001306533816
  batch 300 loss: 0.47397624850273135
  batch 350 loss: 0.4776987737417221
  batch 400 loss: 0.4782836419343948
  batch 450 loss: 0.49110336661338805
  batch 500 loss: 0.4695462030172348
  batch 550 loss: 0.46313213407993314
  batch 600 loss: 0.47350739777088163
  batch 650 loss: 0.490784637928009
  batch 700 loss: 0.46868220567703245
  batch 750 loss: 0.4754931777715683
  batch 800 loss: 0.4851571929454803
  batch 850 loss: 0.4727702486515045
  batch 900 loss: 0.47395369052886965
LOSS train 0.47395 valid 0.82046, valid PER 25.06%
EPOCH 17:
  batch 50 loss: 0.444783910214901
  batch 100 loss: 0.4436643451452255
  batch 150 loss: 0.42880756556987765
  batch 200 loss: 0.42023088335990905
  batch 250 loss: 0.4366071662306786
  batch 300 loss: 0.43742531478405
  batch 350 loss: 0.40901867985725404
  batch 400 loss: 0.464728354215622
  batch 450 loss: 0.4214769107103348
  batch 500 loss: 0.4107323175668716
  batch 550 loss: 0.42473291397094726
  batch 600 loss: 0.44810878098011014
  batch 650 loss: 0.43525089144706725
  batch 700 loss: 0.425519218146801
  batch 750 loss: 0.4283260127902031
  batch 800 loss: 0.4018078514933586
  batch 850 loss: 0.4344786494970322
  batch 900 loss: 0.40161234557628633
LOSS train 0.40161 valid 0.81860, valid PER 24.68%
EPOCH 18:
  batch 50 loss: 0.4110852324962616
  batch 100 loss: 0.4018263539671898
  batch 150 loss: 0.418580242395401
  batch 200 loss: 0.4018011736869812
  batch 250 loss: 0.4229510921239853
  batch 300 loss: 0.3929069027304649
  batch 350 loss: 0.4073549285531044
  batch 400 loss: 0.39055094063282014
  batch 450 loss: 0.41275184392929076
  batch 500 loss: 0.412817208468914
  batch 550 loss: 0.41466175377368925
  batch 600 loss: 0.3885930037498474
  batch 650 loss: 0.37883389860391614
  batch 700 loss: 0.4191001838445663
  batch 750 loss: 0.3873528519272804
  batch 800 loss: 0.39455772876739503
  batch 850 loss: 0.38943553268909453
  batch 900 loss: 0.4125159350037575
LOSS train 0.41252 valid 0.81363, valid PER 24.74%
EPOCH 19:
  batch 50 loss: 0.3859451484680176
  batch 100 loss: 0.3723700085282326
  batch 150 loss: 0.39096035361289977
  batch 200 loss: 0.39590872198343274
  batch 250 loss: 0.3900231796503067
  batch 300 loss: 0.3892916509509087
  batch 350 loss: 0.3867897117137909
  batch 400 loss: 0.4000350072979927
  batch 450 loss: 0.404359812438488
  batch 500 loss: 0.3982700246572495
  batch 550 loss: 0.38750408053398133
  batch 600 loss: 0.3945115727186203
  batch 650 loss: 0.41970223844051363
  batch 700 loss: 0.37018990606069563
  batch 750 loss: 0.3845623245835304
  batch 800 loss: 0.4183685278892517
  batch 850 loss: 0.3943971705436706
  batch 900 loss: 0.37931049704551695
LOSS train 0.37931 valid 0.81688, valid PER 24.46%
EPOCH 20:
  batch 50 loss: 0.3858413961529732
  batch 100 loss: 0.37756494104862215
  batch 150 loss: 0.37486407458782195
  batch 200 loss: 0.3764679050445557
  batch 250 loss: 0.3763456240296364
  batch 300 loss: 0.38740893989801406
  batch 350 loss: 0.36247196286916733
  batch 400 loss: 0.38440452694892885
  batch 450 loss: 0.3824595159292221
  batch 500 loss: 0.35456101536750795
  batch 550 loss: 0.4056115385890007
  batch 600 loss: 0.35438895046710966
  batch 650 loss: 0.3830039918422699
  batch 700 loss: 0.37600768893957137
  batch 750 loss: 0.3582281395792961
  batch 800 loss: 0.39959439396858215
  batch 850 loss: 0.388514356315136
  batch 900 loss: 0.3673288491368294
LOSS train 0.36733 valid 0.81815, valid PER 24.46%
Training finished in 15.0 minutes.
Model saved to checkpoints/20230118_121629/model_18
Loading model from checkpoints/20230118_121629/model_18
SUB: 16.10%, DEL: 7.03%, INS: 2.66%, COR: 76.87%, PER: 25.79%
