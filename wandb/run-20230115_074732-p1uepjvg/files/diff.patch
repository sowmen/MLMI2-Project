diff --git a/run.py b/run.py
index 16de46b..8938f1f 100755
--- a/run.py
+++ b/run.py
@@ -46,7 +46,7 @@ print(device)
 args.device = device
 args.vocab = vocab
 
-exit(0)
+wandb.config = vars(args)
 
 model = models.BiLSTM(args.num_layers, args.fbank_dims * args.concat, args.model_dims, len(args.vocab))
 num_params = sum(p.numel() for p in model.parameters())
@@ -67,3 +67,11 @@ model.eval()
 model.to(device)
 results = decode(model, args, args.test_json)
 print("SUB: {:.2f}%, DEL: {:.2f}%, INS: {:.2f}%, COR: {:.2f}%, PER: {:.2f}%".format(*results))
+
+wandb.log({
+    "Test SUB": results[0],
+    "Test DEL": results[1],
+    "Test INS": results[2],
+    "Test COR": results[3],
+    "Test PER": results[4]                  
+})
diff --git a/trainer.py b/trainer.py
index b70dd90..ecdf4d1 100755
--- a/trainer.py
+++ b/trainer.py
@@ -1,5 +1,7 @@
 from datetime import datetime
 from pathlib import Path
+import wandb
+import tqdm
 
 import torch
 from torch.nn import CTCLoss
@@ -46,6 +48,11 @@ def train(model, args):
             if idx % args.report_interval + 1 == args.report_interval:
                 last_loss = running_loss / args.report_interval
                 print('  batch {} loss: {}'.format(idx + 1, last_loss))
+                wandb.log({
+                    "train_batch_loss": last_loss,
+                    "epoch": epoch,
+                    "train_batch": idx+1
+                })
                 tb_x = epoch * len(train_loader) + idx + 1
                 running_loss = 0.
         return last_loss
@@ -81,6 +88,11 @@ def train(model, args):
         print('LOSS train {:.5f} valid {:.5f}, valid PER {:.2f}%'.format(
             avg_train_loss, avg_val_loss, val_decode[4])
             )
+        wandb.log({
+            "avg_train_loss": avg_train_loss,
+            "avg_valid_loss": avg_val_loss,
+            "valid PER": val_decode[4]
+        })
 
         if avg_val_loss < best_val_loss:
             best_val_loss = avg_val_loss
