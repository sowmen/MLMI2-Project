Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20)
cuda:0
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.148040428161621
  batch 100 loss: 3.2870999670028684
  batch 150 loss: 3.1974140548706056
  batch 200 loss: 2.9167411136627197
  batch 250 loss: 2.723570261001587
  batch 300 loss: 2.569783639907837
  batch 350 loss: 2.418174777030945
  batch 400 loss: 2.402426424026489
  batch 450 loss: 2.3106922912597656
  batch 500 loss: 2.1726973414421082
  batch 550 loss: 2.137751216888428
  batch 600 loss: 2.0569660806655885
  batch 650 loss: 1.9792465925216676
  batch 700 loss: 1.9694508504867554
  batch 750 loss: 1.8915409469604492
  batch 800 loss: 1.8516327500343324
  batch 850 loss: 1.7811926794052124
  batch 900 loss: 1.7700436401367188
LOSS train 1.77004 valid 1.71529, valid PER 62.07%
EPOCH 2:
  batch 50 loss: 1.7011679005622864
  batch 100 loss: 1.6400078678131103
  batch 150 loss: 1.6296810054779052
  batch 200 loss: 1.6405953502655028
  batch 250 loss: 1.6403714323043823
  batch 300 loss: 1.5900803565979005
  batch 350 loss: 1.5072998356819154
  batch 400 loss: 1.517961971759796
  batch 450 loss: 1.4628626108169556
  batch 500 loss: 1.4969636797904968
  batch 550 loss: 1.5053270936012269
  batch 600 loss: 1.467102246284485
  batch 650 loss: 1.487163188457489
  batch 700 loss: 1.4596313405036927
  batch 750 loss: 1.4331553304195404
  batch 800 loss: 1.380222430229187
  batch 850 loss: 1.3611739349365235
  batch 900 loss: 1.4022100758552551
LOSS train 1.40221 valid 1.33883, valid PER 42.81%
EPOCH 3:
  batch 50 loss: 1.3473393630981445
  batch 100 loss: 1.3267758893966675
  batch 150 loss: 1.310409858226776
  batch 200 loss: 1.2830746388435363
  batch 250 loss: 1.3019715750217438
  batch 300 loss: 1.2639224469661712
  batch 350 loss: 1.3209969449043273
  batch 400 loss: 1.2775625228881835
  batch 450 loss: 1.261579337120056
  batch 500 loss: 1.2537652671337127
  batch 550 loss: 1.2615879964828491
  batch 600 loss: 1.2455703806877136
  batch 650 loss: 1.2014082419872283
  batch 700 loss: 1.2171194100379943
  batch 750 loss: 1.2733142864704132
  batch 800 loss: 1.197924952507019
  batch 850 loss: 1.2370715498924256
  batch 900 loss: 1.1818166160583496
LOSS train 1.18182 valid 1.27101, valid PER 37.82%
EPOCH 4:
  batch 50 loss: 1.153406354188919
  batch 100 loss: 1.1876293575763703
  batch 150 loss: 1.1237901425361634
  batch 200 loss: 1.16167587518692
  batch 250 loss: 1.1676617097854614
  batch 300 loss: 1.1855119526386262
  batch 350 loss: 1.116052827835083
  batch 400 loss: 1.159795960187912
  batch 450 loss: 1.1510420060157776
  batch 500 loss: 1.1297696769237517
  batch 550 loss: 1.1663763856887817
  batch 600 loss: 1.171902197599411
  batch 650 loss: 1.1663785254955292
  batch 700 loss: 1.1168007826805115
  batch 750 loss: 1.1033308267593385
  batch 800 loss: 1.0901983392238617
  batch 850 loss: 1.111436414718628
  batch 900 loss: 1.1354314613342285
LOSS train 1.13543 valid 1.12785, valid PER 35.08%
EPOCH 5:
  batch 50 loss: 1.045133295059204
  batch 100 loss: 1.0566268348693848
  batch 150 loss: 1.1152559208869934
  batch 200 loss: 1.0431070494651795
  batch 250 loss: 1.0559653639793396
  batch 300 loss: 1.0706267404556273
  batch 350 loss: 1.0536989486217498
  batch 400 loss: 1.078405169248581
  batch 450 loss: 1.0432000422477723
  batch 500 loss: 1.0782091653347015
  batch 550 loss: 1.0045608615875243
  batch 600 loss: 1.0857686805725097
  batch 650 loss: 1.0552609884738922
  batch 700 loss: 1.09277658700943
  batch 750 loss: 1.0135041284561157
  batch 800 loss: 1.0757385683059693
  batch 850 loss: 1.075183002948761
  batch 900 loss: 1.0692170822620393
LOSS train 1.06922 valid 1.06947, valid PER 33.35%
EPOCH 6:
  batch 50 loss: 1.03406454205513
  batch 100 loss: 0.9778183722496032
  batch 150 loss: 0.9738279449939727
  batch 200 loss: 1.0209052562713623
  batch 250 loss: 1.022140781879425
  batch 300 loss: 0.9940957915782929
  batch 350 loss: 1.017984162569046
  batch 400 loss: 1.034526698589325
  batch 450 loss: 1.064496796131134
  batch 500 loss: 1.0142025971412658
  batch 550 loss: 1.0517258536815643
  batch 600 loss: 0.9669726240634918
  batch 650 loss: 1.0028799653053284
  batch 700 loss: 0.9976350057125092
  batch 750 loss: 1.0059945559501648
  batch 800 loss: 0.9969777166843414
  batch 850 loss: 0.9976046967506409
  batch 900 loss: 1.0041804277896882
LOSS train 1.00418 valid 1.05370, valid PER 33.16%
EPOCH 7:
  batch 50 loss: 0.9862943315505981
  batch 100 loss: 1.0546923089027405
  batch 150 loss: 1.0104046642780304
  batch 200 loss: 0.9778892707824707
  batch 250 loss: 0.9738347220420838
  batch 300 loss: 0.9593068659305573
  batch 350 loss: 0.9674678826332093
  batch 400 loss: 0.9557912337779999
  batch 450 loss: 0.9693877947330475
  batch 500 loss: 0.9548641574382782
  batch 550 loss: 0.9524310827255249
  batch 600 loss: 0.9544105696678161
  batch 650 loss: 0.9506598913669586
  batch 700 loss: 0.9848659491539001
  batch 750 loss: 0.9542868185043335
  batch 800 loss: 0.9841422617435456
  batch 850 loss: 0.9827741968631745
  batch 900 loss: 1.015564250946045
LOSS train 1.01556 valid 1.04999, valid PER 32.73%
EPOCH 8:
  batch 50 loss: 0.9247843933105468
  batch 100 loss: 0.9209428524971008
  batch 150 loss: 0.9146142327785491
  batch 200 loss: 0.9018881392478942
  batch 250 loss: 0.9202842497825623
  batch 300 loss: 0.9021169483661652
  batch 350 loss: 0.9482201337814331
  batch 400 loss: 0.9226531457901
  batch 450 loss: 0.944461464881897
  batch 500 loss: 0.9517556130886078
  batch 550 loss: 0.9117832791805267
  batch 600 loss: 0.9476573300361634
  batch 650 loss: 0.9654175543785095
  batch 700 loss: 0.9193656671047211
  batch 750 loss: 0.916325433254242
  batch 800 loss: 0.9292415165901184
  batch 850 loss: 0.9331794464588166
  batch 900 loss: 0.9249403953552247
LOSS train 0.92494 valid 1.00610, valid PER 31.48%
EPOCH 9:
  batch 50 loss: 0.8598204112052917
  batch 100 loss: 0.9074993252754211
  batch 150 loss: 0.910837014913559
  batch 200 loss: 0.8695145535469055
  batch 250 loss: 0.8784643077850341
  batch 300 loss: 0.8974568200111389
  batch 350 loss: 0.9130504357814789
  batch 400 loss: 0.9046166002750397
  batch 450 loss: 0.9120292568206787
  batch 500 loss: 0.8676769125461579
  batch 550 loss: 0.9082792973518372
  batch 600 loss: 0.942627671957016
  batch 650 loss: 0.8874996423721313
  batch 700 loss: 0.885665830373764
  batch 750 loss: 0.88743013381958
  batch 800 loss: 0.9168216717243195
  batch 850 loss: 0.9193230020999908
  batch 900 loss: 0.8896377599239349
LOSS train 0.88964 valid 0.99215, valid PER 30.73%
EPOCH 10:
  batch 50 loss: 0.8299224150180816
  batch 100 loss: 0.8878891062736511
  batch 150 loss: 0.9018731510639191
  batch 200 loss: 0.8848244845867157
  batch 250 loss: 0.9122215664386749
  batch 300 loss: 0.8422266590595245
  batch 350 loss: 0.8761095082759858
  batch 400 loss: 0.8417192053794861
  batch 450 loss: 0.9657581198215485
  batch 500 loss: 0.9920936834812164
  batch 550 loss: 0.9515195119380951
  batch 600 loss: 0.9115337944030761
  batch 650 loss: 0.9006284165382386
  batch 700 loss: 0.9060707581043244
  batch 750 loss: 0.8780096566677094
  batch 800 loss: 0.8881629133224487
  batch 850 loss: 0.9156864058971405
  batch 900 loss: 0.8989378762245178
LOSS train 0.89894 valid 1.01750, valid PER 32.11%
EPOCH 11:
  batch 50 loss: 0.8247075748443603
  batch 100 loss: 0.8104170346260071
  batch 150 loss: 0.816878080368042
  batch 200 loss: 0.8800205278396607
  batch 250 loss: 0.8747499680519104
  batch 300 loss: 0.8435065627098084
  batch 350 loss: 0.8374447762966156
  batch 400 loss: 0.8679637110233307
  batch 450 loss: 0.8980356693267822
  batch 500 loss: 0.8728065836429596
  batch 550 loss: 0.8456721389293671
  batch 600 loss: 0.8389268040657043
  batch 650 loss: 0.9008435380458831
  batch 700 loss: 0.820966295003891
  batch 750 loss: 0.8582126498222351
  batch 800 loss: 0.886383045911789
  batch 850 loss: 0.9158874559402466
  batch 900 loss: 0.88783989071846
LOSS train 0.88784 valid 1.02613, valid PER 32.14%
EPOCH 12:
  batch 50 loss: 0.8473555862903595
  batch 100 loss: 0.8240465557575226
  batch 150 loss: 0.8134656155109405
  batch 200 loss: 0.81785062789917
  batch 250 loss: 0.879021931886673
  batch 300 loss: 0.8383072423934936
  batch 350 loss: 0.8373039281368255
  batch 400 loss: 0.8786343681812286
  batch 450 loss: 0.8438774871826172
  batch 500 loss: 0.8957338845729828
  batch 550 loss: 0.812827183008194
  batch 600 loss: 0.8219930922985077
  batch 650 loss: 0.8781734073162079
  batch 700 loss: 0.8672535288333892
  batch 750 loss: 0.8120170736312866
  batch 800 loss: 0.844660279750824
  batch 850 loss: 0.8685885500907898
  batch 900 loss: 0.863030766248703
LOSS train 0.86303 valid 0.97002, valid PER 29.82%
EPOCH 13:
  batch 50 loss: 0.7897424268722534
  batch 100 loss: 0.8047798299789428
  batch 150 loss: 0.776273120045662
  batch 200 loss: 0.8023475337028504
  batch 250 loss: 0.8145025277137756
  batch 300 loss: 0.8151745140552521
  batch 350 loss: 0.8553728020191192
  batch 400 loss: 0.8383960282802582
  batch 450 loss: 0.848638322353363
  batch 500 loss: 0.8284393978118897
  batch 550 loss: 0.8381977415084839
  batch 600 loss: 0.8281030142307282
  batch 650 loss: 0.8425961565971375
  batch 700 loss: 0.8285680270195007
  batch 750 loss: 0.7822006213665008
  batch 800 loss: 0.8060970520973205
  batch 850 loss: 0.8553679370880127
  batch 900 loss: 0.8443221807479858
LOSS train 0.84432 valid 0.98732, valid PER 29.54%
EPOCH 14:
  batch 50 loss: 0.7940945053100585
  batch 100 loss: 0.7578733897209168
  batch 150 loss: 0.77222860455513
  batch 200 loss: 0.7915945851802826
  batch 250 loss: 0.7813042944669724
  batch 300 loss: 0.8171482157707214
  batch 350 loss: 0.7594617283344269
  batch 400 loss: 0.7803643083572388
  batch 450 loss: 0.7636608898639679
  batch 500 loss: 0.8031694018840789
  batch 550 loss: 0.8128081107139588
  batch 600 loss: 0.7590040403604508
  batch 650 loss: 0.7754449129104615
  batch 700 loss: 0.8167164468765259
  batch 750 loss: 0.7821649789810181
  batch 800 loss: 0.7806548798084259
  batch 850 loss: 0.8128893172740936
  batch 900 loss: 0.8007036352157593
LOSS train 0.80070 valid 0.99424, valid PER 30.47%
EPOCH 15:
  batch 50 loss: 0.7483471912145615
  batch 100 loss: 0.7365513527393341
  batch 150 loss: 0.7697370874881745
  batch 200 loss: 0.8214203095436097
  batch 250 loss: 0.7982780289649963
  batch 300 loss: 0.7664464092254639
  batch 350 loss: 0.7748291468620301
  batch 400 loss: 0.77304279088974
  batch 450 loss: 0.7709104430675506
  batch 500 loss: 0.7393930542469025
  batch 550 loss: 0.7561365807056427
  batch 600 loss: 0.8042099678516388
  batch 650 loss: 0.8005958116054535
  batch 700 loss: 0.8066688096523285
  batch 750 loss: 0.785336629152298
  batch 800 loss: 0.7886973965167999
  batch 850 loss: 0.7672967767715454
  batch 900 loss: 0.8096137106418609
LOSS train 0.80961 valid 0.97729, valid PER 30.04%
EPOCH 16:
  batch 50 loss: 0.7612762868404388
  batch 100 loss: 0.733053605556488
  batch 150 loss: 0.7429095953702927
  batch 200 loss: 0.7585696136951446
  batch 250 loss: 0.8119871366024017
  batch 300 loss: 0.7853033304214477
  batch 350 loss: 0.785664609670639
  batch 400 loss: 0.7754239666461945
  batch 450 loss: 0.7907001519203186
  batch 500 loss: 0.7439379680156708
  batch 550 loss: 0.8050004708766937
  batch 600 loss: 0.8003699064254761
  batch 650 loss: 0.7956631481647491
  batch 700 loss: 0.7695375084877014
  batch 750 loss: 0.7730973446369171
  batch 800 loss: 0.7732992231845855
  batch 850 loss: 0.7612441116571427
  batch 900 loss: 0.7888038790225983
LOSS train 0.78880 valid 0.97376, valid PER 29.63%
EPOCH 17:
  batch 50 loss: 0.7220035058259964
  batch 100 loss: 0.7358054375648498
  batch 150 loss: 0.7211787617206573
  batch 200 loss: 0.7249554538726807
  batch 250 loss: 0.7423304748535157
  batch 300 loss: 0.7413150715827942
  batch 350 loss: 0.7174928033351898
  batch 400 loss: 0.7597637677192688
  batch 450 loss: 0.7556102693080902
  batch 500 loss: 0.709527645111084
  batch 550 loss: 0.7329203760623932
  batch 600 loss: 0.7823018634319305
  batch 650 loss: 0.7236536741256714
  batch 700 loss: 0.7209166419506073
  batch 750 loss: 0.7247713869810104
  batch 800 loss: 0.7325588780641555
  batch 850 loss: 0.7550155705213547
  batch 900 loss: 0.717274265885353
LOSS train 0.71727 valid 0.97235, valid PER 29.40%
EPOCH 18:
  batch 50 loss: 0.7172261202335357
  batch 100 loss: 0.7098569321632385
  batch 150 loss: 0.7451466059684754
  batch 200 loss: 0.7431651663780212
  batch 250 loss: 0.7272495400905609
  batch 300 loss: 0.7030431234836578
  batch 350 loss: 0.7379575586318969
  batch 400 loss: 0.6997046673297882
  batch 450 loss: 0.7449069476127624
  batch 500 loss: 0.7139456188678741
  batch 550 loss: 0.7252621912956237
  batch 600 loss: 0.7026676398515701
  batch 650 loss: 0.7227833890914916
  batch 700 loss: 0.7542230570316315
  batch 750 loss: 0.7506058597564698
  batch 800 loss: 0.7272594964504242
  batch 850 loss: 0.735837213397026
  batch 900 loss: 0.7322594833374023
LOSS train 0.73226 valid 0.95081, valid PER 28.72%
EPOCH 19:
  batch 50 loss: 0.6823028701543808
  batch 100 loss: 0.7224343174695969
  batch 150 loss: 0.7045343798398972
  batch 200 loss: 0.7427331459522247
  batch 250 loss: 0.7608783268928527
  batch 300 loss: 0.7486211764812469
  batch 350 loss: 0.7182601684331894
  batch 400 loss: 0.7406561303138733
  batch 450 loss: 0.786253228187561
  batch 500 loss: 0.7491291284561157
  batch 550 loss: 0.7289661288261413
  batch 600 loss: 0.7364247399568558
  batch 650 loss: 0.7761666691303253
  batch 700 loss: 0.737123287320137
  batch 750 loss: 0.7230999386310577
  batch 800 loss: 0.7477868223190307
  batch 850 loss: 0.7165792226791382
  batch 900 loss: 0.7192047494649887
LOSS train 0.71920 valid 0.99583, valid PER 29.42%
EPOCH 20:
  batch 50 loss: 0.6686962848901749
  batch 100 loss: 0.6733762365579605
  batch 150 loss: 0.6886617976427079
  batch 200 loss: 0.6763800501823425
  batch 250 loss: 0.6880058538913727
  batch 300 loss: 0.7564493107795716
  batch 350 loss: 0.6905810326337815
  batch 400 loss: 0.7270684838294983
  batch 450 loss: 0.7319974792003632
  batch 500 loss: 0.7090062266588211
  batch 550 loss: 0.75009810090065
  batch 600 loss: 0.7074062025547028
  batch 650 loss: 0.7206228417158127
  batch 700 loss: 0.7194267439842225
  batch 750 loss: 0.6924223083257676
  batch 800 loss: 0.740111101269722
  batch 850 loss: 0.7327448856830597
  batch 900 loss: 0.7320103478431702
LOSS train 0.73201 valid 0.96584, valid PER 29.20%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230115_074753/model_18
Loading model from checkpoints/20230115_074753/model_18
SUB: 16.57%, DEL: 11.87%, INS: 2.63%, COR: 71.56%, PER: 31.07%